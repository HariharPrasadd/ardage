## **The Inductive Bias of Quantum Kernels**

**Jonas M. Kübler** _[∗]_ **Simon Buchholz** _[∗]_ **Bernhard Schölkopf**
Max Planck Institute for Intelligent Systems
Tübingen, Germany
```
          {jmkuebler, sbuchholz, bs}@tue.mpg.de

```

**Abstract**


It has been hypothesized that quantum computers may lend themselves well to
applications in machine learning. In the present work, we analyze function classes
defined via _quantum kernels_ . Quantum computers offer the possibility to efficiently
compute inner products of exponentially large density operators that are classically
hard to compute. However, having an exponentially large feature space renders
the problem of generalization hard. Furthermore, being able to evaluate inner
products in high dimensional spaces efficiently by itself does not guarantee a
quantum advantage, as already classically tractable kernels can correspond to highor infinite-dimensional reproducing kernel Hilbert spaces (RKHS).
We analyze the spectral properties of quantum kernels and find that we can expect
an advantage if their RKHS is low dimensional and contains functions that are
hard to compute classically. If the target function is known to lie in this class, this
implies a quantum advantage, as the quantum computer can encode this _inductive_
_bias_, whereas there is no classically efficient way to constrain the function class in
the same way. However, we show that finding suitable quantum kernels is not easy
because the kernel evaluation might require exponentially many measurements.
In conclusion, our message is a somewhat sobering one: we conjecture that quantum machine learning models can offer speed-ups only if we manage to encode
knowledge about the problem at hand into quantum circuits, while encoding the
same bias into a classical model would be hard. These situations may plausibly
occur when learning on data generated by a quantum process, however, they appear
to be harder to come by for classical datasets.


**1** **Introduction**


In recent years, much attention has been dedicated to studies of how small and noisy quantum devices

[ 1 ] could be used for near term applications to showcase the power of quantum computers. Besides
fundamental demonstrations [ 2 ], potential applications that have been discussed are in quantum
chemistry [3], discrete optimization [4] and machine learning (ML) [5–12].


Initiated by the seminal HHL algorithm [ 13 ], early work in quantum machine learning (QML) was
focused on speeding up linear algebra subroutines, commonly used in ML, offering the perspective
of a runtime logarithmic in the problem size [ 14 – 17 ]. However, most of these works have an inverse
polynomial scaling of the runtime in the error and it was shown rigorously by Ciliberto et al. [18]
that due to the quantum mechanical measurement process a runtime complexity _O_ ( _[√]_ ~~_n_~~ ~~)~~ is necessary
for convergence rate 1 _/_ _[√]_ ~~_n_~~ ~~.~~


Rather than speeding up linear algebra subroutines, we focus on more recent suggestions that use a
quantum device to define and implement the function class and do the optimization on a classical
computer. There are two ways to that: the first are so-called _Quantum Neural Networks_ (QNN) or


_∗_ JMK and SB contributed equally and are ordered randomly.


35th Conference on Neural Information Processing Systems (NeurIPS 2021).


|Col1|R (x )<br>X 1|Col3|V|ρ˜V (x<br>id<br>· · ·<br>id|
|---|---|---|---|---|
||_RX_(_xd_)|_RX_(_xd_)|_RX_(_xd_)|_RX_(_xd_)|
||||||


(c)





|Col1|R (x )<br>X 1|Col3|V|Col5|
|---|---|---|---|---|
||_RX_(_x_1)||_V_||
||_RX_(_x_1)||_V_||
||_RX_(_xd_)|_RX_(_xd_)|_RX_(_xd_)|_RX_(_xd_)|
||||||


(b)



 ~~~~ 

_ρ_ _[V]_ ( _x_ )

~~~~ 



|Col1|R (x )<br>X 1|Col3|V|M<br>id<br>· · ·<br>id|
|---|---|---|---|---|
||_RX_(_xd_)|_RX_(_xd_)|_RX_(_xd_)|_RX_(_xd_)|
||||||


(a)



Figure 1: **Quantum advantage via inductive bias:** (a) Data generating quantum circuit _f_ ( _x_ ) =
Tr � _ρ_ _[V]_ ( _x_ )( _M ⊗_ id)� = Tr � _ρ_ ˜ _[V]_ ( _x_ ) _M_ � . (b) The full quantum kernel _k_ ( _x, x_ _[′]_ ) = Tr � _ρ_ _[V]_ ( _x_ ) _ρ_ _[V]_ ( _x_ _[′]_ )�

is too general and cannot learn _f_ efficiently. (c) The biased quantum kernel _q_ ( _x, x_ _[′]_ ) =
Tr � _ρ_ ˜ _[V]_ ( _x_ )˜ _ρ_ _[V]_ ( _x_ _[′]_ )� meaningfully constrains the function space and allows to learn _f_ with little data.


parametrized quantum circuits [ 5 – 7 ] which can be trained via gradient based optimization [ 5, 19 – 23 ].
The second approach is to use a predefined way of encoding the data in the quantum system and
defining a _quantum kernel_ as the inner product of two quantum states [ 7 – 11 ]. These two approaches
essentially provide a parametric and a non-parametric path to quantum machine learning, which are
closely related to each other [ 11 ]. Since the optimization of QNNs is non-convex and suffers from
so-called Barren Plateaus [ 24 ], we here focus on quantum kernels, which allow for convex problems
and thus lend themselves more readily to theoretical analysis.


The central idea of using a QML model is that it enables to do computations that are exponentially
hard classically. However, also in classical ML, kernel methods allow us to implicitly work with
high- or infinite dimensional function spaces [ 25, 26 ]. Thus, purely studying the expressivity of
QML models [27] is not sufficient to understand when we can expect speed-ups. Only recently first
steps where taken into this direction [ 10, 12, 28 ]. Assuming classical hardness of computing discrete
logarithms, Liu et al. [10] proposed a task based on the computation of the discrete logarithm where
the quantum computer, equipped with the right feature mapping, can learn the target function with
exponentially less data than any classical (efficient) algorithm. Similarly, Huang et al. [12] analyzed
generalization bounds and realized that the expressivity of quantum models can hinder generalization.
They proposed a heuristic to optimize the labels of a dataset such that it can be learned well by a
quantum computer but not a classical machine.


In this work, we relate the discussion of quantum advantages to the classical concept of _inductive_
_bias_ . The _no free lunch_ theorem informally states that no learning algorithm can outperform other
algorithms on all problems. This implies that an algorithm that performs well on one type of problem
necessarily performs poorly on other problems. A standard inductive bias in ML is to prefer functions
that are continuous. An algorithm with that bias, however, will then struggle to learn functions that
are discontinuous. For a QML model to have an edge over classical ML models, we could thus ensure
that it is equipped with an inductive bias that cannot be encoded (efficiently) with a classical machine.
If a given dataset fits this inductive bias, the QML model will outperform any classical algorithm. For
kernel methods, the qualitative concept of inductive bias can be formalized by analyzing the spectrum
of the kernel and relating it to the target function [25, 29–33].


Our main contribution is the analysis of the inductive bias of quantum machine learning models based
on the spectral properties of quantum kernels. First, we show that quantum kernel methods will fail to
generalize as soon as the data embedding into the quantum Hilbert space is too expressive (Theorem
1). Then we note that projecting the quantum kernel appropriately allows to construct inductive biases
that are hard to create classically (Figure 1). However, our Theorem 2 also implies that estimating the
biased kernel requires exponential measurements, a phenomenon reminiscent of the Barren plateaus
observed in quantum neural networks. Finally we show experiments supporting our main claims.


While our work gives guidance to find a quantum advantage in ML, this yields no recipe for obtaining
a quantum advantage on a classical dataset. We conjecture that unless we have a clear idea how the
data generating process can be described with a quantum computer, we cannot expect an advantage
by using a quantum model in place of a classical machine learning model.


2


**2** **Supervised learning**


We briefly introduce the setting and notation for supervised learning as a preparation for our analysis
of quantum mechanical methods in this context. The goal of supervised learning is the estimation of
a functional mechanism based on data generated from this mechanism. For concreteness we focus
on the regression setting where we assume data is generated according to _Y_ = _f_ _[∗]_ ( _X_ ) + _ε_ where
_ε_ denotes zero-mean noise. We focus on _X ∈X ⊂_ R _[d]_, _Y ∈_ R . We denote the joint probability
distribution of ( _X, Y_ ) by _D_ and we are given _n_ i.i.d. observations _D_ _n_ from _D_ . We will refer to the
marginal distribution of _X_ as _µ_, define the _L_ [2] _µ_ [inner product] _[ ⟨][f, g][⟩]_ [=] � _f_ ( _x_ ) _g_ ( _x_ ) _µ_ (d _x_ ) and denote
the corresponding norm by _∥· ∥_ . The least square risk and the empirical risk of some hypothesis
_h_ : _X →_ R is defined by _R_ ( _h_ ) = E _D_ �( _h_ ( _X_ ) _−_ _Y_ ) [2] [�] and _R_ _n_ ( _h_ ) = E _D_ _n_ �( _h_ ( _X_ ) _−_ _Y_ ) [2] [�] .


In supervised machine learning, one typically considers a hypothesis space _H_ of functions _h_ : _X →_ R
and tries to infer argmin _h∈H_ _R_ ( _h_ ) (assuming for simplicity that the minimizer exists). Typically this
is done by (regularized) empirical risk minimization argmin _h∈H_ _R_ _n_ ( _h_ ) + _λ_ Ω( _h_ ), where _λ >_ 0 and
Ω determine the regularization. The risk of _h_ can then be decomposed in generalization and training
error _R_ ( _h_ ) = ( _R_ ( _h_ ) _−_ _R_ _n_ ( _h_ )) + _R_ _n_ ( _h_ ) _._


**Kernel ridge regression.** We will focus on solving the regression problem over a reproducing
kernel Hilbert space (RKHS) [ 25, 26 ]. An RKHS _F_ associated with a positive definite kernel
_k_ : _X × X →_ R is the space of functions such that for all _x ∈X_ and _h ∈F_ the _reproducing_ property
_h_ ( _x_ ) = _⟨h, k_ ( _x, ·_ ) _⟩_ _F_ holds. Kernel ridge regression regularizes the RKHS norm, i.e., Ω( _h_ ) = _∥h∥_ [2] _F_ _[.]_
With observations _{_ ( _x_ [(] _[i]_ [)] _, y_ [(] _[i]_ [)] ) _}_ _[n]_ _i_ =1 [we can compute the kernel matrix] _[ K]_ [(] _[X, X]_ [)] _[ij]_ [ =] _[ k]_ [(] _[x]_ [(] _[i]_ [)] _[, x]_ [(] _[j]_ [)] [)]
and the Representer Theorem [ 34 ] ensures that the empirical risk minimizer of kernel ridge regression
is of the form _f_ [ˆ] _n_ _[λ]_ [(] _[·]_ [) =][ �] _[n]_ _i_ =1 _[α]_ _[i]_ _[k]_ [(] _[x]_ [(] _[i]_ [)] _[,][ ·]_ [)] [, with] _[ α]_ [ = (] _[K]_ [(] _[X, X]_ [) +] _[ λ]_ [ id)] _[−]_ [1] _[y]_ [. The goal of our work is]
to study when a (quantum) kernel is suitable for learning a particular problem. The central object to
study this is the integral operator.



**Spectral properties and inductive bias.** For kernel _k_ and marginal distribution _µ_, the integral
operator _K_, is defined as ( _Kf_ )( _x_ ) = � _k_ ( _x, x_ _[′]_ ) _f_ ( _x_ _[′]_ ) _µ_ (d _x_ _[′]_ ) . Mercer’s Theorem ensures that there
exist a spectral decomposition of _K_ with (possibly infinitely many) eigenvalues _γ_ _i_ (ordered nonincreasingly) and corresponding eigenfunctions _φ_ _i_, which are orthonormal in _L_ [2] _µ_ [, i.e.,] _[ ⟨][φ]_ _[i]_ _[, φ]_ _[j]_ _[⟩]_ [=] _[ δ]_ _[i,j]_ [.]
We will assume that Tr [ _K_ ] = [�] _i_ _[γ]_ _[i]_ [ = 1] [ which we can ensure by rescaling the kernel. We can]

then write _k_ ( _x, x_ _[′]_ ) = [�] _i_ _[γ]_ _[i]_ _[φ]_ _[i]_ [(] _[x]_ [)] _[φ]_ _[i]_ [(] _[x]_ _[′]_ [)] [. While the functions] _[ φ]_ [ form a basis of] _[ F]_ [ they might not]

completely span _L_ [2] _µ_ [. In this case we simply complete the basis and implicitly take] _[ γ]_ [ = 0] [ for the]
added functions. Then we can decompose functions in _L_ [2] _µ_ [as]



_f_ ( _x_ ) = � _i_ _[a]_ _[i]_ _[φ]_ _[i]_ [(] _[x]_ [)] _[.]_ (1)



We have _∥f_ _∥_ [2] = [�] _i_ _[a]_ _i_ [2] [and] _[ ∥][f]_ _[∥]_ [2] _F_ [=][ �] _i_ _aγ_ [2] _ii_ [(if] _[ f][ ∈F]_ [). Kernel ridge regression penalizes the]

RKHS norm of functions. The components corresponding to zero eigenvalues are infinitely penalized
and cannot be learned since they are not in the RKHS. For large regularization _λ_ the solution _f_ [ˆ] _n_ _[λ]_ [is]
heavily _biased_ towards learning only the coefficients of the principal components (corresponding to
the largest eigenvalues) and keeps the other coefficients small (at the risk of _underfitting_ ). Decreasing
the regularization allows ridge regression to also fit the other components, however, at the potential
risk of overfitting to the noise in the empirical data. Finding good choices of _λ_ thus balances this
_bias-variance_ tradeoff.



We have _∥f_ _∥_ [2] = [�]



_i_ _[a]_ _i_ [2] [and] _[ ∥][f]_ _[∥]_ [2] _F_ [=][ �]



We are less concerned with the choice of _λ_, but rather with the spectral properties of a kernel that
allow for a quantum advantage. Similar to the above considerations, a target function _f_ can easily
be learned if it is well _aligned_ with the principal components of a kernel. In the easiest case, the
kernel only has a single non-zero eigenvalue and is just _k_ ( _x, x_ _[′]_ ) = _f_ ( _x_ ) _f_ ( _x_ _[′]_ ) . Such a construction is
arguably the simplest path to a quantum advantage in ML.


**Example 1** (Trivial Quantum Advantage) **.** Let _f_ be a scalar function that is easily computable on
a quantum device but requires exponential resources to approximate classically. Generate data as
_Y_ = _f_ ( _X_ ) + _ϵ_ . The kernel _k_ ( _x, x_ _[′]_ ) = _f_ ( _x_ ) _f_ ( _x_ _[′]_ ) then has an exponential advantage for learning _f_
from data.


3


To go beyond this trivial case, we introduce two qualitative measures to judge the quality of a kernel
for learning the function _f_ . The _kernel target alignment_ of Cristianini et al. [30] is



(2)
_i_ _[a]_ _i_ [2]



_⟨k,_ _f ⊗_ _f_ _⟩_ �

[=]
_⟨k, k⟩_ [1] _[/]_ [2] _⟨f ⊗_ _f, f ⊗_ _f_ _⟩_ [1] _[/]_ [2] ( ~~[�]~~ _i_ _[γ]_ _i_ [2]



_⟨k,_ _f ⊗_ _f_ _⟩_
_A_ ( _k, f_ ) =



� _i_ _[γ]_ _[i]_ _[a]_ _i_ [2]

( ~~[�]~~ _i_ _[γ]_ _i_ [2] [)] [1] _[/]_ [2]



_i_ _[γ]_ _i_ [2] [)] [1] _[/]_ [2] ~~[ �]~~



and measures how well the kernel fits _f_ . If _A_ = 1, learning reduces to estimating a single real
parameter, whereas for _A_ = 0, learning is infeasible. We note that the kernel target alignment also
weighs the contributions of _f_ depending on the corresponding eigenvalue, i.e., the alignment is better
if large _|a_ _i_ _|_ correspond to large _γ_ _i_ . The kernel target alignment was used extensively to optimize
kernel functions [31] and recently also used to optimize quantum kernels [35].


In a similar spirit, the _task-model alignment_ of Canatar et al. [32] measures how much of the signal
of _f_ is captured in the first _i_ principal components: _C_ ( _i_ ) = [�] _i_ _[a]_ [2] [(][�] _[a]_ [2] [)] _[−]_ [1] _[.]_ [ The slower] _[ C]_ [(] _[i]_ [)]



of _f_ is captured in the first _i_ principal components: _C_ ( _i_ ) = [�] _j≤i_ _[a]_ [2] _j_ [(][�] _j_ _[a]_ [2] _j_ [)] _[−]_ [1] _[.]_ [ The slower] _[ C]_ [(] _[i]_ [)]

approaches 1, the harder it is to learn as the target function is more spread over the eigenfunctions.



_j≤i_ _[a]_ [2] _j_ [(][�]



**3** **Quantum computation in machine learning**


In this section we introduce hypothesis spaces containing functions whose output is given by the
result of a quantum computation. For a general introduction to concepts of quantum computation we
refer to the book of Nielsen and Chuang [36].


We will consider quantum systems comprising _d ∈_ N qubits. Discussing such systems and their
algebraic properties does not require in-depth knowledge of quantum mechanics. A _pure state_ of a
single qubit is described by vector ( _α, β_ ) _[⊤]_ _∈_ C [2] s.t. _|α|_ [2] + _|β|_ [2] = 1 and we write _|ψ⟩_ = _α |_ 0 _⟩_ + _β |_ 1 _⟩_,
where _{|_ 0 _⟩_ _, |_ 1 _⟩}_ forms the computational basis. A _d_ qubit pure state lives in the tensor product of the
single qubit state spaces, i.e., it is described by a normalized vector in C [2] _[d]_ . A _mixed state_ of a _d_ -qubit
system can be described by a density operator _ρ ∈_ C [2] _[d]_ _[×]_ [2] _[d]_, i.e., a positive definite matrix ( _ρ ≥_ 0 )
with unit trace ( Tr [ _ρ_ ] = 1 ). For a pure state _|ψ⟩_ the corresponding density operator is _ρ_ = _|ψ⟩⟨ψ|_
(here, _⟨ψ|_ is the complex conjugate transpose of _|ψ⟩_ ). A general density operator can be thought of
as a classical probabilistic mixture of pure states. We can extract information from _ρ_ by estimating
(through repeated measurements) the expectation of a suitable _observable_, i.e., a Hermitian operator
_M_ = _M_ _[†]_ (where the adjoint ( _·_ ) _[†]_ is the complex conjugate of the transpose), as


Tr [ _ρM_ ] _._ (3)


Put simply, the potential advantage of a quantum computer arises from its state space being exponentially large in the number of qubits _d_, thus computing general expressions like (3) on a classical
computer is exponentially hard. However, besides the huge obstacles in building quantum devices
with high fidelity, the fact that the outcome of the quantum computation (3) has to be estimated from
measurements often prohibits to easily harness this power, see also Wang et al. [37], Peters et al. [38] .
We will discuss this in the context of quantum kernels in Section 4.


We consider parameter dependent quantum states _ρ_ ( _x_ ) = _U_ ( _x_ ) _ρ_ 0 _U_ _[†]_ ( _x_ ) that are generated by
evolving the initial state _ρ_ 0 with the data dependent unitary transformation _U_ ( _x_ ) [ 7, 11 ]. Most often
we will without loss of generality assume that the initial state is _ρ_ 0 = ( _|_ 0 _⟩⟨_ 0 _|_ ) _[⊗][d]_ . We then define
quantum machine learning models via observables _M_ of the data dependent state


_f_ _M_ ( _x_ ) = Tr � _U_ ( _x_ ) _ρ_ 0 _U_ _[†]_ ( _x_ ) _M_ � = Tr [ _ρ_ ( _x_ ) _M_ ] _._ (4)


In the following we introduce the two most common function classes suggested for quantum machine
learning. We note that there also exist proposals that do not fit into the form of Eq. (4) [27, 35, 39].


**Quantum neural networks.** A "quantum neural network" (QNN) is defined via a _variational_
_quantum circuit_ (VQC) [ 6, 40, 41 ]. Here the observable _M_ _θ_ is parametrized by _p ∈_ N classical
parameters _θ ∈_ Θ _⊆_ R _[p]_ . This defines a parametric function class _F_ Θ = _{f_ _M_ _θ_ _|θ ∈_ Θ _}_ . The most
common ansatz is to consider _M_ _θ_ = _U_ (Θ) _MU_ _[†]_ (Θ) where _U_ (Θ) = [�] _i_ _[U]_ [(] _[θ]_ _[i]_ [)] [ is the composition of]
unitary evolutions each acting on few qubits. For this and other common models of the parametric
circuit it is possible to analytically compute gradients and specific optimizers for quantum circuits
based on gradient descent have been developed [ 5, 19 – 23 ]. Nevertheless, the optimization is usually
a non-convex problem and suffers from additional difficulties due to oftentimes exponentially (in _d_ )


4


Table 1: Concepts in the quantum Hilbert space _H_ and the reproducing kernel Hilbert space _F_ .

|Quantum Space of d qubits|RKHS|
|---|---|
|_x →ρ_(_x_)_ ∈H_ (explicit feature map)<br>_H_ =<br>n<br>_ρ ∈_C2_d×_2_d | ρ_ =_ ρ†, ρ ≥_0_,_ Tr [_ρ_] = 1<br>o|_x →k_(_·, x_)_ ∈F_ (canonical feature map)|
|_k_(_x, x′_) = Tr [_ρ_(_x_)_ρ_(_x′_)] =_ ⟨ρ_(_x_)_, ρ_(_x′_)_⟩H_|_k_(_x, x′_) =_ ⟨k_(_·, x_)_, k_(_·, x′_)_⟩F_|
|_F_ =_ {fM|fM_(_·_) = Tr [_ρ_(_·_)_M_]_, M_ =_ M †}_|_F_ = Span (_{k_(_·, x_)_ | x ∈X}_)|



vanishing gradients [ 24 ]. This hinders a theoretical analysis. Note that the non-convexity does not
arise from the fact that the QNN can learn non-linear functions, but rather because the observable _M_ _θ_
depends non-linearly on the parameters. In fact, the QNN functions are linear in the _fixed_ feature
mapping _ρ_ ( _x_ ). Therefore the analogy to classical neural networks is somewhat incomplete.


**Quantum kernels.** The class of functions we consider are RKHS functions where the kernel is
expressed by a quantum computation. The key observation is that (4) is linear in _ρ_ ( _x_ ) . Instead of
optimizing over the parametric function class _F_ Θ, we can define the nonparametric class of functions
_F_ = _{f_ _M_ _|f_ _M_ ( _·_ ) = Tr [ _ρ_ ( _·_ ) _M_ ] _, M_ = _M_ _[†]_ _}_ . [2] To endow this function class with the structure of an
RKHS, observe that the expression Tr [ _ρ_ 1 _ρ_ 2 ] defines a scalar product on density matrices. We then
define kernels via the inner product of data-dependent density matrices:


**Definition 1** (Quantum Kernel [ 7, 8, 11 ]) **.** Let _ρ_ : _x �→_ _ρ_ ( _x_ ) be a fixed feature mapping from _X_ to
density matrices. Then the corresponding _quantum kernel_ is _k_ ( _x, x_ _[′]_ ) = Tr [ _ρ_ ( _x_ ) _ρ_ ( _x_ _[′]_ )].


The Representer Theorem [ 34 ] reduces the empirical risk minimization over the exponentially
large function class _F_ to an optimization problem with a set of parameters whose dimensionality
corresponds to the training set size. Since the ridge regression objective is convex (and so are many
other common objective functions in ML), this can be solved efficiently with a classical computer.


In the described setting, the quantum computer is only used to estimate the kernel. For pure state
encodings, this is done by inverting the data encoding transformation (taking its conjugate transpose)
and measuring the probability that the resulting state equals the initial state _ρ_ 0 . To see this we use
the cyclic property of the trace _k_ ( _x, x_ _[′]_ ) = Tr [ _ρ_ ( _x_ ) _ρ_ ( _x_ _[′]_ )] = Tr � _U_ ( _x_ ) _ρ_ 0 _U_ _[†]_ ( _x_ ) _U_ ( _x_ _[′]_ ) _ρ_ 0 _U_ _[†]_ ( _x_ _[′]_ )� =
Tr �� _U_ _[†]_ ( _x_ _[′]_ ) _U_ ( _x_ ) _ρ_ 0 _U_ _[†]_ ( _x_ ) _U_ ( _x_ _[′]_ )� _ρ_ 0 � . If _ρ_ 0 = ( _|_ 0 _⟩⟨_ 0 _|_ ) _[⊗][d]_, then _k_ ( _x, x_ _[′]_ ) corresponds to the probability of observing every qubit in the ’ 0 ’ state after the initial state was evolved with _U_ _[†]_ ( _x_ _[′]_ ) _U_ ( _x_ ) . To
evaluate the kernel, we thus need to estimate this probability from a finite number of measurements.
For our theoretical analysis we work with the exact value of the kernel and in our experiments we
also simulate the full quantum state. We discuss the difficulties related to measurements in Sec. 4.


**4** **The inductive bias of simple quantum kernels**


We now study the inductive bias for simple quantum kernels and their learning performance. We
first give a high level discussion of a general hurdle for quantum machine learning models to surpass
classical methods and then analyze two specific kernel approaches in more detail.


**Continuity in classical machine learning.** Arguably the most important bias in nonparametric
regression are continuity assumptions on the regression function. This becomes particularly apparent
in, e.g., nearest neighbour regression or random regression forests [ 42 ] where the regression function
is a weighted average of close points. Here we want to emphasize that there is a long list of results
concerning the minimax optimality of kernel methods for regression problems [ 43 – 45 ]. In particular
these results show that asymptotically the convergence of kernel ridge regression of, e.g., Sobolev
functions reaches the statistical limits which also apply to any quantum method.


2 _F_ is defined for a fixed feature mapping _x �→_ _ρ_ ( _x_ ) . Although _M_ is finite dimensional and thus _F_ can be
seen as a parametric function class, we will be interested in the case where _M_ is exponentially large in _d_ and we
can only access functions from _F_ implicitly. Therefore we refer to it as nonparametric class of functions.


5


**A simple quantum kernel.** We now restrict our attention to rather simple kernels to facilitate
a theoretical analysis. As indicated above we consider data in _X ⊂_ R _[d]_ and we assume that the
distribution _µ_ of the data factorizes over the coordinates (i.e. _µ_ can be written as _µ_ = [�] _µ_ _i_ ). This
data is embedded in a _d_ -qubit quantum circuit. Let us emphasize here that the RKHS based on a
quantum state of _d_ -qubits is at most 4 _[d]_ dimensional, i.e., finite dimensional and in the infinite data
limit _n →∞_ standard convergence guarantees from parametric statistics apply. Here we consider
growing dimension _d →∞_, and sample size polynomial in the dimension _n_ = _n_ ( _d_ ) _∈_ Poly( _d_ ) . In
particular the sample size _n ≪_ 4 _[d]_ will be much smaller than the dimension of the feature space and
bounds from the parametric literature do not apply.


Here we consider embeddings where each coordinate is embedded into a single qubit using a map
_ϕ_ _i_ followed by an arbitrary unitary transformation _V_, so that we can express the embedding in
the quantum Hilbert space as _|ψ_ _[V]_ ( _x_ ) _⟩_ = _V_ [�] _|ϕ_ _i_ ( _x_ _i_ ) _⟩_ with corresponding density matrix (feature
map) [3]


_ρ_ _[V]_ ( _x_ ) = _|ψ_ _[V]_ ( _x_ ) _⟩⟨ψ_ _[V]_ ( _x_ ) _| ._ (5)


Note that the corresponding kernel _k_ ( _x, x_ _[′]_ ) = Tr [ _ρ_ ( _x_ ) _ρ_ ( _x_ _[′]_ )] is independent of _V_ and factorizes
_k_ ( _x, x_ _[′]_ ) = Tr [ [�] _ρ_ _i_ ( _x_ _i_ ) [�] _ρ_ _i_ ( _x_ _[′]_ _i_ [)] =][ �] [Tr][ [] _[ρ]_ _[i]_ [(] _[x]_ _[i]_ [)] _[ρ]_ _[i]_ [(] _[x]_ _[′]_ _i_ [)]] [ where] _[ ρ]_ _[i]_ [(] _[x]_ _[i]_ [) =] _[ |][ϕ]_ _[i]_ [(] _[x]_ _[i]_ [)] _[⟩⟨][ϕ]_ _[i]_ [(] _[x]_ _[i]_ [)] _[|]_ [. The]
product structure of the kernel allows us to characterize the RKHS generated by _k_ based on the
one dimensional case. The embedding of a single variable can be parametrized by complex valued
functions _a_ ( _x_ ), _b_ ( _x_ ) as


_|ϕ_ _i_ ( _x_ ) _⟩_ = _a_ ( _x_ ) _|_ 0 _⟩_ + _b_ ( _x_ ) _|_ 1 _⟩._ (6)


One important object characterizing this embedding turns out to be the mean density matrix of this
embedding given by _ρ_ _µ_ _i_ = � _ρ_ _i_ ( _y_ ) _µ_ _i_ (d _y_ ) = � _|ϕ_ _i_ ( _y_ ) _⟩⟨ϕ_ _i_ ( _y_ ) _| µ_ _i_ (d _y_ ) . This can be identified with
the kernel mean embedding of the distribution [ 46 ]. Note that for factorizing base measure _µ_ the
factorization _ρ_ _µ_ = [�] _ρ_ _µ_ _i_ holds. Let us give a concrete example to clarify the setting, see Fig. 1(b).

**Example 2.** [ 11, Example III.1.] We consider the cosine kernel where _a_ ( _x_ ) = cos( _x/_ 2), _b_ ( _x_ ) =
_i_ sin( _x/_ 2) . This embedding can be realized using a single quantum _R_ _X_ ( _x_ ) = exp ( _−i_ _[x]_ 2 _[σ]_ _[x]_ [)] [ gate]

such that _|ψ_ ( _x_ ) _⟩_ = _R_ _X_ ( _x_ ) _|_ 0 _⟩_ = cos( _x/_ 2) _|_ 0 _⟩_ + _i_ sin( _x/_ 2) _|_ 1 _⟩_ . In this case the kernel is given by




_[x]_ _[x]_ _[′]_

2 [) cos(] 2




_[x]_ _[x]_ _[′]_

2 [) sin(] 2




_[x]_

2 ) [2] _._ (7)



_k_ ( _x, x_ _[′]_ ) = _|⟨_ 0 _|R_ _X_ _[†]_ [(] _[x]_ [)] _[R]_ _[X]_ [(] _[x]_ [)] _[|]_ [0] _[⟩|]_ [2] [ =] _[ |]_ [ cos(] _[x]_ 2




_[x]_
2 [) + sin(] 2




_[′]_ _[x][−][x]_ _[′]_

2 [)] _[|]_ [2] [ = cos(] 2



As a reference measure _µ_ we consider the uniform measure on [ _−π, π_ ] . Then the mean density
matrix is the completely mixed state _ρ_ _µ_ = [1] 2 [id] [. For] [ R] _[d]_ [ valued data whose coordinates are encoded]

independently the kernel is given by _k_ ( _x, x_ _[′]_ ) = [�] cos [2] (( _x_ _i_ _−_ _x_ _[′]_ _i_ [)] _[/]_ [2)] [ and] _[ ρ]_ _[µ]_ [ = 2] _[−][d]_ [id] 2 _[d]_ _×_ 2 _[d]_ [. We]
emphasize that due to the kernel trick this kernel can be evaluated classically in runtime _O_ ( _d_ ).


**Quantum RKHS.** We now characterize the RKHS and the eigenvalues of the integral operator
for quantum kernels. The RKHS consists of all functions _f ∈F_ that can be written as _f_ ( _x_ ) =
Tr [ _ρ_ ( _x_ ) _M_ ] where _M ∈_ C [2] _[d]_ _[×]_ [2] _[d]_ is a Hermitian operator. Using this characterization of the finite
dimensional RKHS we can rewrite the infinite dimensional eigenvalue problem of the integral operator
as a finite dimensional problem. The action of the corresponding integral operator on _f_ can be written

as


( _Kf_ )( _x_ ) = _f_ ( _y_ ) _k_ ( _y, x_ ) _µ_ (d _y_ ) = Tr [ _Mρ_ ( _y_ )] Tr [ _ρ_ ( _y_ ) _ρ_ ( _x_ )] _µ_ (d _y_ )
� �

(8)
= Tr [( _M ⊗_ _ρ_ ( _x_ ))( _ρ_ ( _y_ ) _⊗_ _ρ_ ( _y_ ))] _µ_ (d _y_ ) = Tr ( _M ⊗_ _ρ_ ( _x_ )) _ρ_ ( _y_ ) _⊗_ _ρ_ ( _y_ ) _µ_ (d _y_ )
� � � �


We denote the operator _O_ _µ_ = � _ρ_ ( _y_ ) _⊗_ _ρ_ ( _y_ ) _µ_ (d _y_ ) for which Tr [ _O_ _µ_ ] = 1 holds. Then we can write


( _Kf_ )( _x_ ) = Tr [ _O_ _µ_ ( _M ⊗_ _ρ_ ( _x_ ))] = Tr [ _O_ _µ_ ( _M ⊗_ id)(id _⊗_ _ρ_ ( _x_ ))] (9)
= Tr [Tr 1 [ _O_ _µ_ ( _M ⊗_ id)] _ρ_ ( _x_ )]


3 When we can ignore _V_, we simply assume _V_ = id and write _ρ_ ( _x_ ) instead of _ρ_ _V_ ( _x_ ). For the kernel, since
_V_ _[†]_ _V_ = id = _V_ _[†]_ _V_ and due to the cyclic property of the trace we have _k_ _[V]_ ( _x, x_ _[′]_ ) = Tr � _ρ_ _[V]_ ( _x_ ) _ρ_ _[V]_ ( _x_ _[′]_ )� =
Tr � _V ρ_ ( _x_ ) _V_ _[†]_ _V ρ_ ( _x_ _[′]_ ) _V_ _[†]_ [�] = Tr � _V_ _[†]_ _V ρ_ ( _x_ ) _V_ _[†]_ _V ρ_ ( _x_ _[′]_ )� = Tr [ _ρ_ ( _x_ ) _ρ_ ( _x_ _[′]_ )] = _k_ ( _x, x_ _[′]_ ).


6


where Tr 1 [ _·_ ] refers to the partial trace over the first factor. For the definition and a proof of the last
equality we refer to Appendix A. The eigenvalues of _K_ can now be identified with the eigenvalues
of the linear map _T_ _µ_ mapping _M →_ Tr 1 [ _O_ _µ_ ( _M ⊗_ id)] . As shown in the appendix there is an
eigendecomposition such that _T_ _µ_ ( _M_ ) = [�] _λ_ _i_ _A_ _i_ Tr [ _A_ _i_ _M_ ] where _A_ _i_ are orthonormal Hermitian
matrices (for details, a proof and an example we refer to Appendix C). The eigenfunctions of _K_ are
given by _f_ _i_ ( _x_ ) = Tr [ _ρ_ ( _x_ ) _A_ _i_ ].


We now state a bound that controls the largest eigenvalue of the integral operator _K_ in terms of the
eigenvalues of the mean density matrix _ρ_ _µ_ (Proof in Appendix C.2).



**Lemma 1.** _The largest eigenvalue γ_ _max_ _of K satisfies the bound γ_ _max_ _≤_
�



_Tr_ ~~�~~ _ρ_ [2] _µ_ ~~�~~ _._



The lemma above shows that the squared eigenvalues of _K_ are bounded by Tr � _ρ_ [2] _µ_ �, an expression
known as the _purity_ [ 36 ] of the density matrix _ρ_ _µ_, which measures the diversity of the data embedding.
On the other hand the eigenvalues of _K_ are closely related to the learning guarantees of kernel ridge
regression. In particular, standard generalization bounds for kernel ridge regression [ 47 ] become
vacuous when _γ_ _max_ is exponentially smaller than the training sample size (if Tr [ _K_ ] = 1 which holds
for pure state embeddings). The next result shows that this is not just a matter of bounds.

**Theorem 1.** _Suppose the purity of the embeddings_ _ρ_ _µ_ _i_ _satisfies_ _Tr_ � _ρ_ [2] _µ_ _i_ � _≤_ _δ <_ 1 _as the dimension_
_and number of qubits_ _d_ _grows. Furthermore, suppose the training sample size only grows polynomially_
_in_ _d_ _, i.e.,_ _n ≤_ _d_ _[l]_ _for some fixed_ _l ∈_ N _. Then there exists_ _d_ 0 = _d_ 0 ( _δ, l, ε_ ) _such that for all_
_d ≥_ _d_ 0 _no function can be learned using kernel ridge regression with the_ _d_ _-qubit kernel_ _k_ ( _x, x_ _[′]_ ) =
_Tr_ [ _ρ_ ( _x_ ) _ρ_ ( _x_ _[′]_ )] _in the sense that for any f ∈_ _L_ [2] _, with probability at least_ 1 _−_ _ε for all λ ≥_ 0


_R_ ( _f_ [ˆ] _n_ _[λ]_ [)] _[ ≥]_ [(1] _[ −]_ _[ε]_ [)] _[∥][f]_ _[∥]_ [2] _[.]_ (10)


The proof of the theorem can be found in Appendix D. It relies on a general result (Theorem 3 in
Appendix D) which shows that for any (not necessarily quantum) kernel the solution of kernel ridge
regression cannot generalize when the largest eigenvalue in the Mercer decomposition is sufficiently
small (depending on the sample size). Then the proof of Theorem 1 essentially boils down to proving
a bound on the largest eigenvalue using Lemma 1.


Theorem 1 implies that generalization is only possible when the mean embedding of most coordinates
is close to a pure state, i.e. the embedding _x →|ϕ_ _i_ ( _x_ ) _⟩_ is almost constant. To make learning from
data feasible we cannot use the full expressive power of the quantum Hilbert space but instead only
very restricted embeddings allow to learn from data. This generalizes an observation already made
in [ 12 ]. Since also classical methods allow to handle high-dimensional and infinite dimensional
RKHS the same problem occurs for classical kernels where one solution is to adapt the bandwidth of
the kernel to control the expressivity of the RKHS. In principle this is also possible in the quantum
context, e.g., for the cosine embedding.


**Biased kernels.** We have discussed that without any inductive bias, the introduced quantum kernel
cannot learn any function for large _d_ . One suggestion to reduce the expressive power of the kernel
is the use of _projected_ kernels [ 12 ]. They are defined using reduced density matrices given by

˜
_ρ_ _[V]_ _m_ [(] _[x]_ [) =][ Tr] _[m]_ [+1] _[...d]_ � _ρ_ _[V]_ ( _x_ )� where Tr _m_ +1 _...d_ [ _·_ ] denotes the partial trace over qubits _m_ + 1 to
_d_ (definition in Appendix A) . Then they consider the usual quantum kernel for this embedding
_q_ _m_ _[V]_ [(] _[x, x]_ _[′]_ [) =][ Tr] � _ρ_ ˜ _[V]_ _m_ [(] _[x]_ [)˜] _[ρ]_ _[V]_ _m_ [(] _[x]_ _[′]_ [)] � . Physically, this corresponds to just measuring the first _m_ qubits and
the functions _f_ in the RKHS can be written in terms of a hermitian operator _M_ acting on _m_ qubits so
that _f_ ( _x_ ) = Tr � _ρ_ _[V]_ ( _x_ )( _M ⊗_ id)� = Tr � _ρ_ ˜ _[V]_ _m_ [(] _[x]_ [)] _[M]_ � . If _V_ is sufficiently complex it is assumed that _f_
is hard to compute classically [48].


Indeed above procedure reduces the generalization gap. But this comes at the price of an increased
_approximation error_ if the remaining RKHS cannot fully express the target function _f_ _[∗]_ anymore,
i.e., the learned function _underfits_ . Without any reason to believe that the target function is well
represented via the projected kernel, we cannot hope for a performance improvement by simply
reducing the size of the RKHS in an arbitrary way. However, if we _know_ something about the data
generating process than this can lead to a meaningful inductive bias. For the projected kernel this
could be that we _know_ that the target function can be expressed as _f_ _[∗]_ ( _x_ ) = Tr � _ρ_ ˜ _[V]_ _m_ [(] _[x]_ [)] _[M]_ _[ ∗]_ [�], see
Fig. 1. In this case using _q_ _m_ _[V]_ [improves the generalization error without increasing the approximation]
error. To emphasize this, we will henceforth refer to _q_ _m_ _[V]_ [as] _[ biased kernel]_ [.]


7


2 [−1]


2 [−2]


2 [−3]


2 [−4]


2 [−5]


2 [−6]


2 [−7]


2 [−8]





10 [0]


10 [−1]


10 [−2]


10 [−3]


10 [−4]


10 [−5]


10 [−6]


|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|
|---|---|---|---|---|---|---|---|---|
||||||||||
||||||||||
||||||||||
||||||||||



1 2 3 4 5 6 7
Number of Qubits _d_




|Col1|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
||||||||
||||||||
||||||||
||_γ_0<br>~~_γ_~~||||||
|2|3<br>4<br>5<br>6<br>7<br>Number of Qubits_d_<br>1<br>_γ_2<br>_γ_3|3<br>4<br>5<br>6<br>7<br>Number of Qubits_d_<br>1<br>_γ_2<br>_γ_3|3<br>4<br>5<br>6<br>7<br>Number of Qubits_d_<br>1<br>_γ_2<br>_γ_3|3<br>4<br>5<br>6<br>7<br>Number of Qubits_d_<br>1<br>_γ_2<br>_γ_3|3<br>4<br>5<br>6<br>7<br>Number of Qubits_d_<br>1<br>_γ_2<br>_γ_3|3<br>4<br>5<br>6<br>7<br>Number of Qubits_d_<br>1<br>_γ_2<br>_γ_3|



Figure 2: **Left:** Spectral behavior of biased kernel _q_, see Theorem 2b) and Equation (11) **Right:** The
biased kernel _q_, equipped with prior knowledge, easily learns the function for arbitrary number of
qubits and achieves optimal mean squared error (MSE). Models that are ignorant to the structure of
_f_ _[∗]_ fail to learn the function. The classical kernel _k_ rbf and the full quantum kernel overfit (they have
low training error, but large test error). The biased kernel on the wrong qubit _q_ _w_ has litle capacity
with the wrong bias and thus underfits (training and test error essentially overlap).


We now investigate the RKHS for reduced density matrices where _V_ is a Haar-distributed random
unitary matrix (proof in Appendix E).


**Theorem 2.** _Suppose_ _V_ _is distributed according to the Haar measure on the group of unitary_
_matrices. Fix m. Then the following two statements hold:_

_a) The reduced density operator satisfies with high probability_ ˜ _ρ_ _[V]_ _m_ [= 2] _[−][m]_ [id +] _[ O]_ [(2] _[−][d/]_ [2] [)] _[ and the]_
_projected kernel satisfies with high probability q_ _m_ _[V]_ [(] _[x, x]_ _[′]_ [) = 2] _[−][m]_ [ +] _[ O]_ [(2] _[−][d/]_ [2] [)] _[ as][ d][ →∞][.]_

_b) Let_ _T_ _µ,m_ _[V]_ _[denote the linear integral operator for the kernel]_ _[ q]_ _m_ _[V]_ _[as defined above. Then the]_
_averaged operator_ E _V_ � _T_ _µ,m_ _[V]_ � _has one eigenvalue_ 2 _[−][m]_ + _O_ (2 _[−]_ [2] _[d]_ ) _whose eigenfunction is constant_
_(up to higher order terms of order O_ (2 _[−]_ [2] _[d]_ ) _and_ 2 [2] _[m]_ _−_ 1 _eigenvalues_ 2 _[−][m][−][d]_ + _O_ (2 _[−]_ [2] _[d]_ ) _._


The averaged integral operator in the second part of the result is not directly meaningful, however it
gives some indication of the behavior of the operators _T_ _µ,m_ _[V]_ [. In particular, we expect a similar result]
to hold for most _V_ if the mean embedding _ρ_ _µ_ is sufficiently mixed. A proof of this result would
require us to bound the variance of the matrix elements of _T_ _µ,m_ _[V]_ [which is possible using standard]
formula for expectations of polynomials over the unitary group but lengthy.


Note that the dimension of the RKHS for the biased kernel _q_ _m_ _[V]_ [with] _[ m]_ [-qubits is bounded by] [ 4] _[m]_ [. This]
implies that learning is possible when projecting to sufficiently low dimensional biased kernels such
that the training sample size satisfies _n_ ≳ 4 _[m]_ _≥_ dim( _F_ ).


Let us now focus on the case _m_ = 1, that is the biased kernel is solely defined via the first qubit.
Assuming that Theorem 2b) also holds for fixed _V_ we can assume that the biased kernel has the form



3
_q_ ( _x, x_ _[′]_ ) _≡_ _q_ 1 _[V]_ [(] _[x, x]_ _[′]_ [) =] _[ γ]_ [0] _[φ]_ [0] [(] _[x]_ [)] _[φ]_ [0] [(] _[x]_ _[′]_ [) +] � _i_ =1 _[γ]_ _[i]_ _[φ]_ _[i]_ [(] _[x]_ [)] _[φ]_ _[i]_ [(] _[x]_ _[′]_ [)] _[,]_ (11)



where _γ_ 0 = 1 _/_ 2 + _O_ (2 _[−]_ [2] _[d]_ ) and _φ_ 0 ( _x_ ) = 1 is the constant function up to terms of order _O_ (2 _[−]_ [2] _[d]_ ) .
For _i_ = 1 _,_ 2 _,_ 3 we have _γ_ _i_ = _O_ (2 _[−][d][−]_ [1] ) = _O_ (2 _[−][d]_ ) (Fig. 2) and _φ_ _i_ is a function that conjectured to
be exponentially hard in _d_ to compute classically [ 48 ]. It is thus impossible to include a bias towards
those three eigenfunctions classically. On the other hand we can include a strong bias towards the
constant eigenfunction also classically. The straightforward way to do this is to center the data in the
RKHS (see Appendix B for details).


**Barren plateaus.** Another conclusion from Theorem 2a) is that the fluctuations of the reduced
density matrix around its mean are exponentially vanishing in the number of qubits. In practice the
value of the kernel would be determined by measurements and exponentially many measurements
are necessary to obtain exponential accuracy of the kernel function. Therefore the theorem suggests
that it is not possible in practice to learn anything beyond the constant function from generic biased


8


kernels for (modestly) large values of _d_ . This observation is closely related to the fact that for many
quantum neural networks architectures, the gradient of the parameters with respect to the loss is
exponentially vanishing with the system size _d_, a phenomenon known as _Barren plateaus_ [24, 49].


**5** **Experiments**


Since for small _d_ we can simulate the biased kernel efficiently, we illustrate our theoretical findings in
the following experiments. Our implementation, building on standard open source packages [ 50, 51 ],
is available online. [4] We consider the case described above where we _know_ that the data was generated
by measuring an observable on the first qubit, i.e., _f_ _[∗]_ ( _x_ ) = Tr � _ρ_ ˜ _[V]_ 1 [(] _[x]_ [)] _[M]_ _[ ∗]_ [�], but we do not know
_M_ _[∗]_, see Fig. 1. We use the full kernel _k_ and the biased kernel _q_ for the case _m_ = 1 . To show the
effect of selecting the wrong bias, we also include the behavior of a biased kernel defined only on the
second qubit, denoted as _q_ _w_ . As a classical reference we also include the performance of a radial
basis function kernel _k_ rbf ( _x, x_ _[′]_ ) = exp( _−∥x −_ _x_ _[′]_ _∥_ [2] _/_ 2) . For the experiments we fix a single qubit
observable _M_ _[∗]_ = _σ_ _z_ and perform the experiment for varying number _d_ of qubits. First we draw a
random unitary _V_ . The dataset is then generated by drawing _N_ = 200 realizations _{x_ [(] _[i]_ [)] _}_ _[N]_ _i_ =1 [from the]
_d_ dimensional uniform distribution on [0 _,_ 2 _π_ ] _[d]_ . We then define the labels as _y_ [(] _[i]_ [)] = _cf_ _[∗]_ ( _x_ [(] _[i]_ [)] ) + _ϵ_ [(] _[i]_ [)],
where _f_ _[∗]_ ( _x_ ) = Tr � _ρ_ ˜ _[V]_ ( _x_ ) _σ_ _z_ �, _ϵ_ [(] _[i]_ [)] is Gaussian noise with Var[ _ϵ_ ] = 10 _[−]_ [4], and _c_ is chosen such that
Var[ _f_ ( _X_ )] = 1 . Keeping the variances fixed ensures that we can interpret the behavior for varying _d_ .


We first verify our findings from Theorem 2b) and Equation (11) by estimating the spectrum of _q_ .
Fig. 2 (left) shows that Theorem 2b) also holds for individual _V_ with high probability. We then
use 2 _/_ 3 of the data for training kernel ridge regression (we fit the mean seperately) with preset
regularization, and use 1 _/_ 3 to estimate the test error. We average the results over ten random seeds
(random _V_, _x_ [(] _[i]_ [)] _, ϵ_ [(] _[i]_ [)] ) and results are reported in the right panel of Fig. 2. This showcases that as
the number of qubits increases, it is impossible to learn _f_ _[∗]_ without the appropriate spectral bias. _k_
and _k_ rbf have too little bias and overfit, whereas _q_ _w_ has the wrong bias and severly underfits. The
performance of _q_ _w_ underlines that randomly biasing the kernel does not significantly improve the
performance over the full kernel _k_ . In the appendix we show that this is not due to a bad choice of
regularization, by reporting cherry-picked results over a range of regularizations.


To further illustrate the spectral properties, we empirically estimate the kernel target alignment [30]
and the task-model alignment [ 32 ] that we introduced in Sec. 2. By using the centered kernel matrix
(see App. B) and centering the data we can ignore the first eigenvalue in (11) corresponding the
constant function. In Figure 3 (left) we show the empirical (centered) kernel target alignment for 50
random seeds. The biased kernel is the only one well aligned with the task. The right panel of Fig. 3
shows the task model alignment. This shows that _f_ _[∗]_ can be completely expressed with the first four
components of the biased kernel, while the other kernels essentially need the entire spectrum (we
use a sample size of 200, hence the empirical kernel matrix is only 200 dimensional) and thus are
unable to learn. Note that the kernel _q_ _w_ is four dimensional, and so higher contributions correspond
to functions outside its RKHS that it actually cannot even learn at all.


**6** **Discussion**


We provided an analysis of the reproducing kernel Hilbert space (RKHS) and the inductive bias
of quantum kernel methods. Rather than the dimensionality of the RKHS, its spectral properties
determine whether learning is feasible. Working with exponentially large RKHS comes with the risk
of having a correspondingly small inductive bias. This situation indeed occurs for naive quantum
encodings, and hinders learning unless datasets are of exponential size. To enable learning, we necessarily need to consider models with a stronger inductive bias. Encoding a bias towards continuous
functions is likely not a promising path for a quantum advantage, as this is where classical machine
learning models excel.


Our results suggest that we can only achieve a quantum advantage if we _know_ something about the
data generating process and cannot efficiently encode this classically, yet are able use this information
to bias the quantum model. We indeed observe an exponential advantage in the case where we know
that the data comes from a single qubit observable and constrain the RKHS accordingly. However,


4 `[https://github.com/jmkuebler/quantumbias](https://github.com/jmkuebler/quantumbias)`


9


1.0


0.8


0.6


0.4


0.2


0.0



40


30


20


10


0






|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|q<br>k<br>krbf|
|---|---|---|---|---|---|---|---|---|
|||||||||_qw_|
||||||||||
||||||||||
||||||||||


|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
|||||
||||_q_<br>|
||||~~_k_~~<br>_krbf_<br>_qw_|



0.0 0.2 0.4 0.6 0.8 1.0
Kernel Alignment _A_



10 [0] 10 [1] 10 [2]


_i_



Figure 3: Histogram of the kernel target alignment over 50 runs (left) and task model alignment
(right) for _d_ = 7.


we find that evaluating the kernel requires exponentially many measurements, an issue related to
Barren Plateaus encountered in quantum neural networks.


With fully error-corrected quantum computers it becomes feasible to define kernels with a strong bias
that do not require exponentially many measurements. An example of this kind was recently presented
by Liu et al. [10] : here one knows that the target function is extremely simple after computing the
discrete logarithm. A quantum computer is able to encode this inductive bias by using an efficient
algorithm for computing the discrete logarithm.


However, even for fully coherent quantum computers it is unclear how we can reasonably encode a
strong inductive bias about a classical dataset (e.g., images of cancer cells, climate-data, etc.). The
situation might be better when working with _quantum data_, i.e., data that is collected via observations
of systems at a quantum mechanical scale. To summarize, we conclude that there is no indication
that quantum machine learning will substantially improve supervised learning on classical datasets.


**Acknowledgments and Disclosure of Funding**


The authors thank the anonymous reviewers for their helpful comments that made the theorems
and their proofs more accessible. This work was in part supported by the German Federal Ministry
of Education and Research (BMBF) through the Tübingen AI Center (FKZ: 01IS18039B) and the
Machine Learning Cluster of Excellence, EXC number 2064/1 – Project number 390727645.


**References**


[1] John Preskill. Quantum computing in the NISQ era and beyond. _Quantum_, 2:79, 2018.


[2] Frank Arute, Kunal Arya, Ryan Babbush, et al. Quantum supremacy using a programmable
superconducting processor. _Nature_, 574(7779):505–510, 2019.


[3] Alberto Peruzzo, Jarrod McClean, Peter Shadbolt, Man-Hong Yung, Xiao-Qi Zhou, Peter J
Love, Alán Aspuru-Guzik, and Jeremy L O’brien. A variational eigenvalue solver on a photonic
quantum processor. _Nature Communications_, 5:4213, 2014.


[4] Edward Farhi, Jeffrey Goldstone, and Sam Gutmann. A quantum approximate optimization
algorithm. _arXiv:1411.4028_, 2014.


[5] Kosuke Mitarai, Makoto Negoro, Masahiro Kitagawa, and Keisuke Fujii. Quantum circuit
learning. _Phys. Rev. A_, 98:032309, 2018.


[6] Edward Farhi and Hartmut Neven. Classification with quantum neural networks on near term
processors. _arXiv:1802.06002_, 2018.


10


[7] Vojtˇech Havlíˇcek, Antonio D Córcoles, Kristan Temme, Aram W Harrow, Abhinav Kandala,
Jerry M Chow, and Jay M Gambetta. Supervised learning with quantum-enhanced feature
spaces. _Nature_, 567(7747):209, 2019.


[8] Maria Schuld and Nathan Killoran. Quantum machine learning in feature Hilbert spaces. _Phys._
_Rev. Lett._, 122:040504, 2019.


[9] Jonas M. Kübler, Krikamol Muandet, and Bernhard Schölkopf. Quantum mean embedding of
probability distributions. _Phys. Rev. Research_, 1:033159, 2019.


[10] Yunchao Liu, Srinivasan Arunachalam, and Kristan Temme. A rigorous and robust quantum
speed-up in supervised machine learning. _Nature Physics_, 17(9):1013–1017, 2021.


[11] Maria Schuld. Quantum machine learning models are kernel methods. _arXiv:2101.11020_, 2021.


[12] Hsin-Yuan Huang, Michael Broughton, Masoud Mohseni, Ryan Babbush, Sergio Boixo, Hartmut Neven, and Jarrod R. McClean. Power of data in quantum machine learning. _Nature_
_Communications_, 12(1):2631, 2021.


[13] Aram W Harrow, Avinatan Hassidim, and Seth Lloyd. Quantum algorithm for linear systems of
equations. _Phys. Rev. Lett._, 103(15):150502, 2009.


[14] Patrick Rebentrost, Masoud Mohseni, and Seth Lloyd. Quantum support vector machine for big
data classification. _Phys. Rev. Lett._, 113(13), 2014.


[15] Nathan Wiebe, Daniel Braun, and Seth Lloyd. Quantum algorithm for data fitting. _Phys. Rev._
_Lett._, 109:050505, 2012.


[16] Seth Lloyd, Masoud Mohseni, and Patrick Rebentrost. Quantum algorithms for supervised and
unsupervised machine learning. _arXiv:1307.0411_, 2013.


[17] Iordanis Kerenidis and Anupam Prakash. Quantum gradient descent for linear systems and least
squares. _Phys. Rev. A_, 101:022316, 2020.


[18] Carlo Ciliberto, Andrea Rocchetto, Alessandro Rudi, and Leonard Wossnig. Statistical limits of
supervised quantum learning. _Physical Review A_, 102(4), 2020.


[19] Gian Giacomo Guerreschi and Mikhail Smelyanskiy. Practical optimization for hybrid quantumclassical algorithms. _arXiv:1701.01450_, 2017.


[20] Maria Schuld, Ville Bergholm, Christian Gogolin, Josh Izaac, and Nathan Killoran. Evaluating
analytic gradients on quantum hardware. _Physical Review A_, 99(3):032331, 2019.


[21] James Stokes, Josh Izaac, Nathan Killoran, and Giuseppe Carleo. Quantum natural gradient.
_Quantum_, 4:269, 2020.


[22] Ryan Sweke, Frederik Wilde, Johannes Jakob Meyer, Maria Schuld, Paul K Fährmann,
Barthélémy Meynard-Piganeau, and Jens Eisert. Stochastic gradient descent for hybrid quantumclassical optimization. _Quantum_, 4:314, 2020.


[23] Jonas M. Kübler, Andrew Arrasmith, Lukasz Cincio, and Patrick J. Coles. An Adaptive
Optimizer for Measurement-Frugal Variational Algorithms. _Quantum_, 4:263, 2020.


[24] Jarred R. McClean, Sergio Boixo, Vadim N. Smelyanskiy, Ryan Babbush, and Hartmut Neven.
Barren plateaus in quantum neural network training landscapes. _Nature Communications_, 9:
4812, 2018.


[25] Bernhard Schölkopf and Alexander J. Smola. _Learning with Kernels_ . MIT Press, Cambridge,
MA, USA, 2002.


[26] John Shawe-Taylor and Nello Cristianini. _Kernel Methods for Pattern Analysis_ . Cambridge
University Press, 2004.


[27] Maria Schuld, Ryan Sweke, and Johannes Jakob Meyer. Effect of data encoding on the
expressive power of variational quantum-machine-learning models. _Phys. Rev. A_, 103:032430,
2021.


11


[28] Hsin-Yuan Huang, Richard Kueng, and John Preskill. Information-theoretic bounds on quantum
advantage in machine learning. _Phys. Rev. Lett._, 126:190505, 2021.


[29] Robert C. Williamson, Alexander J. Smola, and Bernhard Schölkopf. Generalization performance of regularization networks and support vector machines via entropy numbers of compact
operators. _IEEE Transactions on Information Theory_, 47(6):2516–2532, 2001.


[30] Nello Cristianini, Jaz Kandola, Andre Elisseeff, and John Shawe-Taylor. On kernel target
alignment. In Dawn E. Holmes and Lakhmi C. Jain, editors, _Innovations in Machine Learning:_
_Theory and Applications_, pages 205–256. Springer Berlin Heidelberg, 2006.


[31] Corinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh. Algorithms for learning kernels
based on centered alignment. _Journal of Machine Learning Research_, 13(28):795–828, 2012.


[32] Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model
alignment explain generalization in kernel regression and infinitely wide neural networks.
_Nature Communications_, 12(1):2914, 2021.


[33] Arthur Jacot, Berfin Simsek, Francesco Spadaro, Clement Hongler, and Franck Gabriel. Kernel
alignment risk estimator: Risk prediction from training data. In _NeurIPS_, 2020.


[34] Bernhard Schölkopf, Ralf Herbrich, and Alexander J. Smola. A generalized representer theorem.
In _COLT_, 2001.


[35] Thomas Hubregtsen, David Wierichs, Elies Gil-Fuster, Peter-Jan H. S. Derks, Paul K.
Faehrmann, and Johannes Jakob Meyer. Training quantum embedding kernels on near-term
quantum computers. _arXiv:2105.02276_, 2021.


[36] Michael A. Nielsen and Isaac L. Chuang. _Quantum Computation and Quantum Information:_
_10th Anniversary Edition_ . Cambridge University Press, 2010.


[37] Xinbiao Wang, Yuxuan Du, Yong Luo, and Dacheng Tao. Towards understanding the power of
quantum kernels in the NISQ era. _Quantum_, 5:531, 2021.


[38] Evan Peters, João Caldeira, Alan Ho, Stefan Leichenauer, Masoud Mohseni, Hartmut Neven,
Panagiotis Spentzouris, Doug Strain, and Gabriel N. Perdue. Machine learning of high dimensional data on a noisy quantum processor. _arXiv:2101.09581_, 2021.


[39] Adrián Pérez-Salinas, Alba Cervera-Lierta, Elies Gil-Fuster, and José I. Latorre. Data reuploading for a universal quantum classifier. _Quantum_, 4:226, 2020.


[40] M. Cerezo, Andrew Arrasmith, Ryan Babbush, Simon C. Benjamin, Suguru Endo, Keisuke
Fujii, Jarrod R. McClean, Kosuke Mitarai, Xiao Yuan, Lukasz Cincio, and Patrick J. Coles.
Variational quantum algorithms. _Nature Reviews Physics_, 3(9):625–644, 2021.


[41] Kishor Bharti, Alba Cervera-Lierta, Thi Ha Kyaw, et al. Noisy intermediate-scale quantum
(nisq) algorithms. _arXiv:2101.08448_, 2021.


[42] Leo Breiman. Random forests. _Mach. Learn._, 45(1):5–32, 2001.


[43] Andrea Caponnetto and Ernesto De Vito. Optimal rates for the regularized least-squares
algorithm. _Foundations of Computational Mathematics_, 7(3):331–368, 2007.


[44] Ingo Steinwart, Don R. Hush, and Clint Scovel. Optimal rates for regularized least squares
regression. In _COLT_, 2009.


[45] Simon Fischer and Ingo Steinwart. Sobolev norm learning rates for regularized least-squares
algorithms. _Journal of Machine Learning Research_, 21(205):1–38, 2020.


[46] Krikamol Muandet, Kenji Fukumizu, Bharath K. Sriperumbudur, and Bernhard Schölkopf.
Kernel mean embedding of distributions: A review and beyond. _Found. Trends Mach. Learn._,
10(1-2):1–141, 2017.


[47] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. _Foundations of Machine Learning_ .
Adaptive computation and machine learning. MIT Press, 2012.


12


[48] Aram W. Harrow and Ashley Montanaro. Quantum computational supremacy. _Nature_, 549
(7671):203–209, 2017.


[49] M. Cerezo, Akira Sone, Tyler Volkoff, Lukasz Cincio, and Patrick J. Coles. Cost function
dependent barren plateaus in shallow parametrized quantum circuits. _Nature Communications_,
12(1):1791, 2021.


[50] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, et al. Scikit-learn: Machine learning
in Python. _Journal of Machine Learning Research_, 12:2825–2830, 2011.


[51] Ville Bergholm, Josh Izaac, Maria Schuld, Christian Gogolin, and Nathan Killoran. Pennylane:
Automatic differentiation of hybrid quantum-classical computations. _arXiv:1811.04968_, 2018.


[52] Vern I. Paulsen and Mrinal Raghupathi. _An Introduction to the Theory of Reproducing Kernel_
_Hilbert Spaces_ . Cambridge Studies in Advanced Mathematics. Cambridge University Press,
2016.


[53] Zbigniew Puchała and Jarosław A. Miszczak. Symbolic integration with respect to the haar
measure on the unitary groups. _Bulletin of the Polish Academy of Sciences: Technical Sciences_,
65(No 1):21–27, 2017.


[54] Christoph Dankert, Richard Cleve, Joseph Emerson, and Etera Livine. Exact and approximate
unitary 2-designs and their application to fidelity estimation. _Phys. Rev. A_, 80:012304, 2009.


13


## The Inductive Bias of Quantum Kernels

### Supplementary Material

**A** **Partial trace in quantum mechanics**


Here we provide the definition of the partial trace used for the biased quantum kernels. For details
we refer to [ 36 ]. The state space of the union of two quantum systems with state space _H_ 1 and _H_ 2
is given by the tensor product _H_ 1 _⊗H_ 2 . A general mixed state is described by a density matrix
_ρ_ 12 which is hermitian positive linear operator on _H_ 1 _⊗H_ 2 with Tr [ _ρ_ 12 ] = 1 . The state _ρ_ 1 on the
subsystem _H_ 1 is obtained by the partial trace operation _ρ_ 1 = Tr 2 [ _ρ_ 12 ] . The partial trace can be
defined as the linear map Tr 2 : _L_ ( _H_ 1 _⊗H_ 2 ) _→L_ ( _H_ 1 ) that satisfies


Tr 2 [ _S ⊗_ _T_ ] = Tr [ _T_ ] _S_ (12)


for all _S ∈L_ ( _H_ 1 ), _T ∈L_ ( _H_ 2 ) . It can be shown that this map exists and is unique. Picking a basis
on _H_ 1 and _H_ 2 we consider the tensor product basis on _H_ 1 _⊗H_ 2 . In coordinates given by this basis
we can write



( _ρ_ 1 ) _i_ 1 _j_ 1 = Tr 2 [ _ρ_ 12 ] _i_ 1 _j_ 1 =



dim( _H_ 2 )
� ( _ρ_ 12 ) _i_ 1 _k,j_ 1 _k_ _._ (13)


_k_ =1



For completeness and to illustrate the handling of the partial trace we prove the last equality in (9) .
We want to show that for _S ∈L_ ( _H_ 1 _⊗H_ 2 ) and _T ∈L_ ( _H_ 1 ) the identity


Tr [ _S_ ( _T ⊗_ id] = Tr [Tr 2 [ _S_ ] _T_ ] (14)


holds. We assume first that _S_ = _A ⊗_ _B_ for some _A ∈L_ ( _H_ 1 ) and _B ∈L_ ( _H_ 2 ). Then, by definition,


Tr [Tr 2 [ _S_ ] _T_ ] = Tr [Tr 2 [ _A ⊗_ _B_ ] _T_ ] = Tr [ _AT_ ] Tr [ _B_ ]
(15)
= Tr [ _AT ⊗_ _B_ ] = Tr [( _A ⊗_ _B_ )( _T ⊗_ id)] = Tr [ _S_ ( _T ⊗_ id] _._


Here we used that the trace of a tensor product is the product of the traces. For general _S_ the statement
now follows from the linearity of both sides in _S_ .


**B** **General results about RKHS**


In this section we briefly discuss basic results on centering in RKHS and the RKHS of tensor product
kernels.


**B.1** **Centering in the RKHS**


As shown in Section 4, the constant function plays a special role for typical biased kernels as the
corresponding eigenvalue is much larger than the remaining eigenvalues. Clearly, it is also possible
classically to treat the constant function separately. To do so, it is natural to center the data by
subtracting the mean ¯ _y_ = _n_ _[−]_ [1] [ �] _[n]_ _i_ =1 _[y]_ _[i]_ [ and to consider the] _[ centered]_ [ kernel. This corresponds to]
putting no penalty on the constant function which is also common in linear models where no penalty
is put on the intercept. Formally, for a kernel _k_, the centered kernel is defined as


_k_ _c_ ( _x, x_ _[′]_ ) = _k_ ( _x, x_ _[′]_ ) _−_ E _X_ [ _k_ ( _X, x_ _[′]_ )] _−_ E _X_ _′_ [ _k_ ( _x, X_ _[′]_ )] + E _X,X_ _′_ [ _k_ ( _X, X_ _[′]_ )] _._ (16)



In analogy we can center the kernel matrix as _K_ _c_ ( _X, X_ ) = �id _−_ _n_ [1]



_n_ [1] **[11]** _[⊤]_ [�] _K_ ( _X, X_ ) �id _−_ _n_ [1]



In analogy we can center the kernel matrix as _K_ _c_ ( _X, X_ ) = �id _−_ _n_ [1] **[11]** _[⊤]_ [�] _K_ ( _X, X_ ) �id _−_ _n_ [1] **[11]** _[⊤]_ [�],

where **1** is a vector of all ones.


Let _k_ be a kernel with Mercer decomposition


_k_ ( _x, x_ _[′]_ ) = � _γ_ _i_ _φ_ _i_ ( _x_ ) _φ_ _i_ ( _x_ _[′]_ ) _,_ (17)


and define _a_ _i_ = � _φ_ _i_ ( _x_ ) _µ_ (d _x_ ). Then the centered kernel can be written as


_k_ _c_ ( _x, x_ _[′]_ ) = � _γ_ _i_ ( _φ_ _i_ ( _x_ ) _−_ _a_ _i_ )( _φ_ _i_ ( _x_ _[′]_ ) _−_ _a_ _i_ ) _._ (18)


14


To make things explicit let us focus on the biased kernel of Equation (11) . Ignoring terms of order
_O_ (2 _[−]_ [2] _[d]_ ), the constant function is an eigenfunction of the kernel. In such a case centering corresponds
to setting the corresponding eigenvalue _γ_ 0 to zero, while the other terms in the spectral decomposition
are invariant under centering (by orthogonality we have _a_ _i_ = 0 for _i ̸_ = 0 ). The centered biased kernel
of Eq. (11) is thus



_q_ _c_ ( _x, x_ _[′]_ ) =



3
� _γ_ _i_ _φ_ _i_ ( _x_ ) _φ_ _i_ ( _x_ _[′]_ ) _._ (19)


_i_ =1



By Theorem 2 we expect that all the eigenvalues of the centered biased kernel are similarly large.
Further we know that the centered part of the target function can completely be expressed in terms of
the eigenfunctions of the centered biased kernel _f_ _[∗]_ ( _x_ ) _−f_ [¯] _[∗]_ = [�] [3] _i_ =1 _[a]_ _[i]_ _[φ]_ _[i]_ [(] _[x]_ [)] [, where] [ ¯] _[f]_ _[ ∗]_ [=][ E][ [] _[f]_ _[ ∗]_ [(] _[X]_ [)]] [.]
Let us assume that all the three eigenvalues are completely equal. Then we can compute the kernel
target alignment of Eq. (2)



_A_ ( _q_ _c_ _, f_ _[∗]_ _−_ _f_ [¯] _[∗]_ ) = � 3 _i_ =1 _[γ][a]_ _i_ [2] = _γ_ [�] [3] _i_ =1 _[a]_ _i_ [2]
( ~~[�]~~ [3] _i_ =1 _[γ]_ [2] [)] [1] _[/]_ [2] ~~[ �]~~ [3] _i_ =1 _[a]_ _i_ [2] ~~_√_~~ 3 _γ_ ~~[�]~~ [3] _i_ =1




[�] _i_ =1 _[a]_ _i_ = 1

3 _γ_ ~~[�]~~ [3] _i_ =1 _[a]_ _i_ [2] ~~_√_~~



3 _[≈]_ [0] _[.]_ [58] _[.]_ (20)



We emphasize that this expectation is in good accordance with the results of our experiments reported
in Fig. 3. Further, note that computing the kernel target alignment after centering is quite common in
the kernel literature and is used to optimize the kernel function [31].


**B.2** **Tensor product of kernels**


In this section we describe the construction of product kernels on product spaces. More details can be
found in any textbook on RKHS [ 25 ]. Let ( _X_ 1 _, k_ 1 ) and ( _X_ 2 _, k_ 2 ) be two spaces with positive definite
kernels with RKHS _F_ 1 and _F_ 2 . Then the function


_k_ (( _x_ 1 _, x_ 2 ) _,_ ( _y_ 1 _, y_ 2 )) = _k_ 1 ( _x_ 1 _, y_ 1 ) _k_ 2 ( _x_ 2 _, y_ 2 ) (21)


defines a positive definite kernel on _X_ 1 _× X_ 2 and the RKHS is given by _{f_ 1 ( _x_ 1 ) _f_ 2 ( _x_ 2 ) : _f_ 1 _∈_
_F_ 1 _, f_ 2 _∈F_ 2 _}_ . Morevoer, if we are given a product measure _µ_ = _µ_ 1 _⊗_ _µ_ 2 on _X_ 1 _× X_ 2 then the
integral operator for the kernel _k_ factorizes, i.e., for functions _f_ ( _x_ 1 _, x_ 2 ) = _f_ 1 ( _x_ 1 ) _f_ 2 ( _x_ 2 )


( _Kf_ )( _x_ 1 _, x_ 2 ) = _f_ ( _y_ ) _k_ ( _y, x_ ) _µ_ (d _y_ )
�



= _f_ 1 ( _y_ 1 ) _k_ 1 ( _y_ 1 _, x_ 1 ) _µ_ 1 (d _x_ 1 ) _f_ 2 ( _y_ 2 ) _k_ 2 ( _y_ 2 _, x_ 2 ) _µ_ 1 (d _x_ 2 )
� �

= ( _K_ 1 _f_ 1 )( _x_ 1 )( _K_ 2 _f_ 2 )( _x_ 2 ) _._



(22)



Therefore the eigenvalue problems of the integral operators decouple and the eigenvalues of _K_ are
given by _{γ_ [1] _γ_ [2] : _γ_ [1] _∈_ _E_ 1 _, γ_ [2] _∈_ _E_ 2 _}_ where _E_ _i_ denotes the eigenvalues of _K_ _i_ .


It can be derived from the results above that the RKHS of the product kernel _k_ ( _x, x_ _[′]_ ) =
_k_ 1 ( _x, x_ _[′]_ ) _k_ 2 ( _x, x_ _[′]_ ) on _X_ is given by _{f_ 1 ( _x_ ) _f_ 2 ( _x_ ) : _f_ 1 _∈F_ 1 _, f_ 2 _∈F_ 2 _}_ where _F_ _i_ denotes the
RKHS of ( _X, k_ _i_ ). There is no simple relation for the integral operators.


**C** **More details on quantum kernels for classical data**


In this section we analyze in more detail the properties of quantum kernel methods for classical data.


**C.1** **Description of the RKHS**


To understand the quantum kernel better we give a description of the RKHS for the quantum kernels.
We consider the one-qubit embedding _x →_ _a_ ( _x_ ) _|_ 0 _⟩_ + _b_ ( _x_ ) _|_ 1 _⟩_ . The RKHS _F_ [˜] corresponding to the
(non-physical) kernel _k_ [˜] ( _x, y_ ) = _⟨ϕ_ ( _x_ ) _, ϕ_ ( _y_ ) _⟩_ is then generated by _a_ ( _x_ ) _, b_ ( _x_ ) . Moreover, the RKHS
corresponding to the physical kernel _k_ ( _x, x_ _[′]_ ) = Tr [ _ρ_ ( _x_ ) _ρ_ ( _x_ _[′]_ ))] = _|⟨ϕ_ ( _x_ ) _, ϕ_ ( _x_ _[′]_ ) _⟩|_ [2] = _|k_ [˜] ( _x, x_ _[′]_ ) _|_ [2] =

˜ ˜ ˜
_k_ ( _x, x_ _[′]_ ) _k_ [¯] ( _x, x_ _[′]_ ) is the vector space _F_ generated by _{f ·_ ¯ _g_ : _f, g ∈_ _F_ _}_ [ 52 ] (to obtain the real valued
RKHS which is more relevant in the learning theoretic setting we consider the real and imaginary


15


part). This result can also be obtained by looking at the feature map _x →_ _ρ_ ( _x_ ) of the physical kernel
directly. When we consider data from R _[d]_ where all dimensions are encoded independently in a single
qubit the resulting RKHS has the tensor product structure _F_ = [�] _F_ _i_ where _F_ _i_ are the RKHS for the
single coordinate embeddings.


**C.2** **Proof of Lemma 1**


Here we analyze the integral operators in a bit more detail and prove Lemma 1. For the proof of
Lemma 1 we need to briefly look again at the simpler non-physical kernel _k_ [˜] ( _x, y_ ) = _⟨ϕ_ ( _x_ ) _, ϕ_ ( _y_ ) _⟩_
and its integral operator. Suppose data has distribution _µ_ on R . We consider the integral operator _K_ [˜]
acting on _f_ ( _x_ ) = _⟨ω, ϕ_ ( _x_ ) _⟩_ defined by


˜
_Kf_ ( _x_ ) = _f_ ( _y_ ) _k_ [˜] ( _y, x_ ) _µ_ (d _y_ ) = _⟨ω, ϕ_ ( _y_ ) _⟩⟨ϕ_ ( _y_ ) _, ϕ_ ( _x_ ) _⟩_ _µ_ (d _y_ ) = _⟨ω, ρ_ _µ_ _ϕ_ ( _x_ ) _⟩_ (23)
� �


where _ρ_ _µ_ = � _|ϕ_ ( _y_ ) _⟩⟨ϕ_ ( _y_ ) _| µ_ (d _y_ ) denotes the mean density matrix associated with the measure _µ_ .
We observe that the eigenvalues ˜ _γ_ _i_ of _K_ [˜] agree with the the eigenvalues of the density matrix _ρ_ _µ_ . In
particular we conclude


_∥K_ [˜] _∥_ [2] _HS_ [=] � _γ_ ˜ _i_ [2] [=] _[ ∥][ρ]_ _[µ]_ _[∥]_ [2] _HS_ [=][ Tr] � _ρ_ [2] _µ_ � (24)


where _∥·∥_ _HS_ denotes the Hilbert-Schmidt norm (which for symmetric matrices agrees with the
Frobenius norm). This observation corresponds to the fact that for the linear kernel the eigenvalues of
the integral operator agree with the eigenvalues of the covariance matrix.


Now we can give the simple proof of Lemma 1. For convenience we restate the lemma.


**Lemma 2** (Lemma 1 in the main part) **.** _The largest eigenvalue_ _γ_ _max_ _of_ _K_ _satisfies the bound_



_γ_ _max_ _≤_
�



_Tr_ ~~�~~ _ρ_ [2] _µ_ ~~�~~ _._



_Proof._ We observe, denoting the constant function with value 1 by **1**,

**1** ( _x_ ) _k_ ( _x, y_ ) **1** ( _y_ ) _µ_ (d _x_ ) _µ_ (d _y_ ) = _|k_ [˜] ( _x, y_ ) _|_ [2] _µ_ (d _x_ ) _µ_ (d _y_ ) = _∥K_ [˜] _∥_ [2] _HS_ [=] _[ ∥][ρ]_ _[µ]_ _[∥]_ [2] _HS_ [=][ Tr] � _ρ_ [2] _µ_ �
� �

(25)


where we used (24) in the last two steps. Suppose that _f_ is a normalized eigenfunction for the
eigenvalue _γ_ _max_ . From the Mercer decomposition we obtain


1 = _K_ ( _x, x_ ) _≥_ _γ_ _max_ _f_ ( _x_ ) [2] _._ (26)


Hence _f_ is bounded by _[√]_ ~~_γ_~~ _max_ ~~_−_~~ 1 and we conclude that


_γ_ _max_ = _f_ ( _x_ )( _Kf_ )( _x_ ) _µ_ (d _x_ ) = _f_ ( _x_ ) _k_ ( _x, y_ ) _f_ ( _y_ ) _µ_ (d _x_ ) _µ_ (d _y_ )
� �



_≤_ _γ_ _max_ _[−]_ [1]



**1** ( _x_ ) _k_ ( _x, y_ ) **1** ( _y_ ) _µ_ (d _x_ ) _µ_ (d _y_ ) = _γ_ _max_ _[−]_ [1] [Tr] � _ρ_ [2] _µ_ �
�



where we used that _k_ is pointwise positive. This ends the proof.


Let us look at this result in our main setting where each coordinate of _d_ -dimensional data is embedded
in a single qubit. If the measure _µ_ on R _[d]_ factorizes as _µ_ = [�] _µ_ _i_ . The integral operator factorizes
over the _d_ coordinates and the eigenvalues of the integral operator are given by _{_ [�] _[d]_ _j_ =1 _[γ]_ _[i]_ _j_ _[, γ]_ _[i]_ _j_ _[∈]_ _[E]_ _[j]_ _[}]_
with _E_ _j_ denoting the eigenvalues of the one-dimensional integral operators. In particular the largest
eigenvalue will be exponentially small (in _d_ ) as soon as max( _E_ _j_ ) _≤_ _δ <_ 1 for a fixed _δ_ which
holds if the individual embeddings satisfy Tr � _ρ_ [2] _µ_ _i_ � _< δ_ . Note that Tr � _ρ_ [2] _µ_ _i_ � = 1 if and only if the
embedding is constant.


16


**C.3** **Spectral decomposition of the integral operator**


As shown in the main text the integral operator _K_ applied to _f_ ( _x_ ) = Tr [ _ρ_ ( _x_ ) _M_ ] can be written as


( _Kf_ )( _x_ ) = Tr [ _O_ _µ_ ( _M ⊗_ _ρ_ ( _x_ ))] = Tr [ _O_ _µ_ ( _M ⊗_ id)(id _⊗_ _ρ_ ( _x_ ))] = Tr [Tr 1 [ _O_ _µ_ ( _M ⊗_ id)] _ρ_ ( _x_ )]
(27)


where _O_ _µ_ = � _ρ_ ( _y_ ) _⊗ρ_ ( _y_ ) _µ_ (d _y_ ) . Note that this reformulation makes the isomorphism of _L_ ( _H, H_ ) _⊗_
_L_ ( _H, H_ ) _≃L_ ( _H ⊗H, H ⊗H_ ) _≃L_ ( _L_ ( _H, H_ ) _, L_ ( _H, H_ )) explicit. The spectrum of _K_ thus agrees
with the eigenvalues of the linear map _T_ acting on matrices by


_T_ ( _M_ ) = Tr 2 [ _O_ _µ_ (id _⊗_ _M_ )] _._ (28)


We claim that there is an eigendecomposition


_T_ ( _M_ ) = � _γ_ _i_ _A_ _i_ Tr [ _A_ _i_ _M_ ] (29)


where _A_ _i_ are orthonormal hermitian matrices. Moreover, the eigenfunctions of _K_ are _f_ _i_ ( _x_ ) =
Tr [ _ρ_ ( _x_ ) _A_ _i_ ] . This result follows from standard results in linear algebra, we give all details in the next
subsection.


**C.4** **Spectral decomposition of linear maps preserving hermitian matrices**


We consider the space of matrices C _[n][×][n]_ equipped with the usual scalar product _⟨A, B⟩_ = Tr � _A_ _[†]_ _B_ �

which agrees with the standard scalar product on _C_ _[n]_ [2] after vectorisation. We will need them following
fact: For hermitian matrices _A, B_ the scalar product _⟨A, B⟩∈_ R is real.


**Lemma 3.** _Let_ _T_ : C _[n][×][n]_ _→_ C _[n][×][n]_ _be a linear and hermitian map that maps hermitian matrices_
_to hermitian matrices. Then there is a eigendecomposition_ ( _γ_ _i_ _, H_ _i_ ) _with real eigenvalues_ _γ_ _i_ _and_
_orthonormal hermitian matrices H_ _i_ _such that_

_T_ ( _A_ ) = � _γ_ _i_ _H_ _i_ _Tr_ � _H_ _i_ _[†]_ _[A]_ � _._ (30)

_i_


_Proof._ Hermitian matrices can be diagonalized with real values _γ_ _i_ so we can write

_T_ ( _A_ ) = � _γ_ _i_ _X_ _i_ Tr � _X_ _i_ _[†]_ _[A]_ � (31)

_i_


where _X_ _i_ form an orthonormal eigenbasis. It remains to show that we can find such a decomposition
where the _X_ _i_ are hermitian. We decompose _X_ _i_ = _H_ [˜] _i_ + _iS_ [˜] _i_ where _H_ [˜] _i_ and _S_ [˜] _i_ are hermitian. Then we
observe


_γ_ _i_ ( _H_ [˜] _i_ + _iS_ [˜] _i_ ) = _γ_ _i_ _X_ _i_ = _T_ ( _X_ _i_ ) = _T_ ( _H_ [˜] _i_ ) + _iT_ ( _S_ [˜] _i_ ) _._ (32)


Using the invariances of _T_ on hermitian matrices we conclude that _S_ [˜] _i_ and _H_ [˜] _i_ are again eigenvectors
with eigenvalue _γ_ _i_ . Now we can iteratively replace _X_ _i_ by either _S_ [˜] _i_ or _H_ [˜] _i_ so that the set of vectors
remains a basis. Finally we orthonormalize the resulting basis of all eigenspaces using the GramSchmidt procedure. Since scalar products of hermitian matrices are real we obtain an orthonormal
eigenbasis _H_ _i_ consisting of hermitian matrices.


We now apply this to the integral operator for the quantum embedding. Recall that the linear map _T_
acting on matrices was defined by


_T_ ( _M_ ) = Tr 2 [ _O_ _µ_ (id _⊗_ _M_ )] _._ (33)


Clearly, _T_ is linear. To show that _T_ is hermitian we observe that


_⟨M, T_ ( _M_ ) _⟩_ = Tr � _M_ _[†]_ Tr 2 [ _O_ _µ_ (id _⊗_ _M_ )]� = Tr � _M_ _[†]_ Tr 2 [ _ρ_ ( _y_ ) _⊗_ _ρ_ ( _y_ )(id _⊗_ _M_ )]� _µ_ (d _y_ )
�

(34)
= Tr � _M_ _[†]_ _ρ_ ( _y_ )� Tr [ _ρ_ ( _y_ ) _M_ ] _µ_ (d _y_ ) _∈_ R _._
�


17


Similarly we see that _T_ preserves hermitian matrices, indeed, if _M_ = _M_ _[†]_


_T_ ( _M_ ) = Tr 2 [ _ρ_ ( _y_ ) _⊗_ _ρ_ ( _y_ )(id _⊗_ _M_ )] _µ_ (d _y_ ) = _ρ_ ( _y_ )Tr [ _ρ_ ( _y_ ) _M_ )] _µ_ (d _y_ ) _._ (35)
� �

which is hermitian because _ρ_ ( _y_ ) is hermitian and the scalar product of hermitian matrices is real.
Using Lemma 3 above we conclude that we can write _T_ ( _M_ ) = [�] _i_ _[γ]_ _[i]_ _[A]_ _[i]_ [Tr][ [] _[A]_ _[i]_ _[M]_ []] [ where] _[ γ]_ _[i]_ [ are the]

eigenvalues of _T_ which agree with the eigenvalues of the corresponding integral operator and the
eigenfunctions are given by _x →_ Tr [ _ρ_ ( _x_ ) _A_ _i_ ].


**C.5** **A complete example**


To illustrate the analysis above we consider the setting from Example 2 where _x →_ cos( _x/_ 2) _|_ 0 _⟩_ +
_i_ sin( _x/_ 2) _|_ 1 _⟩_ . Then _F_ [˜] = _⟨_ sin( _x_ ) _,_ cos( _x_ ) _⟩_ and _F_ = _⟨_ sin [2] ( _x_ ) _,_ cos [2] ( _x_ ) _,_ sin( _x_ ) cos( _x_ ) _⟩_ . Note that the
RKHS has dimension 4 when the relative phase between _a_ ( _x_ ) and _b_ ( _x_ ) is not constant (then ¯ _ab_ and
_a_ [¯] _b_ are not linearly dependent). The feature map of the physical kernel for our example is



cos [2] ( _[y]_ 2
_ρ_ ( _y_ ) =
� _i_ cos( _[y]_ 2




_[y]_ 2 [)] _−i_ cos( _[y]_ 2




_[y]_ _[y]_

2 [) sin(] 2



_._ (36)
�



cos [2] ( _[y]_ 2 [)] _−i_ cos( _[y]_ 2 [) sin(] _[y]_ 2 [)]

_i_ cos( _[y]_ [) sin(] _[y]_ [)] sin [2] ( _[y]_ [)]




_[y]_ 2 [)] sin [2] ( _[y]_ 2




_[y]_ _[y]_

2 [) sin(] 2




_[y]_

2 [)]



For the analysis of the integral operator we need the matrix elements of the linear map _T_ We observe
that in index notation using the Einstein summation convention and denoting complex conjugation
without transposition by _∗_


_T_ ( _M_ ) _ij_ = _ρ_ ( _y_ ) _ij_ _ρ_ ( _y_ ) _kl_ _M_ _lk_ _µ_ (d _y_ ) = _ρ_ ( _y_ ) _ij_ _ρ_ _[∗]_ ( _y_ ) _lk_ _M_ _lk_ _µ_ (d _y_ ) _._ (37)
� �


Using vectorisation we obtain


Vec( _T_ ( _M_ )) = Vec( _ρ_ ( _y_ ))Vec( _ρ_ ( _y_ )) _[⊤]_ _µ_ (d _y_ )Vec( _M_ ) = _A_ _µ_ Vec _M._ (38)
�


In our example we obtain




_[y]_ 2 [)] _i_ cos( _[y]_ 2




_[y]_ _[y]_

2 [) sin(] 2



cos [2] ( _[y]_




_[y]_ _[y]_

2 [) sin] [2] [(] 2




_[y]_ 2 [)] _i_ cos( _[y]_ 2



cos [2] ( _[y]_ 2 [)]

_−i_ cos( _[y]_




_[y]_ _[y]_

2 [) sin(] 2 [)]

sin [2] ( _[y]_ [)]







 �cos [2] ( _[y]_ 2



_A_ _µ_ = [1]

_π_


= [1]


_π_


= [1]

8



� 0 _π_










3 0 0 1

0 1 _−_ 1 0

0 _−_ 1 1 0

1 0 0 3



_i_ cos( _[y]_ 2 [) sin(] _[y]_ 2 [)]

_i_ cos( _[y]_ [) sin(] _[y]_ [)]




_[y]_ _[y]_

2 [) sin(] 2




_[y]_ 2 [)] _−i_ cos( _[y]_ 2




_[y]_ _[y]_

2 [) sin(] 2




_[y]_ 2 [)] sin [2] ( _[y]_ 2




_[y]_ 2 [)] � d _y_




_[y]_ _[y]_

2 [) sin(] 2




_[y]_

2 [)]



cos [4] ( _[y]_




_[y]_ _[y]_

2 [) sin(] 2




_[y]_ 2 [)] _i_ cos [3] ( _[y]_ 2




_[y]_ 2 [)] _−i_ cos [3] ( _[y]_ 2




_[y]_ 2 [)] cos [2] ( _[y]_ 2




_[y]_ _[y]_

2 [) sin(] 2




_[y]_ _[y]_

2 [) sin] [2] [(] 2




_[y]_ _[y]_

2 [) sin] [3] [(] 2









 [d] _[y]_




_[y]_ _[y]_

2 [) sin] [2] [(] 2




_[y]_ 2 [)] _−_ cos [2] ( _[y]_ 2




_[y]_ 2 [)] _−i_ cos( _[y]_ 2



� 0 _π_
















cos [4] ( _[y]_ 2 [)] _i_ cos [3] ( _[y]_ 2 [) sin(] _[y]_ 2 [)] _−i_ cos [3] ( _[y]_ 2 [) sin(] _[y]_ 2 [)] cos [2] ( _[y]_ 2 [) sin] [(] _[y]_ 2 [)]

_−i_ cos [3] ( _[y]_ [) sin(] _[y]_ [)] cos [2] ( _[y]_ [) sin] [2] [(] _[y]_ [)] _−_ cos [2] ( _[y]_ [) sin] [2] [(] _[y]_ [)] _−i_ cos( _[y]_ [) sin] [3] [(] _[y]_



_i_ cos [3] ( _[y]_ 2 [) sin(] _[y]_ 2 [)] _−_ cos [2] ( _[y]_ 2 [) sin] [(] _[y]_ 2 [)] cos [2] ( _[y]_ 2 [) sin] [(] _[y]_ 2 [)] _i_ cos( _[y]_ 2 [) sin] [(] _[y]_ 2 [)]

cos [2] ( _[y]_ [) sin] [2] [(] _[y]_ [)] _i_ cos( _[y]_ [) sin] [3] [(] _[y]_ [)] _−i_ cos( _[y]_ [) sin] [3] [(] _[y]_ [)] sin [4] ( _[y]_ [)]




_[y]_ _[y]_

2 [) sin(] 2




_[y]_ 2 [)] _−_ cos [2] ( _[y]_ 2




_[y]_ 2 [)] cos [2] ( _[y]_ 2




_[y]_ _[y]_

2 [) sin(] 2




_[y]_ _[y]_

2 [) sin] [2] [(] 2



_i_ cos [3] ( _[y]_ 2 [) sin(] _[y]_ 2 [)] cos [2] ( _[y]_ 2 [) sin] [(] _[y]_ 2 [)] _−_ cos [2] ( _[y]_ 2 [) sin] [(] _[y]_ 2 [)] _−i_ cos( _[y]_ 2 [) sin] [(] _[y]_ 2 [)]

_i_ cos [3] ( _[y]_ [) sin(] _[y]_ [)] _−_ cos [2] ( _[y]_ [) sin] [2] [(] _[y]_ [)] cos [2] ( _[y]_ [) sin] [2] [(] _[y]_ [)] _i_ cos( _[y]_ [) sin] [3] [(] _[y]_ [)]




_[y]_ _[y]_

2 [) sin] [2] [(] 2




_[y]_ _[y]_

2 [) sin] [2] [(] 2




_[y]_ _[y]_

2 [) sin] [3] [(] 2




_[y]_ 2 [)] _i_ cos( _[y]_ 2




_[y]_ _[y]_

2 [) sin] [3] [(] 2




_[y]_ 2 [)] cos [2] ( _[y]_ 2




_[y]_ _[y]_

2 [) sin] [3] [(] 2




_[y]_

2 [)]




_[y]_ 2 [)] _−i_ cos( _[y]_ 2




_[y]_ 2 [)] sin [4] ( _[y]_ 2














We obtain the eigenvalues [1]




[1] [1]

2 _[,]_ 4




[1] [1]

4 _[,]_ 4



4 _[,]_ [ 0][ the eigenvectors are, in matrix notation,]



1 0 1 0
_H_ 1 = 0 1 _,_ _H_ 2 = 0 _−_ 1
� � �



1 0
_,_ _H_ 2 = 0 _−_ 1
� �



0 _i_ 0 1
_,_ _H_ 3 = _−i_ 0 _,_ _H_ 4 = 1 0
� � � �



0 _i_
_,_ _H_ 3 = _−i_ 0
� �



(39)


_._ (40)
�



The corresponding eigenfunctions _f_ _i_ of the integral operator are given by _x →_ Tr [ _ρ_ ( _x_ ) _H_ _i_ ], i.e.,




_[x]_ _[x]_

2 [)] _[ −]_ [sin] [2] [(] 2



_f_ 1 ( _x_ ) = 1 _,_ _f_ 2 ( _x_ ) = cos [2] ( _[x]_




_[x]_

2 [) = cos(] _[x]_ [)] _[,]_



_f_ 3 ( _x_ ) = 2 cos( _[x]_




_[x]_ _[x]_

2 [) sin(] 2



(41)

_[x]_ 2 [) = sin(] _[x]_ [)] _[,]_ _f_ 4 ( _x_ ) = 0 _._



We can also parametrize the functions in the RKHS by _a_ cos( _x_ + _b_ ) + _c_ with _a, b, c ∈_ R.


Let us also look at the generalization to the vector valued case with _d_ -qubits. Then the RKHS is given
by all functions of the form



_x →_



_d_
�( _a_ _i_ cos( _x_ _i_ + _b_ _i_ ) + _c_ _i_ ) _._ (42)


_i_ =1


18


The eigenfunctions of the integral operator are given by


_d_
� sin _[α]_ _[i]_ ( _x_ _i_ ) cos _[β]_ _[i]_ ( _x_ _i_ ) (43)


_i_ =1


where _α_ _i_ _, β_ _i_ are non-negative integers satisfying _α_ _i_ + _β_ _i_ _≤_ 1 . The corresponding eigenvalue is
2 _[−][d][−]_ [�][(] _[α]_ _[i]_ [+] _[β]_ _[i]_ [)] . The degeneracy of the eigenvalue 2 _[−][d][−][l]_ can be calculated to 2 _[l]_ [�] _[d]_ _l_ � _._


**D** **Proof of Theorem 1**


In this section we prove Theorem 1 which will follow easily from the result below. We remark that
the following theorem is by no means sharp but a detailed analysis when learning is not possible is of
limited interest. Note that again typical lower bounds for the learning performance are focused on the
case _n →∞_ [43].

**Theorem 3.** _Consider a measure space_ ( _X, µ_ ) _such that_ _µ_ ( _X_ ) = 1 _with a kernel_ _k_ _satisfying_
_k_ ( _x, x_ ) = 1 _for all_ _x ∈_ _X_ _. Denote by_ _γ_ _max_ _the largest eigenvalue of the corresponding integral_
_operator. Suppose we have_ _n_ _training points_ _D_ _n_ = _{_ ( _x_ _i_ _, y_ _i_ ) _,_ 1 _≤_ _i ≤_ _n}_ _with_ ( _x_ _i_ _, y_ _i_ ) _∈_ _X ×_ R
_where_ _x_ _i_ _are i.i.d. draws from_ _µ_ _and_ _y_ _i_ = _f_ ( _x_ _i_ ) _for some square integrable function_ _f_ _. Then, for any_
_ε >_ 0 _with probability at least_ 1 _−_ _ε −_ _γ_ _max_ _n_ [4]



�



_∥f −_ _f_ [ˆ] _n_ _[λ]_ _[∥]_ [2] _[≥]_



1 _−_

�



~~�~~



2 _γ_ _max_ _n_ [2]


_ε_



_∥f_ _∥_ 2 (44)



_for all λ ≥_ 0 _where_ _f_ [ˆ] _n_ _[λ]_ _[denotes the kernel ridge regression estimator for training data]_ [ (] _[x]_ _[i]_ _[, y]_ _[i]_ [)] _[.]_


_Proof._ Denote the eigenvalues of the integral operator by _γ_ _i_ with _γ_ 1 = _γ_ _max_ . Standard results for
integral operators imply

� _γ_ _i_ = _k_ ( _x, x_ ) _µ_ (d _x_ ) = 1 (45)

�
_i_


� _γ_ _i_ [2] [=] _k_ ( _x, y_ ) [2] _µ_ (d _x_ ) _µ_ (d _y_ ) = _∥k∥_ 2 [2] _[.]_ (46)

�
_i_


We conclude that



_γ_ _i_ [2] _[≤]_ _[γ]_ _[max]_ �

_i_ _i_



_∥k∥_ 2 [2] [=] �



_γ_ _i_ = _γ_ _max_ _._ (47)

_i_



Since E _µ⊗µ_ � _k_ ( _x, y_ ) [2] [�] = _∥k∥_ 2 [2] [, Markov’s inequality together with] [ (47)] [ implies] [ P] _[µ][⊗][µ]_ [(] _[|][k]_ [(] _[x, y]_ [)] _[| ≥]_

1

_ε_ ) _≤_ _[γ]_ _[max]_ _ε_ [2] [. Let] _[ A]_ _[n]_ [ =] _[ {|][k]_ [(] _[x]_ _[i]_ _[, x]_ _[j]_ [)] _[| ≤]_ 2 _n_ [for all] _[ i][ ̸]_ [=] _[ j][}]_ [. Using the union bound we conclude that]



P _D_ _n_ ( _A_ _n_ ) _≥_ 1 _−_ _n_ [2] P _µ⊗µ_ � _|k_ ( _x, y_ ) _| ≥_ 2 [1] _n_



_≥_ 1 _−_ 4 _n_ [4] _γ_ _max_ _._ (48)
�



Conditioned on _A_ _n_ we can bound the eigenvalues of the kernel matrix _K_ ( _X, X_ ) _i,j_ = _k_ ( _x_ _i_ _, x_ _j_ ) using
Gerschgorin circles by 1 _−_ _n_ 2 [1] _n_ [=] [1] 2 [and thus]


_K_ ( _X, X_ ) _[−]_ [1] _≤_ 2 _·_ id _n_ _._ (49)


Let us denote the Mercer decomposition of _k_ by


_k_ ( _x, y_ ) = � _γ_ _i_ _f_ _i_ ( _x_ ) _f_ _i_ ( _y_ ) (50)


_i_


where _f_ _i_ are the orthonormal eigenfunctions. Then we can bound




[1] [1]

2 _n_ [=] 2



2 [and thus]



_|k_ ( _x, ·_ ) _|_ 2 [2] [=] _k_ ( _x, y_ ) [2] _µ_ (d _y_ ) =
� ��



_γ_ _j_ _f_ _j_ ( _x_ ) _f_ _j_ ( _y_ ) _µ_ (d _y_ )

_j_



_γ_ _i_ _f_ _i_ ( _x_ ) _f_ _i_ ( _y_ ) �

_i_



� _δ_ _ij_ _γ_ _i_ _γ_ _j_ _f_ _i_ ( _x_ ) _f_ _j_ ( _x_ ) _≤_ _γ_ _max_ �

_i,j_ _i_



=
�



(51)
_γ_ _i_ _f_ _i_ ( _x_ ) [2] = _γ_ _max_ _._

_i_



19


The kernel ridge regression function _f_ _n_ _[λ]_ [can be written as]

_f_ _n_ _[λ]_ [=] � _α_ _i_ _k_ ( _x_ _i_ _, ·_ ) (52)


_i_

where the vector _α_ is given by _α_ = ( _K_ ( _X, X_ ) + _λ_ id _n×n_ ) _[−]_ [1] _y_ with _y ∈_ R _[n]_ denoting the vector with
components _y_ _i_ . Using (49) we conclude that conditioned on _A_ _n_ we have
_∥α∥_ [2] _≤_ 2 _∥y∥_ [2] _._ (53)
We now claim that for any _ε >_ 0 with probability 1 _−_ _ε_ we have
_|y|_ [2] _≤_ _[n]_ _ε_ _[∥][f]_ _[∥]_ 2 [2] _[.]_ (54)

To show this we remark that E( _y_ _i_ [2] [) =][ E][(] _[f]_ [(] _[x]_ _[i]_ [)] [2] [) =] _[ ∥][f]_ _[∥]_ 2 [2] [because we assumed that] _[ x]_ _[i]_ [is i.i.d. with]
distribution _µ_ . Using Markov’s inequality (and _|y|_ [2] _≥_ 0) we can bound



_|y|_ [2]

_[n]_ 2 _≤_ E

_ε_ _[∥][f]_ _[∥]_ [2] � � _nε_ _[−]_ [1] _∥f_



_ε_
_≤_ E
� _n∥f_ _∥_ [2] 2



_n_
� _|y_ _i_ _|_ [2]
� _i_ =1 �



P � _|y|_ [2] _≥_ _[n]_ _ε_



_nε_ _[−]_ [1] _∥f_ _∥_ [2] 2 **1** _|y|_ 2 _≥nε_ _−_ 1 _∥f_ _∥_ 22



= _ε._ (55)



This implies the claim (54).


Using (51), (53), and (54) we conclude that the _L_ [2] norm of _f_ _n_ _[λ]_ [satisfies now with probability]
1 _−_ _ε −_ _γ_ _max_ _n_ [4] the bound



_∥f_ _n_ _[λ]_ _[∥]_ [2] _[≤]_ �



_|α_ _i_ _| ≤_ _[√]_ ~~_γ_~~ _max_ ~~_n_~~ ~~�~~

_i_ _i_
�



_|α_ _i_ _|∥k_ ( _x_ _i_ _, ·_ ) _∥_ 2 _≤_ _[√]_ ~~_γ_~~ _max_ �

_i_ _i_



_α_ _i_ [2]

_i_



(56)



�



2 _γ_ _max_ _n_ [2]

_∥f_ _∥._
_ε_



_≤_ _[√]_ ~~_γ_~~ _max_ ~~_n_~~



�



2 _n∥f_ _∥_ [2] 2 _≤_
_ε_



2 _γ_ _max_ _n_ [2]



We conclude that with probability 1 _−_ _ε −_ _γ_ _max_ _n_ [4]


_∥f −_ _f_ _n_ _[λ]_ _[∥]_ [2] _[≥∥][f]_ _[∥]_ [2] _[−∥][f]_ _n_ _[ λ]_ _[∥]_ [2] _[≥∥][f]_ _[∥]_ [2]



�



2 _γ_ _max_ _n_ [2]


_ε_



1 _−_

�



�



�



_._ (57)



The proof of Theorem 1 is now a consequence of the result above.


_Proof of Theorem 1._ The general strategy of the proof is to show that the result follows from Theorem 3 for sufficiently large _d_ . We first note that, using the assumption _µ_ = [�] _µ_ _i_

_ρ_ _µ_ = � _ρ_ _µ_ _i_ (58)


and thus
Tr [ _ρ_ _µ_ ] = � Tr [ _ρ_ _µ_ _i_ ] _≤_ _δ_ _[d]_ _._ (59)

Lemma 1 then implies that the largest eigenvalue of the integral operator is bounded by _γ_ _max_ ( _d_ ) _≤_
_δ_ _[d/]_ [2] . Next we observe that there is _d_ 0 ( _δ, l, ε_ ) such that for _d ≥_ _d_ 0
_δ_ _[d/]_ [2] _≤_ _εd_ _[−]_ [4] _[l]_ _/_ 2 and _δ_ _[d/]_ [2] _≤_ _ε_ [3] _d_ _[−]_ [2] _[l]_ _/_ 4 _,_ (60)
because the left sides of the equations are decaying exponentially in _d_ (recall that _δ <_ 1 ) and the right
sides only polynomially.


Using the estimates above and the assumption _n ≤_ _d_ _[l]_ we conclude that for _d ≥_ _d_ 0
_γ_ _max_ _≤_ _δ_ _[d/]_ [2] _≤_ _εd_ _[−]_ [4] _[l]_ _/_ 2 _≤_ _εn_ _[−]_ [4] _/_ 2 _⇒_ _γ_ _max_ _n_ [4] _≤_ _ε/_ 2 (61)



_γ_ _max_ _≤_ _δ_ _[d/]_ [2] _≤_ _ε_ [3] _d_ _[−]_ [2] _[l]_ _/_ 2 _≤_ _ε_ [3] _n_ _[−]_ [2] _/_ 4 _⇒_ �4 _γ_ _max_ _n_ [2] _ε_ _[−]_ [1] _≤_ _ε._ (62)



We now denote the _ε_ used in Theorem 3 as _ε_ _[′]_ and set _ε_ _[′]_ = _ε/_ 2 . Theorem 3 and (61) and (62) then
imply that with probability at least
1 _−_ _ε_ _[′]_ _−_ _γ_ _max_ _n_ [4] _≥_ 1 _−_ _ε/_ 2 _−_ _ε/_ 2 = 1 _−_ _ε_ (63)

the bound



�



�



_∥f −_ _f_ [ˆ] _n_ _[λ]_ _[∥]_ 2 [2] _[≥]_



1 _−_

�



2 _γ_ _max_ _n_ [2]

~~�~~ _ε_ _[′]_



_∥f_ _∥_ 2 =



1 _−_

�


20



4 _γ_ _max_ _n_ [2]

~~�~~ _ε_



_∥f_ _∥_ 2 _≥_ (1 _−_ _ε_ ) _∥f_ _∥_ 2 (64)



holds for all _λ ≥_ 0. This completes the proof.


**E** **Proof of Theorem 2**


We introduce some theory and notation necessary for the proof. We investigate the behavior of
reduced density matrices when _V_ is distributed according to the Haar-measure on the group of unitary
matrices. The first even moments of the Haar measure on _U_ (2 _[d]_ ) are given by (see e.g., [53])



� _V_ _ij_ _V_ _i_ _[∗]_ _[′]_ _j_ _[′]_ _[ µ]_ [(d] _[V]_ [ ) =] _[ δ]_ _[ii]_ 2 _[′]_ _[δ]_ _[d][jj]_ _[′]_


1
� _V_ _i_ 1 _j_ 1 _V_ _i_ 2 _j_ 2 _V_ _i_ _[∗]_ _[′]_ 1 _[j]_ 1 _[′]_ _[V]_ _[ ∗]_ _i_ _[′]_ 2 _[j]_ 2 _[′]_ _[µ]_ [(d] _[V]_ [ ) =] 2 [2] _[d]_ _−_ 1



� _δ_ _i_ 1 _i_ _[′]_ 1 _[δ]_ _[j]_ [1] _[j]_ 1 _[′]_ _[δ]_ _[i]_ [2] _[i]_ _[′]_ 2 _[δ]_ _[j]_ [2] _[j]_ 2 _[′]_ [+] _[ δ]_ _[i]_ [1] _[i]_ _[′]_ 2 _[δ]_ _[j]_ [1] _[j]_ 2 _[′]_ _[δ]_ _[i]_ [2] _[i]_ _[′]_ 1 _[δ]_ _[j]_ [2] _[j]_ 1 _[′]_



�



1

_−_
2 _[d]_ (2 [2] _[d]_ _−_ 1)



� _δ_ _i_ 1 _i_ _[′]_ 1 _[δ]_ _[j]_ [1] _[j]_ 2 _[′]_ _[δ]_ _[i]_ [2] _[i]_ 2 _[′]_ _[δ]_ _[j]_ [2] _[j]_ 1 _[′]_ [+] _[ δ]_ _[i]_ [1] _[i]_ _[′]_ 2 _[δ]_ _[j]_ [1] _[j]_ 1 _[′]_ _[δ]_ _[i]_ [2] _[i]_ _[′]_ 1 _[δ]_ _[j]_ [2] _[j]_ 2 _[′]_



_._
�


(65)



Note that here and in the following _V_ _[∗]_ the conjugated (but not transposed) matrix. Let us remark that
while random circuits that output Haar-distributed unitaries require an exponential (in _d_ ) number of
gates our arguments actually only require unitary _t_ -designs which are point distributions that match
the first _t_ moments of the Haar measure. In particular a 2-design is a measure with finite support on
unitary matrices satisfying (65) (and odd moments of lower order vanish). Those can be implemented
using polynomially many gates. For details and further information we refer to the literature [54].


Recall the definition of the projected quantum kernel


˜
_ρ_ _[V]_ _m_ [(] _[x]_ [) =][ Tr] _[m]_ [+1] _[...d]_ � _ρ_ _[V]_ ( _x_ )� _._ (66)


To denote the partial trace in index notation we split the index _i ∈{_ 1 _, . . .,_ 2 _[d]_ _}_ in ( _α,_ ¯ _α_ ) where
_α ∈{_ 1 _, . . .,_ 2 _[m]_ _}_ denotes the index corresponding to the first _m_ qubits and ¯ _α ∈{_ 1 _, . . .,_ 2 _[d][−][m]_ _}_
denotes the index corresponding to the remaining _d −_ _m_ qubits. We will always use roman letters for
indices in _{_ 1 _, . . .,_ 2 _[d]_ _}_, greek letters for indices in _{_ 1 _, . . .,_ 2 _[m]_ _}_ and greek letters with a bar for indices
in _{_ 1 _, . . .,_ 2 _[d][−][m]_ _}_ . In particular, summing 1 over ¯ _α_ results in 2 _[d][−][m]_ and summing over _i_ results in 2 _[d]_ .
We will always use Einstein summation convention in the following so that, e.g. _δ_ _α_ ¯ _α_ ¯ = 2 _[d][−][m]_ . We
are now ready to prove Theorem 2.


_Proof of Theorem 2._ We start to prove the asymptotic expression for the reduced density matrix
which is a standard result. We can write


E _V_ � _ρ_ ˜ _[V]_ _m_ [(] _[x]_ [)] _[α]_ 1 _[,α]_ 2 � = E _V_ � _V_ _α_ 1 ¯ _α,j_ _ρ_ ( _x_ ) _j,j_ _′_ _V_ _α_ _[∗]_ 2 ¯ _α,j_ _[′]_ � = [2] _[d]_ 2 _[−][d][m]_ _δ_ _α_ 1 _α_ 2 _δ_ _jj_ _′_ _ρ_ ( _x_ ) _j,j_ _′_ = 2 _[−][m]_ _δ_ _α_ 1 _α_ 2 Tr [ _ρ_ ( _x_ )] _._

(67)


To show the concentration around the expectation value we need to calculate the variance of this
expression. We calculate the second moment of the reduced density matrix



E _V_ � _ρ_ ˜ _[V]_ _m_ [(] _[x]_ [)] _[α]_ 1 _[,α]_ 2 _[ρ]_ [˜] _[V]_ _m_ [(] _[y]_ [)] _[β]_ 1 _[,β]_ 2 � = E _V_ � _V_ _α_ 1 ¯ _α,j_ 1 _ρ_ ( _x_ ) _j_ 1 _,j_ 1 _[′]_ _[V]_ _[ ∗]_ _α_ 2 ¯ _α,j_ 1 _[′]_ _[V]_ _[β]_ [2] [ ¯] _[β,j]_ [2] _[ρ]_ [(] _[y]_ [)] _[j]_ [2] _[,j]_ 2 _[′]_ _[V]_ _[ ∗]_ _β_ 1 _β,j_ [¯] 2 _[′]_



�



1 2 1 [2] [2] 2 1 2 (68)

= _A_ 1 + _A_ 2 + _A_ 3 + _A_ 4 _._



21


Here the terms _A_ _i_ correspond to the four contributions on the right hand side of (65) . The four terms
can be evaluated to (assuming that Tr [ _ρ_ ( _x_ )] = 1 for all _x_ )



1 2 [2] _[d]_
_A_ 1 =
2 [2] _[d]_ _−_ 1 _[δ]_ _[α]_ [1] _[α]_ [2] [2] _[d][−][m]_ [Tr][ [] _[ρ]_ [(] _[x]_ [)]] _[ δ]_ _[β]_ [1] _[β]_ [2] [2] _[d][−][m]_ [Tr][ [] _[ρ]_ [(] _[y]_ [)] =] 2 [2] _[d]_ _−_ 1 [2] _[−]_ [2] _[m]_ _[δ]_ _[α]_ [1] _[α]_ [2] _[δ]_ _[β]_ [1] _[β]_ [2]


1
= 2 _[−]_ [2] _[m]_ _δ_ _α_ 1 _α_ 2 _δ_ _β_ 1 _β_ 2 + 2 [2] _[d]_ _−_ 1 [2] _[−]_ [2] _[m]_ _[δ]_ _[α]_ [1] _[α]_ [2] _[δ]_ _[β]_ [1] _[β]_ [2]


1
_A_ 2 = 2 [2] _[d]_ _−_ 1 _[δ]_ _[α]_ [1] _[β]_ [2] _[δ]_ _[α]_ [¯] _[β]_ [ ¯] _[δ]_ _[j]_ [1] _[j]_ 2 _[′]_ _[δ]_ _[α]_ [2] _[β]_ [1] _[δ]_ [ ¯] _[β]_ [ ¯] _[α]_ _[δ]_ _[j]_ [2] _[j]_ 1 _[′]_ _[ρ]_ [(] _[x]_ [)] _[j]_ [1] _[,j]_ 1 _[′]_ _[ρ]_ [(] _[y]_ [)] _[j]_ [2] _[,j]_ 2 _[′]_


1
= [2] _[d][−][m]_
2 [2] _[d]_ _−_ 1 _[δ]_ _[α]_ [¯] _[α]_ [¯] _[ρ]_ [(] _[x]_ [)] _[j]_ [1] _[,j]_ 1 _[′]_ _[ρ]_ [(] _[y]_ [)] _[j]_ 1 _[′]_ _[,j]_ [1] _[δ]_ _[α]_ [1] _[β]_ [2] _[δ]_ _[α]_ [2] _[β]_ [1] [ =] 2 [2] _[d]_ _−_ 1 [Tr][ [] _[ρ]_ [(] _[x]_ [)] _[ρ]_ [(] _[y]_ [)]] _[ δ]_ _[α]_ [1] _[β]_ [2] _[δ]_ _[α]_ [2] _[β]_ [1]


1
_A_ 3 = _−_ 2 _[d]_ (2 [2] _[d]_ _−_ 1) _[δ]_ _[α]_ [1] _[α]_ [2] _[δ]_ _[α]_ [¯] _[α]_ [¯] _[δ]_ _[j]_ [1] _[j]_ 2 _[′]_ _[δ]_ _[β]_ [1] _[β]_ [2] _[δ]_ [ ¯] _[β]_ [ ¯] _[β]_ _[δ]_ _[j]_ [2] _[j]_ 1 _[′]_ _[ρ]_ [(] _[x]_ [)] _[j]_ [1] _[,j]_ 1 _[′]_ _[ρ]_ [(] _[y]_ [)] _[j]_ [2] _[,j]_ 2 _[′]_



(69)



2 [2] _[d][−]_ [2] _[m]_

= _−_



2 [2] _[d][−]_ [2] _[m]_

2 _[d]_ (2 [2] _[d]_ _−_ 1) _[ρ]_ [(] _[x]_ [)] _[j]_ [1] _[,j]_ 1 _[′]_ _[ρ]_ [(] _[y]_ [)] _[j]_ 1 _[′]_ _[,j]_ [1] _[δ]_ _[α]_ [1] _[α]_ [2] _[δ]_ _[β]_ [1] _[β]_ [2] [ =] _[ −]_ 2 [2] [2] _[d][d][−]_ _−_ [2] _[m]_ 1



2 [2] _[d]_ _−_ 1 [Tr][ [] _[ρ]_ [(] _[x]_ [)] _[ρ]_ [(] _[y]_ [)]] _[ δ]_ _[α]_ [1] _[α]_ [2] _[δ]_ _[β]_ [1] _[β]_ [2]



1
_A_ 4 = _−_ 2 _[d]_ (2 [2] _[d]_ _−_ 1) _[δ]_ _[α]_ [1] _[β]_ [2] _[δ]_ _[α]_ [¯] _[β]_ [ ¯] _[δ]_ _[j]_ [1] _[j]_ 1 _[′]_ _[δ]_ _[α]_ [2] _[β]_ [1] _[δ]_ _[α]_ [˜] _[β]_ [ ˜] _[δ]_ _[j]_ [2] _[j]_ 2 _[′]_ _[ρ]_ [(] _[x]_ [)] _[j]_ [1] _[,j]_ 1 _[′]_ _[ρ]_ [(] _[y]_ [)] _[j]_ [2] _[,j]_ 2 _[′]_


1

= _−_
2 _[d]_ (2 [2] _[d]_ _−_ 1) _[δ]_ _[α]_ [¯] _[α]_ [¯] _[ρ]_ [(] _[x]_ [)] _[j]_ [1] _[,j]_ [1] _[ρ]_ [(] _[y]_ [)] _[j]_ [2] _[,j]_ [2] _[δ]_ _[α]_ [1] _[β]_ [2] _[δ]_ _[α]_ [2] _[β]_ [1] [ =] _[ −]_ 2 [2] [2] _[d][−]_ _−_ _[m]_ 1 _[δ]_ _[α]_ [1] _[β]_ [2] _[δ]_ _[α]_ [2] _[β]_ [1]


Altogether we obtain


E _V_ � _ρ_ ˜ _[V]_ _m_ [(] _[x]_ [)] _[α]_ 1 _[,α]_ 2 _[ρ]_ [˜] _[V]_ _m_ [(] _[y]_ [)] _[β]_ 1 _[,β]_ 2 � = _δ_ _α_ 1 _α_ 2 _δ_ _β_ 1 _β_ 2 2 _[−]_ [2] _[m]_ [ �] 1 _−_ 2 _[−][d]_ Tr [ _ρ_ ( _x_ ) _ρ_ ( _y_ )] + 2 _[−]_ [2] _[d]_ [�]

(70)
+ _δ_ _α_ 1 _β_ 2 _δ_ _α_ 2 _β_ 1 2 _[−][m]_ [ �] 2 _[−][d]_ Tr [ _ρ_ ( _x_ ) _ρ_ ( _y_ )] _−_ 2 _[−]_ [2] _[d]_ [�] + _O_ (2 _[−]_ [3] _[d]_ )


Recall that the complex variance of a random variable is defined by E � _|X|_ [2] [�] _−|_ E [ _X_ ] _|_ [2] . Using (70)
and that ˜ _ρ_ is hermitian we can bound the variance of the entries of ˜ _ρ_ ( _x_ ) by


˜ ˜
E _V_ � _ρ_ _[V]_ _m_ [(] _[x]_ [)] _[α,β]_ [(˜] _[ρ]_ _m_ _[V]_ [(] _[x]_ [)] _[α,β]_ [)] _[∗]_ [�] _−_ E _V_ � _ρ_ _[V]_ _m_ [(] _[x]_ [)] _[α,β]_ �E _V_ �(˜ _ρ_ _[V]_ _m_ [(] _[x]_ [)] _[α,β]_ [)] _[∗]_ [�]



˜ ˜ ˜
= E _V_ � _ρ_ _[V]_ _m_ [(] _[x]_ [)] _[α,β]_ _[ρ]_ [˜] _[V]_ _m_ [(] _[x]_ [)] _[β,α]_ � _−_ E _V_ � _ρ_ _[V]_ _m_ [(] _[x]_ [)] _[α,β]_ �E _V_ � _ρ_ _[V]_ _m_ [(] _[x]_ [)] _[β,α]_ �

= 2 _[−]_ [2] _[m]_ _δ_ _αβ_ _−_ (2 _[−][m]_ ) [2] _δ_ _αβ_ + _O_ (2 _[−][d]_ ) = _O_ (2 _[−][d]_ ) _._



(71)



This shows that ˜ _ρ_ _[V]_ ( _x_ ) is close to 2 _[−][m]_ id with high probability for large _d_ and finishes the proof of
the first part of the theorem.


We now turn to the evaluation of the averaged operator E _V_ [ _O_ _µ_ ] and the corresponding operator


_T_ ( _M_ ) = Tr 2 [E _V_ [ _O_ _µ_ ](id _⊗_ _M_ )] (72)


whose matrix elements we denote by _T_ _α_ 1 _α_ 2 ; _β_ 1 _β_ 2 so that _T_ ( _M_ ) _α_ 1 _,α_ 2 = _T_ _α_ 1 _α_ 2 ; _β_ 1 _β_ 2 _M_ _β_ 1 _,β_ 2 . We
assume that _ρ_ ( _x_ ) is pure for all _x_, i.e., Tr � _ρ_ ( _x_ ) [2] [�] = 1 . We have seen in (37) that the matrix elements
of this operator are given by



E _V_



_ρ_ ˜ _[V]_ _m_ [(] _[y]_ [)] _[ ⊗]_ [(˜] _[ρ]_ _[V]_ _m_ [(] _[y]_ [))] _[∗]_ _[µ]_ [(d] _[y]_ [)] = E _V_ � _ρ_ ˜ _[V]_ _m_ [(] _[y]_ [)] _[ ⊗]_ [(˜] _[ρ]_ _[V]_ _m_ [(] _[y]_ [))] _[∗]_ [�] _µ_ (d _y_ ) _._ (73)
�� � �



From (70) we obtain for the matrix elements

E _V_ � _ρ_ ˜ _[V]_ ( _y_ ) _α_ 1 _,α_ 2 (˜ _ρ_ _[V]_ ( _y_ ) _β_ 1 _,β_ 2 ) _[∗]_ [�] = E _V_ � _ρ_ ˜ _[V]_ ( _y_ ) _α_ 1 _,α_ 2 ˜ _ρ_ _[V]_ ( _y_ ) _β_ 2 _,β_ 1 �




_[−][m]_

2 _[d]_ _[δ]_ _[α]_ [1] _[β]_ [1] _[δ]_ _[α]_ [2] _[β]_ [2] _[ −]_ [2] _[−]_ 2 [2] _[d][m]_



= 2 _[−]_ [2] _[m]_ _δ_ _α_ 1 _α_ 2 _δ_ _β_ 1 _β_ 2 + [2] _[−][m]_



_δ_ _α_ 1 _α_ 2 _δ_ _β_ 1 _β_ 2 + _O_ (2 _[−]_ [2] _[d]_ ) _._
2 _[d]_



(74)



22


10 [−1]


10 [−3]


10 [−5]


10 [−7]


10 [−9]





10 [−11]

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|
|---|---|---|---|---|---|---|---|---|
||||||||||
||||||||||
||||||||||
||||||||||
||||||||||
||||||||||
||||||||||
||||||||||
||||||||||



1 2 3 4 5 6 7
Number of Qubits _d_


Figure 4: Similar as in Fig. 2. However, for the full quantum kernel _k_ and the rbf kernel, we compute
train and test loss over multiple choices of the regularization parameter. For each number of qubits,
we only report the loss of the method that achieved smallest test loss. Note that, although this is
invalid to asses the power of the full and rbf kernel, it shows, that the poor performance is not due to
the choice of regularization. Since we cherry-pick on the test loss, it can happen that an underfitting
regularization has the best test loss, which explains the outlier on _k_ at _d_ = 6.


Since this is independent of _y_ we can write the matrix elements of _T_ as



_T_ _α_ 1 _α_ 2 ; _β_ 1 _β_ 2 = 2 _[−]_ [2] _[m]_ (1 _−_ 2 _[−][d]_ ) _δ_ _α_ 1 _α_ 2 _δ_ _β_ 1 _β_ 2 + [2] 2 _[−][d][m]_ _[δ]_ _[α]_ [1] _[β]_ [1] _[δ]_ _[β]_ [2] _[α]_ [2] [ +] _[ O]_ [(2] _[−]_ [2] _[d]_ [)] (75)



From here we conclude that _T_ can be written as



_T_ ( _M_ ) _α_ 1 _,α_ 2 = 2 _[−]_ [2] _[m]_ (1 _−_ 2 _[−][d]_ ) _δ_ _α_ 1 _α_ 2 _δ_ _β_ 1 _β_ 2 _M_ _β_ 1 _,β_ 2 + [2] _[−][m]_



2 _[d]_ _[δ]_ _[α]_ [1] _[β]_ [1] _[δ]_ _[α]_ [2] _[β]_ [2] _[M]_ _[β]_ [1] _[,β]_ [2] [ +] _[ O]_ [(2] _[−]_ [2] _[d]_ [)]



(76)

_[−][m]_

2 _[d]_ _[M]_ _[α]_ [1] _[,α]_ [2] [ +] _[ O]_ [(2] _[−]_ [2] _[d]_ [)]



= 2 _[−]_ [2] _[m]_ (1 _−_ 2 _[−][d]_ ) _δ_ _α_ 1 _α_ 2 _M_ _β_ 1 _,β_ 1 + [2] _[−][m]_



or, more concisely,



_T_ ( _M_ ) = [2] 2 _[−][d][m]_ _[M]_ [ + 2] _[−]_ [2] _[m]_ [(1] _[ −]_ [2] _[−][d]_ [)id] [2] _[m]_ _[×]_ [2] _[m]_ [Tr][ [id] [2] _[m]_ _[×]_ [2] _[m]_ _[M]_ [] +] _[ O]_ [(2] _[−]_ [2] _[d]_ [)] (77)



and we observe that _T_ is the sum of a multiple of the identity and a rank one perturbation (plus higher
order terms): In particular the eigenvalues neglecting the perturbation are


_γ_ 1 = 2 _[−]_ [2] _[m]_ (1 _−_ 2 _[−][d]_ )Tr [id 2 _m_ _×_ 2 _m_ id 2 _m_ _×_ 2 _m_ ] + 2 _[−][m][−][d]_ = 2 _[−][m]_ (1 _−_ 2 _[−][d]_ ) + 2 _[−][m][−][d]_ = 2 _[−][m]_ (78)


with eigenvector _M_ 1 = id 2 _m_ _×_ 2 _m_ and _γ_ 2 = _. . ._ = _γ_ 2 _m_ _×_ 2 _m_ = 2 _[−][m][−][d]_ with traceless eigenvectors,
i.e., Tr [id 2 _[m]_ _×_ 2 _[m]_ _M_ _i_ ] = 0 for _i ̸_ = 1 . Standard bounds show that the higher order terms change
the eigenvalues only by a term of order _O_ (2 _[−]_ [2] _[d]_ ) . Finally, we observe that the function mapping
_x →_ Tr � _ρ_ ˜ _[V]_ _m_ [(] _[x]_ [)] _[M]_ [1] � is a constant function for any _V_ . Indeed,


Tr � _ρ_ ˜ _[V]_ _m_ [(] _[x]_ [)] _[M]_ [1] � = Tr �Tr _m_ +1 _...d_ � _V ρ_ ( _x_ ) _V_ _[†]_ [��] = Tr � _V ρ_ ( _x_ ) _V_ _[†]_ [�] = Tr [ _ρ_ ( _x_ )] = 1 _._ (79)


**F** **More on experiments**


For details on the implementation we refer to the provided code. [5] We emphasize that our experiments
simulate the full quantum state and thus work with the true values of the quantum kernel. This is an
idealized setting and neglects the effect of finite measurements. Please see our discussion on Barren
Plateaus in the main paper.


To reduce computations and speed-up the simulation, we compute the full quantum kernel _k_ ( _x, x_ _[′]_ ) =
2 _′_
� cos (( _x_ _i_ _−_ _x_ _i_ [)] _[/]_ [2)] [ directly without simulating a quantum circuit. For the biased kernels we]


5 `[https://github.com/jmkuebler/quantumbias](https://github.com/jmkuebler/quantumbias)`


23


recommend (and implement it that way) to completely simulate _ρ_ _[V]_ 1 [(] _[x]_ _[i]_ [)] [ for all] _[ i]_ [ = 1] _[, . . ., n]_ [ and]
store the reduced density matrices ( 2 _×_ 2 hermitian matrices). On a real quantum device this would
correspond to doing quantum state tomography [ 36 ]. The benefit of this is that we only need to
simulate the quantum circuit _n_ times and can then directly compute the biased kernels via matrix
products and tracing. If we chose to compute each entry of the kernel matrix individually we would
have to simulate the circuit _n_ [2] times.


**Random generation of** _V_ **.** In order to generate random unitary matrices _V_ we use the PennyLane
function R ANDOM L AYERS [ 51 ]. For _d_ qubits we use _d_ [2] layers of single qubit rotations and 2 -qubit
entangling gates. For more details and the used seeds please refer to the provided implementation.


**Choice of regularization.** For the biased kernels _q, q_ _w_ regularization does not matter much, since
they have only a four-dimensional RKHS and we consider sample sizes much larger than that. The
RKHS simply does not have enough capacity to overfit to random noise. We therefore set the
regularization _λ_ = 0 for the biased kernels. On the other hand for the higher dimensional kernels
_k, k_ rbf, the regulariyation strongly influences their performance. For the experiment in the main paper
we set _λ_ = 10 _[−]_ [3] for the latter methods. Note that in a real application one should use cross-validation
or other model selection techniques to find good hyperparameters, which we omitted for simplicity.
To exclude that the bad performance of _k_ and _k_ rbf stems from a bad choice of regularization, we
include experiments where we fit kernel ridge regression for 15 values of _λ_ on a logarithmic grid
from 10 _[−]_ [6] to 10 [4] . We then cherry-pick only the solution that performs best and report it in Figure
4. Note that such an approach is of course not legit to asses the actual performance. However, it
serves to bound the performance for the optimal choice of regularization. Our observations show that
the behavior does not significantly change and we conclude that the performance difference indeed
comes from the spectral bias as predicted by our theory.


**Additional experiments.** To show how the kernel target alignment changes as we increase the
number of qubits _d_, we include further histograms in Figure 5. The estimated kernel alignment
correlates with the learning performance reported in Figure 2.


24


40


30


20


10


0





40


30


20


10


0




|Col1|Col2|Col3|Col4|q<br>k|
|---|---|---|---|---|
|||||_q_<br>~~_k_~~|
|||||_krbf_<br>_qw_|
||||||
||||||
||||||


|Col1|Col2|Col3|Col4|q|Col6|
|---|---|---|---|---|---|
|||||_q_<br>|_q_<br>|
|||||~~_k_~~<br>_krbf_<br>_qw_||
|||||||
|||||||
|||||||



0.0 0.2 0.4 0.6 0.8 1.0
Kernel Alignment _A_



0.0 0.2 0.4 0.6 0.8 1.0
Kernel Alignment _A_



40


30


20


10


0



40


30


20


10


0






|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|
|---|---|---|---|---|---|---|---|---|---|
|||||||||~~_q_~~<br>_k_<br>_krbf_||
|||||||||_qw_|_qw_|
|||||||||||
|||||||||||
|||||||||||


|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|
|---|---|---|---|---|---|---|---|---|---|
|||||||||~~_q_~~<br>_k_<br>_krbf_||
|||||||||_qw_|_qw_|
|||||||||||
|||||||||||
|||||||||||



0.0 0.2 0.4 0.6 0.8 1.0
Kernel Alignment _A_



0.0 0.2 0.4 0.6 0.8 1.0
Kernel Alignment _A_



Figure 5: Kernel Target Alignment for _d_ = 1 _,_ 3 _,_ 5 _,_ 7.


25


