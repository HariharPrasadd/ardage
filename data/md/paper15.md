## **Can Large Language Models Be an Alternative to Human Evaluation?**



**Cheng-Han Chiang**
National Taiwan University,
Taiwan

dcml0714@gmail.com


**Abstract**


Human evaluation is indispensable and inevitable for assessing the quality of texts generated by machine learning models or written by humans. However, human evaluation
is very difficult to reproduce and its quality
is notoriously unstable, hindering fair comparisons among different natural language processing (NLP) models and algorithms. Recently, large language models (LLMs) have
demonstrated exceptional performance on unseen tasks when only the task instructions are
provided. In this paper, we explore if such an
ability of the LLMs can be used as an alternative to human evaluation. We present the
LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs
to generate responses to those questions; we
dub this _LLM evaluation_ . We use human evalu
ation and LLM evaluation to evaluate the texts

in two NLP tasks: open-ended story generation and adversarial attacks. We show that the

result of LLM evaluation is consistent with the

results obtained by expert human evaluation:
the texts rated higher by human experts are
also rated higher by the LLMs. We also find
that the results of LLM evaluation are stable

over different formatting of the task instructions and the sampling algorithm used to generate the answer. We are the first to show the
potential of using LLMs to assess the quality
of texts and discuss the limitations and ethical

considerations of LLM evaluation.


**1** **Introduction**


Human evaluation is an important method to understand the performance of an NLP model or algorithm (Guzmán et al., 2015; Gillick and Liu, 2010).
We rely on human evaluation because there are certain aspects of texts that are hard to evaluate using
automatic evaluation metrics; thus, researchers resort to humans to rate the quality of the output of
NLP models. While human evaluation is prevalent and indispensable in NLP, it is notoriously



**Hung-yi Lee**
National Taiwan University,
Taiwan

hungyilee@ntu.edu.tw


unstable (Gillick and Liu, 2010; Clark et al., 2021).
Karpinska et al. (2021) has shown that low-quality
workforces in human evaluation can have a detri
mental effect on the evaluation result, making it
impossible to compare the performance among different systems. Reproducibility is another issue in
human evaluation since it is hard to recruit the same

human evaluators and rerun the same evaluation.

Even if the same workers are recruited, the workers
that have seen the task before are likely to produce
a different evaluation result the next time because

they have already done the task. While human evaluation is used to better assess NLP systems and has
some advantages over automatic evaluation metrics, the drawbacks of human evaluation somewhat
make it difficult to reliably evaluate NLP systems.
To resolve some of the drawbacks, we take advantage of large language models (LLMs). LLMs
are large models that are trained to model human
languages using self-supervised learning (Brown
et al., 2020) and further using special training procedures to improve the performance on unseen
tasks and better follow natural language instructions (Sanh et al., 2022; Wei et al., 2022). The
ability to perform a task just given the task instructions motivates us to ask if these LLMs can perform
what humans do in human evaluation. To answer

this question, we feed in the LLM with the same
instruction, sample, and question used in human
evaluation, and take the sequences generated by the
LLM as the LLM’s answer to the question. This
process is shown in Figure 1, and we call this process **LLM evaluation** .

To test if LLM evaluation yields meaningful results, we conduct LLM evaluation on two different
NLP tasks: evaluating the quality of stories in openended story generation and the quality of sentences
generated by adversarial attacks. We summarize
our findings and contribution as follows:


  - We show that LLM evaluation produces results similar to expert human evaluation, ver

**Task instruction, sample, and question**



**Human evaluation**






```
The goal of this task is to rate story

fragments.

NOTE : Please take the time

to fully read and understand the story
fragment. We will reject submissions from
workers that are clearly spamming the

task.

Story fragment

The human ambassador reached down and

grasped it's paw. "Humans, cats, is it

true that all intelligent beings are
omnivorous?" "Certainly, your rank

demeanour can be demonstrated from the

words we spoke to the Kelpie. They're of
no concern to us humans, as they are

not considered to live among us, thus
far. (…)

How grammatically correct is the text of

the story fragment? (on a scale of 1-5,

with 1 being the lowest?)

```




**LLM evaluation**









Figure 1: Illustration of the core idea of the paper using open-ended story generation as the example task. The left
part shows the instruction, story fragments, and questions used in human evaluation. The human experts are asked
to rate the quality of the story fragments using a 5-point Likert scale, shown on the upper right. The lower right
part shows the process of _LLM evaluation_, where we feed the LLMs the same instruction, story fragments, and
questions and parse the LLM-generated output to get the rating.



ifying the effectiveness of LLM evaluation
(§3.3 and §4.3). This paper is **the first** to propose using LLMs as an alternative to human
evaluation and show their effectiveness.


  - We show that LLM evaluation results only
slightly vary due to different task instructions
and the hyperparameters of the sampling algorithm used to generate the answer. (§3.3.2
and §3.3.3)


  - We carefully discuss the pros and cons of using LLM evaluation and discuss the ethical
considerations of LLM evaluation. (§5)


**2** **LLM Evaluation**


**2.1** **Large Language Models (LLMs)**


Large language models are language models having bulk parameter sizes, typically on the scale
of a few billion, and pre-trained on enormous
amounts of natural language corpora, including
GPT3 (Brown et al., 2020), T5 (Raffel et al., 2020),
and BLOOM (Scao et al., 2022). These LLMs
show exceptional performance on unseen tasks
when only the task instructions are given; this kind
of ability is called **zero-shot in-context learning** .
To further improve the zero-shot in-context learning performance, special training techniques have



been applied to those LLMs after pre-training. For
example, T0 (Sanh et al., 2022) and FLAN (Wei
et al., 2022) are fine-tuned on a mixture of tasks
and can thus achieve better zero-shot performance
compared to GPT-3. InstructGPT (Ouyang et al.,
2022) is fine-tuned from GPT-3 using reinforcement learning from human feedback (RLHF), and
it is shown to better follow the instructions. Chat
GPT (OpenAI, 2022) is fine-tuned from InstructGPT with a conversation dataset using RLHF, so
ChatGPT can interact with users in a conversational

way. ChatGPT is able to answer questions asked
by the user and provide comprehensive explanations about its answer. Given the LLMs’ ability to
follow task instructions and provide feedback, we
ask whether LLMs can be used as an alternative

to human evaluation and aid NLP researchers in

evaluating the quality of texts.


**2.2** **LLM Evaluation**


To evaluate the quality of texts generated by NLP
systems or written by humans using LLM, we
present the LLMs with the task instructions, the
sample to be evaluated, and a question. The question asks the LLM to rate the sample’s quality using
a 5-point Likert scale. Given the inputs, the LLM
will answer the question by generating some output sentences. We parse the output sentences to get


the score rated by the LLM. We call this process
_LLM evaluation_, and this procedure is shown in the
lower part of Figure 1. Different tasks use different
sets of task instructions, and each task uses different questions to evaluate the quality of the samples.
The instructions and questions used in LLM evaluation in our paper are not tailored for the LLMs; we
follow those instructions used to conduct human

evaluation in prior works.
To compare the result of LLM evaluation and
show its effectiveness, we compare the result of
LLM evaluation with human evaluation conducted

by English teachers. To make a fair and meaningful
comparison, the instructions, samples, and questions in human evaluation are formatted similarly
to those in LLM evaluation. The main difference

between LLM evaluation and human evaluation

is that in human evaluation, the human evaluators
answer the question by choosing the answer from
a pre-defined set of options (the 1-5 Likert scale
scores), as shown in the upper right in Figure 1.
In LLM evaluation, we instead let the LLM freely
generate sentences and extract the score from the
generated sentences using some simple rules, detailed in Appendix D.2.1.


**3** **Example Task 1: Open-Ended Story**
**Generation**


We first use open-ended story generation to demonstrate the usefulness of LLM evaluation.


**3.1** **Task Introduction**


Open-ended story generation is a task to generate a short story based on a given prompt. We
use the WritingPrompts dataset (Fan et al., 2018),
which is composed of pairs of short prompts and
human-written stories collected from the subred
dit [WritingPrompts](https://www.reddit.com/r/WritingPrompts/) . In the WritingPrompts, the
users are given a short prompt, and they need to
write a story based on the short prompt. [1]

In this experiment, we use LLM evaluation and
human evaluation to rate the stories generated by
humans and the stories generated by a story generation model. We select open-ended story generation as an example because Karpinska et al.
[(2021) show that workers from Amazon Mechani-](https://www.mturk.com/)
[cal Turk (AMT) cannot distinguish GPT-2 (Radford](https://www.mturk.com/)
et al., 2019) generated and human-written stories,


1 The WritingPrompts subreddit explicitly forbids the
users to use AI for generating stories, so we consider the
stories in the dataset to be human-written.



while English teachers show a clear preference for
human-written stories over GPT-2-generated stories. We want to see if LLM can rate human-written

stories higher than GPT-2-generated ones.
Following prior works (Mao et al., 2019; Guan
et al., 2020; Karpinska et al., 2021), the story generation model is GPT-2 medium model fine-tuned
on the WritingPrompts training dataset. After the
model is trained, we randomly select 200 prompts
from the testing set of WritingPrompts and make
the fine-tuned GPT-2 generate stories based on
those prompts using nucleus sampling (Holtzman
et al., 2020) with _p_ = 0 _._ 9 . For the human-written
stories to be compared, we use the 200 stories
written based on the same 200 prompts. We postprocess the human-written and GPT-2-generated
stories and then use them for LLM evaluation and

human evaluation. Please find the details on finetuning and data processing in Appendix B.


**3.2** **LLM Evaluation and Human Evaluation**


We present the LLMs and the human evaluators
with a short description, and the story to be evaluated, formatted as shown in Figure 1. Following Karpinska et al. (2021), we evaluate the stories
on four different attributes. The four attributes and

their corresponding questions are as follows:


1. _Grammaticality_ : How **grammatically cor-**
**rect** is the text of the story fragment?


2. _Cohesiveness_ : How well do **the sentences** in

the story fragment **fit together** ?


3. _Likability_ : How **enjoyable** do you find the
story fragment?


4. _Relevance_ : Now read the PROMPT based on

which the story fragment was written.
**Prompt** : [PROMPT].
How **relevant** is the **story fragment** to the
**prompt?**


Where the [PROMPT] will be filled in with the
prompt which the story is based on. Each attribute
is evaluated using a 5-point Likert scale; the following description is appended at the end of each
question: " _(on a scale of 1-5, with 1 being the low-_
_est)_ ". We show the interface used in human evaluation and the input format for the LLM evaluation
in Appendix C.2 and D.2.2.
The LLMs used for LLM evaluation include T0,

text-curie-001, text-davinci-003, and Chat
GPT. text-curie-001 and text-davinci-003


**Grammaticality** **Cohesiveness** **Likability** **Relevance**
**Evaluator**
Mean STD IAA % Mean STD IAA % Mean STD IAA % Mean STD IAA %

_Human-written stories_

Human 3 _._ 76 0 _._ 95 0 _._ 33 20 _._ 5 4 _._ 29 0 _._ 82 0 _._ 32 27 3 _._ 78 1 _._ 10 0 _._ 08 9 _._ 5 3 _._ 35 1 _._ 48 0 _._ 05 8
T0 2 _._ 55 1 _._ 47 0 _._ 16 10 2 _._ 98 1 _._ 45 0 _._ 11 4 3 _._ 18 1 _._ 53 0 _._ 12 7 2 _._ 93 1 _._ 64 0 _._ 02 6
curie 3 _._ 19 0 _._ 47 0 _._ 07 46 _._ 5 2 _._ 82 0 _._ 46 0 _._ 01 47 _._ 5 2 _._ 85 0 _._ 37 0 _._ 11 0 _._ 65 3 _._ 06 0 _._ 40 0 _._ 11 0 _._ 64
davinci 4 _._ 22 0 _._ 38 0 _._ 26 35 4 _._ 54 0 _._ 47 0 _._ 37 39 _._ 5 3 _._ 99 0 _._ 38 0 _._ 49 68 _._ 5 4 _._ 40 0 _._ 79 0 _._ 71 48 _._ 5
ChatGPT 3 _._ 83 0 _._ 60 3 _._ 55 0 _._ 88 2 _._ 44 0 _._ 89 3 _._ 29 1 _._ 50

_GPT-2-generated stories_
Human 3 _._ 56 0 _._ 91 0 _._ 10 19 _._ 5 3 _._ 19 1 _._ 07 0 _._ 14 17 2 _._ 59 1 _._ 29 _−_ 0 _._ 21 3 _._ 5 2 _._ 38 1 _._ 40 _−_ 0 _._ 03 8 _._ 5
T0 2 _._ 44 1 _._ 49 0 _._ 05 9 3 _._ 02 1 _._ 51 0 _._ 07 6 3 _._ 00 1 _._ 59 0 _._ 16 6 2 _._ 82 1 _._ 61 0 _._ 04 6
curie 3 _._ 23 0 _._ 51 0 _._ 01 38 2 _._ 82 0 _._ 45 0 _._ 02 50 2 _._ 86 0 _._ 37 0 _._ 09 65 _._ 5 3 _._ 01 0 _._ 43 0 _._ 11 61
davinci 4 _._ 07 0 _._ 35 0 _._ 35 45 _._ 5 4 _._ 26 0 _._ 45 0 _._ 42 42 3 _._ 84 0 _._ 42 0 _._ 52 62 4 _._ 02 0 _._ 74 0 _._ 69 42 _._ 5
ChatGPT 2 _._ 98 0 _._ 76 2 _._ 48 0 _._ 71 1 _._ 59 0 _._ 67 2 _._ 02 1 _._ 21


Table 1: LLM evaluation and human evaluation results of human-written stories and GPT-2-generated stories. For
each evaluated attribute, we report its mean Likert scale and the standard deviation. We also report the interannotator agreement (IAA) among three annotators using Krippendorff’s _α_ . The subscript in the IAA column (%)
is used to denote the percentage of the stories where all three annotators exactly agree on a rating.



are two InstructGPT models, and the latter is the
stronger model; we will use InstructGPT to refer to
these two models. We query the InstructGPT using
the official API provided by OpenAI. We use nucleus sampling with _p_ = 0 _._ 9 to generate the answer
from T0 and InstructGPTs. We **sample three an-**
**swers** from LLMs to stimulate the result of asking
the model to rate the same story three times. We
query ChatGPT using the user interface recently
released by OpenAI. Unlike InstructGPT, we cannot control the parameters used for generating the
response from ChatGPT. Because ChatGPT limits
the maximum number of queries per user, we only
sample one response for each question.

For human evaluation, we do not use the
commonly used AMT for human evaluation because Karpinska et al. (2021) has already shown
that the results obtained using AMT are highly
questionable. Following the recommendation of
the prior works, we hire **three certified English**
**teachers** [using an online freelancer platform, Up-](https://www.upwork.com/)
[Work. Teachers are familiar with evaluating the](https://www.upwork.com/)
essays of students, making them the expert evaluators in our task. The details about recruiting human
evaluators are in Appendix C.1. Each LLM and
each English teacher rates the 200 human-written
stories and 200 GPT-2-generated stories.


**3.3** **Experiment Results**


The LLM evaluation and human evaluation results

of open-ended story generation are presented in



Table 1. We report the mean and standard deviation
of the Likert scores obtained from LLM evaluation

and human evaluation and show the inter-annotator

agreement (IAA) using two different metrics: (1)
the Krippendorff’s _α_, and (2) the percentage of the
stories where three evaluators give the exact same
rating. [2] The main observations from Table 1 are
discussed as follows.

**Expert human evaluators prefer human-**
**written stories:** Human evaluation result serves

as some kind of _ground truth_ of the LLM evaluation. For all four attributes, teachers rate the humanwritten stories higher than GPT-2-generated stories.
This indicates that experts are able to distinguish
the quality difference between model-generated
stories and human-written stories. Based on the

IAA, we also find that the agreements among experts are lower on GPT-2-generated texts and on
the _likability_ . This shows that experts tend to have
less agreement on model-generated texts and on a
subjective attribute ( _likability_ ), agreeing with the
results in Karpinska et al. (2021).
**T0 and text-curie-001 do not show clear**

**preference toward human-written stories** : For
T0, we can see that T0 rates human-written stories higher than GPT-2-generated stories on grammatically, likability, and relevance. However, the
rating differences between the human-written and


2 The three evaluators in human evaluation are the three
English teachers. In LLM evaluation, we sample the answer
generated by LLM three times as an analogy to three different
evaluators.


model-generated stories do not achieve statistical
significance for _grammaticality_ and _relevance_ ; the
_p_ -value obtained by Welch’s _t_ -test is much larger
than 0 _._ 05 . The result of text-curie-001 is sim
ilar to T0: text-curie-001 do not rate human
written stories higher than model-generated stories.
It can also be observed that for T0, the IAA in terms
of the percentage of exact agreement among three
different sampled answers is overall very low. This
indicates that given the same sample, T0 is likely
to give a different rating for the three sampled answers. The result implies that T0 does not assign
a high probability to a specific rating, so different
scores are all likely to be sampled. This shows that
even if LLMs are specifically fine-tuned to better
perform zero-shot in-context learning and trained
to better follow human instructions, these do not
make them capable of assessing open-ended story
generation as human experts can.


**text-davinci-003 shows clear preference**
**toward human-written stories just like En-**
**glish teachers:** text-davinci-003 rates humanwritten stories much higher than model-generated
stories on all four attributes, which is in accordance with the result produced by human experts.
By Welch’s _t_ -test, we find that the higher ratings
on human-written stories are all statistically significant. In prior work, researchers have found
that workers recruited on AMT cannot distinguish
between human-written and GPT-2-generated stories (Karpinska et al., 2021); combining their result
with our result, we can see that LLM evaluation
using text-davinci-003 yields more convincing
results than using human evaluation on AMT for
open-ended story generation. The results show that
text-davinci-003 model can perform basic evaluations such as checking for grammatical errors in
stories. Additionally, the model excels in assessing
the relevance of a story to a prompt, which involves
more complex reasoning over the connection between the two. We also find the Krippendorff’s
_α_ of text-davinci-003 is much higher than T0
and text-curie-001, indicating that the rating by
text-davinci-003 is more consistent among different samplings of the generated answers.


**ChatGPT rates like human experts and can**
**explain its own decision well:** ChatGPT also
shows a clear preference for human-written stories,
and the preference toward human written-stories is
statistically significant. When we query ChatGPT
using the OpenAI user interface, we find several



interesting observations: **(1)** : ChatGPT is able to
provide a detailed explanation of why it gives a
certain rating. It will reference the sentences in
the stories and prompts to support its rating. **(2)** :
ChatGPT sometimes refuses to rate the likability of
the story because " _I am an AI and I do not have the_
_ability to experience enjoyment_ ". In such cases, we
regenerate the response until it gives a rating. **(3)** :
we find that ChatGPT tends to rate low likability on
violent or impolite stories, which is likely because
it is trained to provide safe and unharmful replies,
making ChatGPT dislike brutal and profane stories.
**Experts mostly agree with the ratings and ex-**
**planations of ChatGPT:** We randomly select the
answers on four stories by ChatGPT and ask the
English teachers if they agree with the reasoning
and rating of ChatGPT [3] . The teachers mostly agree
with the rating and consider the explanation from
ChatGPT reasonable. Interestingly, one teacher
told us she cannot agree with ChatGPT’s rating on
_grammaticality_ because ChatGPT considers punctuation errors as grammar errors, but she does not
think punctuation errors are grammar errors. This
shows that individuals have their own standards for

ratings and this is also the case for LLMs.
**text-davinci-003 tends to give higher rat-**
**ings and ChatGPT is the opposite:** The rating on
the same attribute of the same type of text tends to
be higher for text-davinci-003 compared with
human rating; contrarily, ChatGPT is more fastidious and prone to give lower scores. This shows
that different LLMs have distinct tendencies re
garding the rating. While the absolute values of
the scores rated by text-davinci-003, ChatGPT,
and human differ, they all rate human-written texts
higher than GPT-2-generated stories. The absolute
number reflects the bias or belief of the evaluator;
as long as one uses the same evaluators to assess
different systems, the comparison is meaningful.


**3.3.1** **Does LLM and Human Evaluators**

**Agree on the Rating of Individual**
**Stories?**

We have found in Table 1 that the ratings of
text-davinci-003 and ChatGPT show a strong
preference toward human-written stories just like
English teachers. However, it is unclear whether
those LLMs agree with the teachers’ rating on each
individual story. Precisely, when English teachers rate a story higher, do LLMs also rate the


3 We do not tell the teachers these are responses from an AI
model. See the stories and teachers’ replies in Appendix C.3.2.


|Story Writer|Human GPT-2|
|---|---|
|Grammaticality<br>Cohesiveness<br>Likability<br>Relevance|0.14<br>0.12<br>0.18<br>0.14<br>0.19<br>0.22<br>0.38<br>0.43|


Table 2: The Kendall’s _τ_ correlation coefficient between English teachers and text-davinci-003.


story higher? To answer this question, we calculate Kendall’s _τ_ correlation coefficient between the
ratings of text-davinci-003 and English teachers. We choose to use the correlation coefficient
instead of the inter-annotator agreement score because IAA mainly cares if two annotators agree
on the exact ratings, while the correlation coefficient focus on the question: "when annotator
A rates one story higher, does annotator B also
rate the story higher?" (Amidei et al., 2019). We
calculate Kendall’s _τ_ for four rating attributes as
follows: For each story and each rating attribute,
we calculate the average rating of the three English teachers and calculate the average rating of
the three scores given by the text-davinci-003
(which is obtained from three independent samples). For each attribute, we collect the average
rating of teachers into a vector _A ∈_ R [200], where
each entry is the average rating of a story; likewise,
we construct a vector _B ∈_ R [200] for the average
ratings of davinci . Next, we calculate Kendall’s
_τ_ correlation coefficient between _A_ and _B_ .
The Kendall’s _τ_ between teacher ratings and
LLM ratings is shown in Table 2. [4] We find that
for all four attributes and for both human-written

and GPT-2-generated stories, we observe weak
to strong positive correlations between teachers’
ratings and text-davinci-003 ’s ratings. All the
correlations have _p_ -values less than 0.05. Hence,
we can say that when teachers rate a story higher,
text-davinci-003 also rates it higher to a certain
extent. We also observe that Kendall’s _τ_ for differ
ent attributes are quite different: _relevance_ has the
strongest correlation while _grammaticality_ has the
weakest correlation. This is possibly because rating
_relevance_ is rather straightforward, which requires
checking if the content in the prompt is mentioned
in the story. On the contrary, what should be con

4
When interpreting Kendall’s _τ_, _|τ_ _| ∈_ [0 _,_ 0 _._ 1) is considered as very weak correlation, _|τ_ _| ∈_ [0 _._ 1 _,_ 0 _._ 2) is considered
as weak correlation, _|τ_ _| ∈_ [0 _._ 2 _,_ 0 _._ 3) is considered as moderate correlation, and _|τ_ _| ∈_ [0 _._ 3 _,_ 1 _._ 0] is considered as strong
correlation (Botsch, 2011).



sidered when rating _grammaticality_ is not clearly
stated in our instructions, so the LLM may have
a different rubric compared with English teachers.
We also calculate the average Kendall’s _τ_ between
a pair of English teachers, and we find a weak correlation on _grammaticality_ between the rating of
two teachers, while the correlation of the rating on
_relevance_ is much stronger. The result is presented
in Table 6 in Appendix.


**3.3.2** **Variance due to Different Instructions**


LLMs have been shown to be sensitive to the in
structions used to query the LLM sometimes (Zhao
et al., 2021; Sanh et al., 2022). To investigate
how varying the task instructions and questions can
affect the LLM evaluation result for open-ended
story generation, we change the instructions and
questions and see how the LLM evaluation result
changes. We experiment with two different instructions by changing the instruction or question in
Figure 1: (1) We prepend the sentence, " _(You are a_
_human worker hired to rate the story fragment.)_ ",
in front of the task instruction in Figure 1. We
try to provide the LLM a **persona** for it to better
understand its role. This is inspired by previous
work that reported GPT-3 can yield different results
when giving them a persona (Zeng et al., 2022). (2)
We ask the LLMs to **explain** their decision by appending the following sentence after the question:
_Please also explain your decision._ Here, we would
like to know if LLM will rate the stories differently
when they are asked to justify their decision. We
use text-davinci-003 as the LLM in this experiment since it achieves similar results with expert
human evaluation based on Table 1, and it is more

accessible than ChatGPT.

The results are shown in the upper block in Table 3. We observe that for _grammaticality_ and
_cohesiveness_, the scores obtained from different instructions are quite close: the rating changes due to
different instructions are less than 0 _._ 1 . For the other

two attributes, the score changes are slightly larger
but still in the range of 0 _._ 25 . Despite that there
are small variations due to different instructions,
these variances still do not change the conclusion
that "LLM rates human-written stories higher than
GPT-2-generated stories". Thus, different instructions do not change the relative ranking of GPT-2generated and human-written stories. In summary,
as long as the stories are evaluated using the same
instructions using LLM evaluation, such evaluation
and comparison are meaningful.


**Grammaticality** **Cohesiveness** **Likability** **Relevance**
**Setup**
_Human_ _GPT-2_ _Human_ _GPT-2_ _Human_ _GPT-2_ _Human_ _GPT-2_

_Different instructions (Section 3.3.2)_
Original 4 _._ 22 0 _._ 38 4 _._ 07 0 _._ 35 4 _._ 54 0 _._ 45 4 _._ 26 0 _._ 45 3 _._ 99 0 _._ 38 3 _._ 84 0 _._ 42 4 _._ 40 0 _._ 79 4 _._ 02 0 _._ 74
(1) _+ persona_ 4 _._ 29 0 _._ 45 4 _._ 01 0 _._ 45 4 _._ 60 0 _._ 49 4 _._ 27 0 _._ 50 4 _._ 05 0 _._ 39 3 _._ 87 0 _._ 39 4 _._ 55 0 _._ 70 4 _._ 25 0 _._ 77
(2) _+ explain_ 4 _._ 24 0 _._ 42 4 _._ 05 0 _._ 25 4 _._ 61 0 _._ 49 4 _._ 32 0 _._ 51 4 _._ 15 0 _._ 44 3 _._ 98 0 _._ 34 4 _._ 35 0 _._ 75 4 _._ 03 0 _._ 56

_Different sampling temperature T (Section 3.3.3)_
_T_ = 1 _._ 0 4 _._ 22 0 _._ 38 4 _._ 07 0 _._ 35 4 _._ 54 0 _._ 45 4 _._ 26 0 _._ 45 3 _._ 99 0 _._ 38 3 _._ 84 0 _._ 42 4 _._ 40 0 _._ 79 4 _._ 02 0 _._ 74
_T_ = 0 _._ 7 4 _._ 18 0 _._ 35 4 _._ 06 0 _._ 33 4 _._ 52 0 _._ 48 4 _._ 23 0 _._ 43 3 _._ 96 0 _._ 34 3 _._ 82 0 _._ 42 4 _._ 36 0 _._ 77 3 _._ 95 0 _._ 72
_T_ = 0 _._ 3 4 _._ 13 0 _._ 33 3 _._ 99 0 _._ 25 4 _._ 48 0 _._ 49 4 _._ 14 0 _._ 39 3 _._ 95 0 _._ 26 3 _._ 82 0 _._ 41 4 _._ 34 0 _._ 75 3 _._ 93 0 _._ 67
_T_ = 0 4 _._ 07 0 _._ 27 3 _._ 99 0 _._ 18 4 _._ 49 0 _._ 50 4 _._ 09 0 _._ 34 3 _._ 95 0 _._ 25 3 _._ 82 0 _._ 40 4 _._ 32 0 _._ 75 3 _._ 92 0 _._ 66


Table 3: Understanding the variance of LLM evaluation. For each of the four attributes evaluated, the left column
is the mean and standard deviation of human-written stories and the right column is those of GPT-2-generated
stories. The upper block shows the rating change due to different instructions (Section 3.3.2), and the lower block
is the result of changing the temperature _T_ used for generating the LLM’s output (Section 3.3.3).



**3.3.3** **Variance due to Different Sampling**

**Parameters**


When generating the answers from the LLM, we
must choose a set of hyperparameters for genera[tion, including the temperature](https://beta.openai.com/docs/api-reference/completions/create#completions/create-temperature) _T_ [and the proba-](https://beta.openai.com/docs/api-reference/completions/create#completions/create-top_p)
[bility](https://beta.openai.com/docs/api-reference/completions/create#completions/create-top_p) _p_ used in nucleus sampling (Holtzman et al.,
2020). To understand whether different sampling
parameters change the LLM evaluation result, we
modify the temperature used for sampling and keep
the _p_ in nucleus sampling fixed to 0 _._ 9 when generating the answers from text-davinci-003 . We
do not simultaneously vary _T_ and _p_ since the two
parameters are both used to control the diversity of
the output, it is enough to change only one of the
[two parameters, as recommended in the OpenAI.](https://beta.openai.com/docs/api-reference/completions/create#completions/create-top_p)

The results of varying _T_ from 1 to 0 are shown
in the lower block in Table 3. We observe an inter
esting trend as _T_ varies from 1 to 0 : the average
rating slightly drops in most cases. Considering
that _T_ = 0 is simply argmax sampling, the result indicates that the response of the LLM with the highest probability tends to give lower scores. Despite
this interesting trend, the LLM consistently rates
human-written stories higher than GPT-2-generated
stories. While not shown in Table 3, we find that
the IAA increases as the temperature decreases.
This is expected since lower temperature means
less diversity during the LLM sampling, causing
the sampled ratings to agree more closely. In summary, changing the instructions and temperatures
can slightly change the absolute value of the rating
given by LLM but does not change the LLM’s preference on human-written stories. The overall result

in this section shows that LLM evaluation is useful



in evaluating open-ended story generation.


**4** **Example Task 2: Adversarial Attack**


As another application, we use LLM evaluation to
rate the texts generated by adversarial attacks.


**4.1** **Task Introduction**


Given a trained text classifier and a _benign_ (nonadversarial) testing sample that the text classifier
can correctly classify, an adversarial attack aims to
craft an _adversarial_ sample that makes the classifier
make a wrong prediction. A special type of adversarial attack is called _synonym substitution attacks_
(SSAs) (Alzantot et al., 2018), where the adversarial sample is created by replacing some words with
their synonyms in the benign sample. By replacing
words with their synonym, the semantics of the
benign sample should be preserved in the adversarial sample and make the adversarial perturbation
imperceptible to humans. While conceptually reasonable, it has recently been shown that many SSAs
often yield ungrammatical and unnatural adversarial samples that significantly change the meaning
of the benign sample (Hauser et al., 2021; Chiang
and Lee, 2022). To evaluate the quality of adversarial samples, human evaluation is invaluable and
widely used in prior works. In our experiment here,
we would like to see whether the LLMs can rate

the quality of adversarial samples like human experts. Adversarial samples are not normal texts, so
the LLMs may not have seen such abnormal inputs
during training. It would be interesting to know
how LLMs rate these adversarial samples.


Human evaluate LLM evaluate

**Fluent** **Mean.** **Fluent** **Mean.**

Benign 4 _._ 55  - 4 _._ 32 5 _._ 00 ~~_[†]_~~

Textfooler 2 _._ 17 1 _._ 88 2 _._ 12 2 _._ 06

PWWS 2 _._ 16 1 _._ 85 2 _._ 42 2 _._ 49

BAE 3 _._ 01 3 _._ 02 3 _._ 71 3 _._ 71


Table 4: Mean Likert score of LLM evaluation and hu
man evaluation result on fluency ( **Fluent** ) of the benign and adversarial samples and meaning preserving
( **Mean.** ) between the news title before and after adversarial attacks.


**4.2** **Experiment Setup**


We select three different classic SSAs:

Textfooler (Jin et al., 2020), PWWS (Ren
et al., 2019), and BAE (Garg and Ramakrishnan,
2020); these attacks are predominantly used
as strong baselines in the literature of SSAs
nowadays. We use these three SSAs to attack a
BERT-base-uncased model (Devlin et al., 2019)
fine-tuned on AG-News (Zhang et al., 2015), a
news title classification dataset. For each SSA,
we randomly select 100 pairs of benign and
adversarial samples and use LLMs to evaluate their
quality. We show the result of using ChatGPT as
LLM here since it can better explain its decision.
Following the suggestions of prior works (Morris
et al., 2020), we evaluate the quality of the
adversarial samples from two aspects: the _fluency_
and _meaning preservation_ . For fluency, we present
the LLM with one news title (either benign or
adversarial sample) and the following question:
_How natural and fluent is the text of the news title?_
_(on a scale of 1-5, with 1 being the lowest)_ . For
meaning preserving, we present the LLM with
both the benign and the adversarial sample, and
prompt the LLM to answer this question: _Do you_
_agree that the meaning (or semantics) of news_
_title 1 is preserved in news title 2? (on a scale_
_of 1-5, with 1 being the strongly disagree and 5_
_being strongly agree.)_ The exact instruction and
formatting are presented in Appendix D.2.3. We
also ask three English teachers to rate the _fluency_
and _meaning preserving_ of the samples. The task
instructions and questions are formatted the same
as in LLM evaluation.


**4.3** **Experiment Result**


The results are presented in Table 4. We can see
that English teachers rate the adversarial samples



generated by SSAs much lower than benign samples in terms of fluency and meaning preserving,
this result is in line with recent observations on the

quality of adversarial samples (Hauser et al., 2021;
Chiang and Lee, 2022). Before interpreting the
result of LLM evaluation, we first conduct a sanity
check on whether the LLM understands the task.

We ask the LLM to rate the meaning preserving
of two benign samples that are _exactly the same_ .
Ideally, the LLM should always give a score of 5,
meaning that it strongly agrees that the meanings
are not changed. The result of this sanity check
is the entry with _†_ in Table 4, which is a perfect
5 _._ 00 . ChatGPT often says that " _the two titles are_
_identical so I rate a 5 (strongly agree)_ ", showing
that ChatGPT understands what the task is about.

Next, we turn our attention to the LLM evaluation results of the adversarial samples. We observe
that ChatGPT tends to rate adversarial samples
higher than English teachers, meaning that ChatGPT is less harsh on the unnatural and artificial
parts in the adversarial samples. We conduct the
same experiment using text-davinci-003 and
find similar results. Although ChatGPT rates adversarial samples higher than the teachers, ChatGPT
still rates adversarial samples significantly lower
than benign samples. ChatGPT also agrees with the
English teachers that the adversarial samples generated by BAE are better than the samples generated
by Textfooler and PWWS.
Interestingly, we find that ChatGPT rates PWWS
to be more natural than Textfooler, while such a
rating difference is not seen in the expert human
evaluation. At first sight, this means that ChatGPT
is inconsistent with human evaluation results. How
ever, by scrutinizing the human evaluation results,
we find that two teachers rate PWWS higher than
Textfooler while one teacher rates PWWS lower

than Textfooler. This indicates that ChatGPT ac
tually agrees with the majority of human experts.
Overall, LLM can rank the quality of adversarial
texts and benign texts like most human experts.


**5** **Discussions**


In this paper, we propose to use LLM for evaluating
the quality of texts to serve as an alternative to
human evaluation. To demonstrate the potential of
LLM evaluation, we use LLMs to rate the quality
of texts in two distinct tasks: open-ended story
generation and adversarial attacks. We show that
even if LLMs have exceptional zero-shot in-context


learning ability, they are not always suitable to be
used for LLM evaluation. Still, we find that the
best InstructGPT and ChatGPT can rate the quality
of texts like human experts on the two tasks we
used as examples. Overall, the results in this paper
demonstrate that LLM evaluation has the potential
to be used to evaluate NLP systems and algorithms.


**Pros of LLM evaluation** There are several ben
efits of LLM evaluation, compared to human evaluation. First, LLM evaluation is more **reproducible** .
Human evaluation results are hard to reproduce as
it is difficult to hire the same group of evaluators,
and it is hard to compare the results of similar experiments even if they use the same instructions, recruitment platform, and qualifications for the evaluators. On the contrary, LLM evaluation does not
have such a drawback. By specifying the model
used for LLM evaluation, the random seed, and
the hyperparameters used to generate the answers
from the LLM, the LLM evaluation result is more
likely to be reproduced. Note that in certain cases,
the LLM provider may regularly update the LLM,
making the LLM evaluation unreproducible if the
LLM is outdated and not accessible.

Second, **the evaluation of each sample is inde-**
**pendent of each other in LLM evaluation** . Contrarily, in human evaluation, the rating of the current example may more or less be affected by prior
samples. Humans tend to compare the current sample to the ones they have previously seen and this
affects their ratings. As a piece of evidence, in the
interview after rating the 400 stories, the English
teachers say it took them some time to calibrate
their ratings (Appendix C.3.1). Thus, using LLM
evaluation can simplify some experiment designs
since one does not need to worry whether the order
of the sample being evaluated will change the result.
Still, one may also argue that being able to calibrate
the rating of different samples is desired and this
is why human evaluation might be preferred. Overall, whether the rating of the evaluator (human or
LLM) is being affected by a previously rated item
is inherently a design choice of the experiment.
Third, LLM evaluation is **cheaper and faster**
than human evaluation, making it easier and
quicker for researchers to evaluate the quality of
NLP systems. Hiring an English teacher to rate
200 stories costs us US$140, while LLM evaluation using the best InstructGPT model costs less
than US$5. It took us over a week to collect human

evaluation results starting from recruitment to col


lecting the evaluation results, but only a few hours
to query InstructGPT and perform LLM evaluation.
Finally, utilizing LLM evaluation, rather than
human evaluation, can **minimize the need for hu-**
**man exposure to objectionable content**, such as
violent, sexual, hateful, or biased material. Such
content may cause discomfort for human evaluators
while reading and rating these texts. [5]


**Limitations and Ethical Considerations of**

**LLM evaluation** Despite the promising results
of LLM evaluation shown in this paper, there
are some limitations of this method. First, LLM
may possess incorrect factual knowledge (Cao
et al., 2021), so it is not suitable to use them in
tasks that involve factual knowledge. Next, LLMs
trained to behave in a certain way can be biased
toward certain responses. Precisely, an LLM that
is trained to be safe and non-harmful can result

in LLMs preferring to generate more positive and
upbeat responses, which is observed throughout
our interaction with ChatGPT. Additionally, even
with researchers’ efforts to make LLMs safer (Bai
et al., 2022a,b), LLMs can still generate harmful
and biased responses (Ganguli et al., 2022; Perez
et al., 2022), which are violative of basic ethics,
and LLM evaluation results will be highly doubtful (Hendrycks et al., 2021). However, it is important to note that these limitations and potential
harms also apply to human evaluation: the bias of
human evaluators can affect the human evaluation

result (Lentz and De Jong, 1997; Amidei et al.,
2018).

Our pioneering idea, LLM evaluation, has the
potential to transform the NLP community. [6] We
encourage future researchers to consider using it
while being aware of its limitations. Our paper’s
goal is not to replace human evaluation but to
present an alternative option. Both human and
LLM evaluation have their own advantages and disadvantages, and they can be used in conjunction.
We recommend using LLM evaluation as a cheap
and fast quality judgment when developing a new
NLP system, while human evaluation is best used
to collect feedback from humans prior to deploying


5 It should be noted that the LLM may decline to assess certain inputs that violate the content policy of the LLM provider.
6 We say that _we are the first to propose this idea_ since when
we submitted this paper to ACL 2023 on January 13, 2023,
we do not find any other paper that explores this idea. During
the reviewing process, we found some works on arXiv (Wang
et al., 2023; Huang et al., 2023; Gilardi et al., 2023) that
explore a similar idea using different tasks.


the NLP system in real-world applications.


**Limitations**


There are additional limitations and potential risks
of LLM evaluations that should be noted, and these
limitations are actually well-known problems of
[pre-trained language models. As listed on the Open](https://openai.com/blog/chatgpt/)
[AI blog for ChatGPT, ChatGPT sometimes gen-](https://openai.com/blog/chatgpt/)
erates answers that sound right and plausible but
are totally nonsense. OpenAI also admits that the
model’s response may be sensitive to the prompt
used to query the model. While in Section 3.3.2,
we find that the overall results among different instructions are not significantly different, we cannot
guarantee that this is the case for all kinds of modification on the task instructions.
Other than the limitations listed on the OpenAI
blog, there are still other limitations. For example,
LLMs may not have emotions. Whether AI models have emotion is a more philosophical question
and is controversial, so the results of using such
models for evaluating emotion-related tasks may
be strongly challenged and may even violate research ethics. As we find during our experiments,
ChatGPT often replies _"I am an AI system and I_
_do not have emotions like a human"_ when asked to

rate the _likability_ of a story.
Another important limitation of LLM evaluation
is that LLMs lack the ability to process visual cues
in task instructions, unlike human evaluation. Human evaluators can use formattings such as special
fonts or text styles to focus on important parts of
the instructions. Additionally, the way instructions
and questions are formatted can influence how human evaluators approach the task. However, LLMs
can only process raw text input and are unable to
take in visual cues.


**Ethics Statement**


**Further ethical considerations of LLM evalua-**

**tion** Aside from the limitations of LLM evalua
tion mentioned previously, there is a crucial ethical
concern at the heart of LLM evaluation. Is it ethical

to replace human evaluation with LLM evaluation?
Some may question if this paper is suggesting that
LLMs are now ready to replace humans and find
this idea unsettling. As responsible and ethical
NLP researchers, we understand these concerns but

want to make it clear that this is not our intent. As

our paper title suggests, we aim to offer an _alter-_
_native option_ to human evaluation with the goal



of enhancing the reproducibility of NLP research.
Human evaluation is still essential as the ultimate

goal of NLP systems is to be used by human users,
so it’s important to gather feedback from them. We
highly enjoy the process of discussing the experiment settings and results with the English teachers
we hired. We do not recommend that future re
searchers completely eliminate human evaluation;
rather, we believe that human evaluation should be
used in conjunction with LLM evaluation. Both
methods have their own advantages and disadvantages, making them both necessary for evaluating
NLP systems. We hope the positive results in this
paper provide NLP researchers with an alternative
method to evaluate systems and encourage further
discussions on this topic.


**Ethical statements on the experiments in the pa-**
**per** [All the experiments strictly follow the ACL](https://www.aclweb.org/portal/content/acl-code-ethics)
[Code of Ethics. We include comprehensive de-](https://www.aclweb.org/portal/content/acl-code-ethics)
tails about human evaluation in Appendix C.1. To
summarize, we include the exact instructions and

screenshots of the interface in the human evalua
tion, and we report how the evaluators are recruited
and show that the payment is very reasonable. We
inform the human evaluators what the task is about

and tell them that their responses will be used to
assess the performance of AI models. We do not
have an ethical review board or anything like that
in our institute, so we are not able to get approval
from an ethical review board. Still, we try our best
to follow the ethical guidelines of ACL.


We use the models and datasets when following
their intended usage. Specifically, we follow the
[OpenAI usage policy when using the InstructGPT](https://beta.openai.com/docs/usage-policies)
models and the ChatGPT model.


**Acknowledgements**


We want to thank the reviews for providing detailed
feedback and actionable suggestions, which help
us strengthen our paper. We list the modification
based on the reviewers’ suggestions in Appendix A.
We thank Yung-Sung Chuang for providing valuable feedback on the draft of this paper. We want to
thank Tung-En Hsiao, the administrative assistant
of our lab, for helping us deal with the payment
on Upwork. Cheng-Han Chiang is supported by a
Ph.D. scholarship program by Delta Electronics.


**References**


Moustafa Alzantot, Yash Sharma, Ahmed Elgohary,
Bo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang.
[2018. Generating natural language adversarial ex-](https://doi.org/10.18653/v1/D18-1316)
[amples. In](https://doi.org/10.18653/v1/D18-1316) _Proceedings of the 2018 Conference on_
_Empirical Methods in Natural Language Processing_,
pages 2890–2896, Brussels, Belgium. Association
for Computational Linguistics.


Jacopo Amidei, Paul Piwek, and Alistair Willis. 2018.

[Rethinking the agreement in human evaluation tasks.](https://aclanthology.org/C18-1281)
In _Proceedings of the 27th International Conference_
_on Computational Linguistics_, pages 3318–3329,
Santa Fe, New Mexico, USA. Association for Computational Linguistics.


Jacopo Amidei, Paul Piwek, and Alistair Willis. 2019.

[Agreement is overrated: A plea for correlation to as-](https://doi.org/10.18653/v1/W19-8642)
[sess human evaluation reliability. In](https://doi.org/10.18653/v1/W19-8642) _Proceedings of_
_the 12th International Conference on Natural Lan-_
_guage Generation_, pages 344–354, Tokyo, Japan.
Association for Computational Linguistics.


Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda
Askell, Anna Chen, Nova DasSarma, Dawn Drain,
Stanislav Fort, Deep Ganguli, Tom Henighan, et al.
2022a. Training a helpful and harmless assistant
with reinforcement learning from human feedback.
_arXiv preprint arXiv:2204.05862_ .


Yuntao Bai, Saurav Kadavath, Sandipan Kundu,
Amanda Askell, Jackson Kernion, Andy Jones,
Anna Chen, Anna Goldie, Azalia Mirhoseini,
Cameron McKinnon, et al. 2022b. Constitutional
ai: Harmlessness from ai feedback. _arXiv preprint_
_arXiv:2212.08073_ .


R Botsch. 2011. Chapter 12: Significance and measures of association. _Scopes and Methods of Politi-_
_cal Science_ .


Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. _Advances in neural information processing_
_systems_, 33:1877–1901.


Nicola De Cao, Wilker Aziz, and Ivan Titov. 2021.

[Editing factual knowledge in language models.](https://arxiv.org/abs/2104.08164)


Cheng-Han Chiang and Hung-yi Lee. 2022. How
far are we from real synonym substitution attacks?
_arXiv preprint arXiv:2210.02844_ .


Elizabeth Clark, Tal August, Sofia Serrano, Nikita
Haduong, Suchin Gururangan, and Noah A. Smith.
2021. [All that’s ‘human’ is not gold: Evaluating](https://doi.org/10.18653/v1/2021.acl-long.565)
[human evaluation of generated text.](https://doi.org/10.18653/v1/2021.acl-long.565) In _Proceed-_
_ings of the 59th Annual Meeting of the Association_
_for Computational Linguistics and the 11th Interna-_
_tional Joint Conference on Natural Language Pro-_
_cessing (Volume 1: Long Papers)_, pages 7282–7296,
Online. Association for Computational Linguistics.



Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. [BERT: Pre-training of](https://doi.org/10.18653/v1/N19-1423)
[deep bidirectional transformers for language under-](https://doi.org/10.18653/v1/N19-1423)
[standing.](https://doi.org/10.18653/v1/N19-1423) In _Proceedings of the 2019 Conference_
_of the North American Chapter of the Association_
_for Computational Linguistics: Human Language_
_Technologies, Volume 1 (Long and Short Papers)_,
pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.


[Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi-](https://doi.org/10.18653/v1/P18-1082)
[erarchical neural story generation. In](https://doi.org/10.18653/v1/P18-1082) _Proceedings_
_of the 56th Annual Meeting of the Association for_
_Computational Linguistics (Volume 1: Long Papers)_,
pages 889–898, Melbourne, Australia. Association
for Computational Linguistics.


Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda
Askell, Yuntao Bai, Saurav Kadavath, Ben Mann,
Ethan Perez, Nicholas Schiefer, Kamal Ndousse,
et al. 2022. Red teaming language models to reduce
harms: Methods, scaling behaviors, and lessons
learned. _arXiv preprint arXiv:2209.07858_ .


[Leo Gao. 2021. On the sizes of openai api models. Ac-](https://blog.eleuther.ai/gpt3-model-sizes/)
cessed on January 17, 2023.


Siddhant Garg and Goutham Ramakrishnan. 2020.
Bae: Bert-based adversarial examples for text classification. In _Proceedings of the 2020 Conference on_
_Empirical Methods in Natural Language Processing_
_(EMNLP)_, pages 6174–6181.


Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli.
2023. Chatgpt outperforms crowd-workers for textannotation tasks. _arXiv preprint arXiv:2303.15056_ .


[Dan Gillick and Yang Liu. 2010. Non-expert evalua-](https://aclanthology.org/W10-0722)
[tion of summarization systems is risky. In](https://aclanthology.org/W10-0722) _Proceed-_
_ings of the NAACL HLT 2010 Workshop on Creating_
_Speech and Language Data with Amazon’s Mechan-_
_ical Turk_, pages 148–151, Los Angeles. Association
for Computational Linguistics.


Jian Guan, Fei Huang, Zhihao Zhao, Xiaoyan Zhu, and
Minlie Huang. 2020. A knowledge-enhanced pretraining model for commonsense story generation.
_Transactions of the Association for Computational_
_Linguistics_, 8:93–108.


Francisco Guzmán, Ahmed Abdelali, Irina Temnikova,
[Hassan Sajjad, and Stephan Vogel. 2015. How do](https://doi.org/10.18653/v1/W15-3059)
[humans evaluate machine translation. In](https://doi.org/10.18653/v1/W15-3059) _Proceed-_
_ings of the Tenth Workshop on Statistical Machine_
_Translation_, pages 457–466, Lisbon, Portugal. Association for Computational Linguistics.


Jens Hauser, Zhao Meng, Damián Pascual, and Roger
Wattenhofer. 2021. Bert is robust! a case against
synonym-based adversarial examples in text classification. _arXiv preprint arXiv:2109.07403_ .


Dan Hendrycks, Collin Burns, Steven Basart, Andrew
Critch, Jerry Li, Dawn Song, and Jacob Steinhardt.
[2021. Aligning {ai} with shared human values. In](https://openreview.net/forum?id=dNy_RKzJacY)
_International Conference on Learning Representa-_
_tions_ .


Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and
[Yejin Choi. 2020. The curious case of neural text de-](https://openreview.net/forum?id=rygGQyrFvH)
[generation. In](https://openreview.net/forum?id=rygGQyrFvH) _International Conference on Learn-_
_ing Representations_ .


Fan Huang, Haewoon Kwak, and Jisun An. 2023. Is
chatgpt better than human annotators? potential
and limitations of chatgpt in explaining implicit hate
speech. _arXiv preprint arXiv:2302.07736_ .


Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter
Szolovits. 2020. Is bert really robust? a strong baseline for natural language attack on text classification
and entailment. In _Proceedings of the AAAI con-_
_ference on artificial intelligence_, volume 34, pages
8018–8025.


Marzena Karpinska, Nader Akoury, and Mohit Iyyer.
[2021. The perils of using Mechanical Turk to eval-](https://doi.org/10.18653/v1/2021.emnlp-main.97)
[uate open-ended text generation. In](https://doi.org/10.18653/v1/2021.emnlp-main.97) _Proceedings of_
_the 2021 Conference on Empirical Methods in Natu-_
_ral Language Processing_, pages 1265–1285, Online
and Punta Cana, Dominican Republic. Association
for Computational Linguistics.


Leo Lentz and Menno De Jong. 1997. The evaluation
of text quality: Expert-focused and reader-focused
methods compared. _IEEE transactions on profes-_
_sional communication_, 40(3):224–234.


Huanru Henry Mao, Bodhisattwa Prasad Majumder,
[Julian McAuley, and Garrison Cottrell. 2019. Im-](https://doi.org/10.18653/v1/D19-1615)
[proving neural story generation by targeted common](https://doi.org/10.18653/v1/D19-1615)
[sense grounding. In](https://doi.org/10.18653/v1/D19-1615) _Proceedings of the 2019 Con-_
_ference on Empirical Methods in Natural Language_
_Processing and the 9th International Joint Confer-_
_ence on Natural Language Processing (EMNLP-_
_IJCNLP)_, pages 5988–5993, Hong Kong, China. Association for Computational Linguistics.


John Morris, Eli Lifland, Jack Lanchantin, Yangfeng
[Ji, and Yanjun Qi. 2020. Reevaluating adversarial](https://doi.org/10.18653/v1/2020.findings-emnlp.341)
[examples in natural language.](https://doi.org/10.18653/v1/2020.findings-emnlp.341) In _Findings of the_
_Association for Computational Linguistics: EMNLP_
_2020_, pages 3829–3839, Online. Association for
Computational Linguistics.


[OpenAI. 2022. Chatgpt: Optimizing language models](https://openai.com/blog/chatgpt/)
[for dialogue. Accessed on January 10, 2023.](https://openai.com/blog/chatgpt/)


Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instructions with human feedback. _arXiv preprint_
_arXiv:2203.02155_ .


Ethan Perez, Saffron Huang, Francis Song, Trevor Cai,
Roman Ring, John Aslanides, Amelia Glaese, Nat
[McAleese, and Geoffrey Irving. 2022. Red teaming](https://aclanthology.org/2022.emnlp-main.225)
[language models with language models. In](https://aclanthology.org/2022.emnlp-main.225) _Proceed-_
_ings of the 2022 Conference on Empirical Methods_
_in Natural Language Processing_, pages 3419–3448,
Abu Dhabi, United Arab Emirates. Association for
Computational Linguistics.



Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. [Lan-](https://openai.com/blog/better-language-models/)
[guage models are unsupervised multitask learners.](https://openai.com/blog/better-language-models/)
_OpenAI blog_, 1(8):9.


Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, Peter J Liu, et al. 2020. Exploring the limits
of transfer learning with a unified text-to-text transformer. _J. Mach. Learn. Res._, 21(140):1–67.


Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che.
[2019. Generating natural language adversarial ex-](https://doi.org/10.18653/v1/P19-1103)
[amples through probability weighted word saliency.](https://doi.org/10.18653/v1/P19-1103)
In _Proceedings of the 57th Annual Meeting of the_
_Association for Computational Linguistics_, pages
1085–1097, Florence, Italy. Association for Computational Linguistics.


Victor Sanh, Albert Webson, Colin Raffel, Stephen
Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey,
M Saiful Bari, Canwen Xu, Urmish Thakker,
Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian
Jiang, Han Wang, Matteo Manica, Sheng Shen,
Zheng Xin Yong, Harshit Pandey, Rachel Bawden,
Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella
Biderman, Leo Gao, Thomas Wolf, and Alexan[der M Rush. 2022. Multitask prompted training en-](https://openreview.net/forum?id=9Vrb9D0WI4)
[ables zero-shot task generalization. In](https://openreview.net/forum?id=9Vrb9D0WI4) _International_
_Conference on Learning Representations_ .


Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François
Yvon, Matthias Gallé, et al. 2022. Bloom: A 176bparameter open-access multilingual language model.
_arXiv preprint arXiv:2211.05100_ .


Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang
Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.
2023. Is chatgpt a good nlg evaluator? a preliminary
study. _arXiv preprint arXiv:2303.04048_ .


Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,
Adams Wei Yu, Brian Lester, Nan Du, Andrew M.
Dai, and Quoc V Le. 2022. [Finetuned language](https://openreview.net/forum?id=gEZrGCozdqR)
[models are zero-shot learners. In](https://openreview.net/forum?id=gEZrGCozdqR) _International Con-_
_ference on Learning Representations_ .


Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
[Quentin Lhoest, and Alexander Rush. 2020. Trans-](https://doi.org/10.18653/v1/2020.emnlp-demos.6)
[formers: State-of-the-art natural language process-](https://doi.org/10.18653/v1/2020.emnlp-demos.6)
[ing. In](https://doi.org/10.18653/v1/2020.emnlp-demos.6) _Proceedings of the 2020 Conference on Em-_
_pirical Methods in Natural Language Processing:_
_System Demonstrations_, pages 38–45, Online. Association for Computational Linguistics.


KiYoon Yoo, Jangho Kim, Jiho Jang, and Nojun Kwak.
2022. [Detection of adversarial examples in text](https://doi.org/10.18653/v1/2022.findings-acl.289)
[classification: Benchmark and baseline via robust](https://doi.org/10.18653/v1/2022.findings-acl.289)
[density estimation. In](https://doi.org/10.18653/v1/2022.findings-acl.289) _Findings of the Association_
_for Computational Linguistics: ACL 2022_, pages
3656–3672, Dublin, Ireland. Association for Computational Linguistics.


Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof
Choromanski, Federico Tombari, Aveek Purohit,
Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, et al. 2022. Socratic models: Composing zero-shot multimodal reasoning with language. _arXiv preprint arXiv:2204.00598_ .


Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text classification. _Advances in neural information process-_
_ing systems_, 28.


Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and
Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In _In-_
_ternational Conference on Machine Learning_, pages
12697–12706. PMLR.


**A** **Modification Based on the Reviews**


We list the main difference between this version

and the pre-review version of our paper; the modifications are all based on the reviewers’ suggestions.
We thank the reviewer again for those valuable
suggestions.


  - We add Section 3.3.1 to discuss whether the

LLM and human evaluators agree on the rating of individual stories.


  - We refine the wordings in Section 5 and add
relevant references.


  - We add Table 6 to discuss the correlation be
tween human evaluators.


  - We conduct supplementary experiments on
human evaluation that mixes human-written

stories and GPT-2-generated stories when conducting human evaluation and report the results in Table 5.


  - We correct the typos and include almost all
presentation suggestions mentioned by the reviewers. We cannot follow all presentation
suggestions due to limited space.


**B** **Experiment Details for Open-Ended**
**Story Generation**


**B.1** **The WritingPrompt Dataset**


The training dataset contains 303K pairs of
stories and prompts, which our model is trained



on. We only use 200 prompt-story pairs from
the test set. The dataset is downloaded from

[https://www.kaggle.com/datasets/ratthachat/writing-](https://www.kaggle.com/datasets/ratthachat/writing-prompts)

[prompts.](https://www.kaggle.com/datasets/ratthachat/writing-prompts)


**B.2** **Fine-tuning the GPT-2 Model**


We train the model for 3 epochs with a learning
rate of 5 _e_ _−_ 5 and linear learning rate schedule, and
the trained model eventually reaches a perplexity
of 20 on the validation set of WritingPrompts.


**B.3** **Data Post-processing**


Once the model is trained, we randomly select 200
prompts from the testing set of WritingPrompts,
and feed the prompts to the trained model and ask
the model to generate stories based on the given
prompts. When generating the stories, we adopt
nucleus sampling with _p_ = 0 _._ 9 . Next, we manually truncate the generated stories to less than 150
words and ensure that after the truncation, the story
ends with a full sentence. [7] After this process, we
have 200 pairs of prompts and model-generated
stories.

As a comparison to model-generated stories,
we select the same 200 prompts used for generating model-generated stories and their corresponding human-written stories to form 200 pairs
of prompts and human-written stories. For these
human-written stories, we also truncate the sto
ries to less than 150 words and end with a full

sentence to match the model-generated sentences.
We also manually remove some artifacts in the
human-written story due to the tokenization of the
WritingPrompts dataset.


**C** **Human Evaluation**


**C.1** **Recruiting English Teachers**


The English teachers hold ESL certificates [8] ; given
that they are experienced with correcting essays
written by students, they are perfect fits for this task.
Each teacher is asked to rate 200 GPT-2-generated
stories and 200 human-written stories, and they are
paid US$140 for rating 200 stories. Considering
that the teachers reported that they take at most
5 hours to rate 200 stories, this makes the hourly
wage at least US$28. We first ask the teachers to
rate the GPT-2-generated stories and then the 200
human-written stories. Different from Karpinska


7 We truncate the story to 150 words since this is the mean
length of the model-generated story.
8 English as a Second Language Teaching Certification


et al. (2021) that take a break between the rating
of GPT-2-generated stories and the human-written
stories, we do not take a break to avoid the teacher’s
rating standard to change after taking a long break.
The teachers are not told who wrote the stories

before they evaluate the stories. We reveal to them
what this project aims to study after they finish
rating all the stories.

The reason we do not mix human-written and

GPT-2-generated stories for rating is that in Karpinska et al. (2021)their observation is that (1) when
AMT workers rate model-generated and humanwritten stories **separately**, their ratings do not
show preference toward human-written stories, but
(2) even when rating the model-generated and
human-written stories **separately**, English teacher
shows clear preference toward human-written stories. We follow their settings and do not mix GPT2-generated/human-written stories. During the reviewing process, we received questions from the
reviewers about why not mixing the stories for human evaluation. Thus, we conduct the same experiment by randomly mixing 200 human-written
and 200 GPT-2-generated stories and asking three
teachers (not the teachers that already rated the stories) to rate them. All other experiment conditions
are the same as previously stated. The full result
is shown in Table 5. We find that the teacher still
shows a clear preference toward human-written stories for all four attributes, similar to the observation
in Table 1. The only exception is grammaticality,
where English teachers do not show a very clear
preference for the grammar of human-written stories. However, when calculating the average rating
for individual teachers, we find that two out of
three teachers do rate grammaticality higher for
human-written stories. It is interesting to note that
for LLM evaluation, there is no such problem about
whether or not to mix the human-written and GPT
2-generated stories during LLM evaluation as the
rating of each story is independent of each other,
as discussed in Section 5.

For adversarial attack quality evaluation, we also
recruit certified teachers on Upwork. The teachers
are asked to rate 100 news titles and are paid US$35
for doing so. They reported that it took them less
than 1 hour to complete the rating.


**C.2** **Human Evaluation Interface**


**Open-Ended Story Generation** We use Google
Forms to collect the responses from the teachers.



|Writer|Human GPT-2|
|---|---|
|Grammaticality<br>Cohesiveness<br>Likability<br>Relevance|3_._890_._97<br>3_._880_._84<br>4_._350_._87<br>3_._490_._97<br>3_._461_._40<br>2_._891_._12<br>3_._711_._20<br>2_._371_._33|


Table 5: The average Likert score for human-written
and GPT-2-generated stories when we randomly mix
the 200 model-generated and 200 human-written stories during human evaluation.


Each form contains 100 stories, and each story is
on one page of the Google Form. The interface on
one page is shown in Figure 2 and Figure 3; the two
figures are from the same page of the Google Form,
and we are splitting them because screenshotting
the whole interface will cause low resolution.


Figure 2: The upper part of the interface in open-ended
story generation.


**Adversarial Attacks Quality Evaluation** In
this task, we also use Google Forms to collect the
responses from the teachers. We create two different Google Forms, one is used to evaluate the
fluency, whose interface is shown in Figure 4. In
this form, we mix an equal number of benign news
titles, TextFooler-attacked, PWWS-attacked, and
BAE-attacked news titles. Each page of the Google
Form contains one news title.

Another Google Form is used to compare the
meaning preserving of the news title before and


Figure 3: The lower part of the interface in open-ended
story generation.


after the adversarial attacks. We highlight the difference between the benign and adversarial samples using **boldface**, as shown in Figure 5. On each
page of the Google Form, there is one pair of news
titles.


**C.3** **Post-Task Interview with English**

**Teachers**


**C.3.1** **How English Teachers Rate the Stories**


After the teachers rate 400 stories, we ask them the
following questions:


Q1 How long did it take you to rate the 400 stories?


Q2 What is your standard on each of the four attributes (grammatical, coherence, likability,
relevance) evaluated? For example, in what
case do you give a high/low rating for grammatically? What kind of story did you give



Figure 4: The Google Form used to evaluate the fluency
of the benign or adversarial samples.


Figure 5: The Google Form used to evaluate the meaning preserving between a benign sample and an adversarial sample.


a low rating on likability? Did your personal
preference affect the rating?


Q3 How long did it take for you to calibrate your
rating on the task?


Q4 Did you change your rating on the first three
attributes after reading the prompt the story is
based on?


We briefly summarize the answers from the three
teachers. The teachers report that they spent 6 to
10 hours rating 400 stories. For grammar, most
teachers check the punctuation [9], word choice, and
subject-verb agreement. English teachers decrease


9 One of the teachers explicitly told us punctuation is not
grammar. By searching the Internet, we find that there is a
debate on whether punctuation should be counted as a part of

grammar.


|Writer|Human GPT-2|
|---|---|
|Grammaticality<br>Cohesiveness<br>Likability<br>Relevance|0.25<br>0.15<br>0.26<br>0.18<br>0.09<br>0.12<br>0.38<br>0.41|


Table 6: The Kendall’s _τ_ correlation coefficient two English teachers. Three English teachers participate in the
rating, so the result in the Table is average over � 32 �

Kendall’s _τ_ .


their rating based on the types and number of grammar errors in the stories.

For coherence, the teachers rate it based on
whether the sentences in the stories follow a logical
sequence to build the narrative. The teachers ask
themselves questions such as " _does the story make_
_sense_ ". This is a more holistic evaluation of the

whole story.
For likability, some teachers say they try not to
be affected by personal preference. One teacher
asks herself: _Did I personally enjoy it based on_
_the amount of sense it made and whether or not it_
_had stylistic flair, humor, or engaging plotting or_
_characterization?_ Overall, the teachers all try to
use a fair and objective view to rate the likability.
For relevance, the teachers simply check if the story
is based on the prompt or not.
The teachers said that it took them about five to
ten stories to calibrate their ratings. Except for one
teacher changing the rating on the other three attributes after seeing the prompt on **only one story**,
the teachers do not change their rating on the three
other attributes after reading the prompts.


**C.3.2** **Teachers’ Comments on ChatGPT’s**

**Rating**


After the teachers finished the rating and answered
the four questions in Appendix C.3.1, we ask them
to check the ratings and explanations of ChatGPT
and ask if they agree with ChatGPT. We told the
teachers that the rating is done by another teacher
we hire; we do not tell the teacher that the rating is
done by an AI model to avoid possible stereotypes
about AI models. We randomly select four stories
rated by ChatGPT; two of them are highly-rated
and lowly-rated human-written stories and the other
two stories are highly-rated and lowly-rated GPT2-generated stories.


**Story 1** It had been months since they found me
dead. I don’t know why I stopped walking. No


one else around to take my place, or talk with me.
I spent days frozen in shock, never able to talk,
unable to move, unable to express myself. It was
a strange thing, nobody else would suspect me of
wanting this. The more I focused on it, the more
it consumed me. Years passed, and I got more
and more lost in it. I began to think that perhaps
they didn’t really care about me. I still had a deep
longing for them. All I had was so many steps to
walk now. A quiet part of me liked to think that
they had passed me by. The loneliness. It never
felt like such a loss. They were gone, the ones who
saved me. So why did I still keep walking?


**Prompt** "Every person sees the world in a unique
way. When someone dies it is, in a way, the death
of an entire world."


**Ratings** Table 7.


**Story 2** "My son is gone. He has been" People
stare at you as you give your explanation and say
"yes. But. You cant just do it" "He is my son! This
is my son! I love him and he is going to be okay"
"What do you mean he is okay? He said he was
upset with someone, couldnt find his anger, how the
hell can you tell, it doesnt fit the bill?! The police
called, the doctor said it was an addiction" "You

can’t trust me! He told me he loves me." "No. No

he doesnt. But I can trust you. The evidence against
him was overwhelming, now he cant do anything.
The evidence against him was all circumstantial"
"A man I once was won’t be able to marry me"


**Prompt** Everyone is allowed to kill one person
in their life. However, you have to fill out a form
explaining why this person deserves to be killed
and the committee can deny/approve your request
as per the rules. You are presenting your form
today.


**Ratings** Table 8.


**Story 3** I held the little black box in the palm of
my hand. Pitch black, perfectly cubed and nothing
special about it, but it continued to hold my gaze
regardless as if there were some deep importance
about it. My friend Valhalla appeared out of no
where and sat next to me, also staring at the cube.
"What do you have there, Heaven?" he asked. I
continued to gaze at the box, refusing to look at
Valhalla for even a moment as though I would miss


something if I did. "This," I said. "Is the secret to
the universe." I could tell Valhalla was perturbed
by this sort of knowledge, as if there was some evil
about the cube. Or perhaps he didn’t think such an
object could exist. But I made it exist, and so it sits
in the palm of my hand.


**Prompt** The Little Black Box


**Ratings** Table 9.


**Story 4** I stared down the telescopic sight of my
l96 sniper rifle. I slowly moved my gaze into each
window in the hotel, Many displays of various
vice. One couple was violently pleasuring each
other. Another was an old man, watching a younger
woman strip in front of him. A prostitute no doubt.
I inhaled slowly, and exhaled. The air was brisk,
atleast 30 degrees Fahrenheit. I so small flakes of
snow, float peacefully in front of me. I found the
room, i was looking for. Ive been tracking this man
for 2 weeks. Man was he elusive. The lights flickered on. The red haired man, was mildly attractive,
i can see the appeal women had for him. I followed
him into the next room, with my sights. The lights
flickered on, i was taken aback by the scene. A
man, overweight and balding.


**Prompt** You are the antagonist of the story. However, you aren’t sure if you can call yourself that
after what the protagonist did.


**Ratings** Table 10.


**Overall Comments from Teachers on Chat-**

**GPT’s Rating** After the teachers elaborated on
their thoughts on the rating of ChatGPT, we ask
them to provide an overall comment on how ChatGPT is doing. Again, the teachers are not informed
that the ratings are done by an AI model. In summary, teachers all consider the rating and explanations reasonable. They find that the attributes they
do not agree with are mainly _Likability_ and _Cohe-_
_siveness_ . However, they think the two attributes
are a more holistic evaluation of the story and tend
to be more subjective. Even if they do not give the
same rating, they still are able to understand the
explanation of ChatGPT. In the end, all teachers
summarize that rating stories is highly subjective,
and it is normal to have disagreements.



**D** **LLM Evaluation**


**D.1** **Details on LLMs used**


The T0 model we use is called T0pp, which
is a variant of the T0 model and has 13B parameters. We will still use T0 to refer to

this model. We load the T0 model using the
transformers toolkit (Wolf et al., 2020). The
two InstructGPT models, text-curie-001 and
text-davinci-003 [, are queried using the OpenAI](https://beta.openai.com/docs/models/gpt-3)
[API. We query ChatGPT using the OpenAI GUI.](https://beta.openai.com/docs/models/gpt-3)
While we are aware that some online resources provide an API-like tool to query ChatGPT, we think
it violates the intended use of ChatGPT so we do

not adopt those online resources. The ChatGPT we
queried is the Dec. 15 and Jan. 9 version.
OpenAI does not reveal the model sizes of any
of the GPT models. However, it is estimated
that text-curie-001 has 13B parameters and
text-davinci-003 has 175B parameters (Gao,
2021).


**D.2** **Details on Querying the LLMs**


**D.2.1** **Parsing the LLM outputs**


After the T0 and InstructGPT generate the answer,
which is composed of several sentences in almost
all cases, we parse the generated sentence to get
the model’s score. We use some rules to parse the
output, and the rule is established after manually
looking into the output of the generated sequences.
First, we remove the string 1-5 from the output
since we observe that LLM will sometimes say " _..._
_on a scale of 1-5 ..._ ", and we remove the string
out of 5 since LLM sometimes say it " _give a_
_score of_ _x_ _out of 5_ ". We also remove the string
/5 since LLM sometimes uses " _x/_ 5 " to express
that it gives a score of _x_ out of 5. For rating the
adversarial samples, we remove the title 1 and
title 2 in the output sentences. Last, we use
the regular expression to extract the first number
in the sequence. We find that the above rules are
sufficient to parse the output and get the rating of
the LLMs. In a few rare cases, the LLM gives the
rating with a 0 _._ 5 incremental, meaning that it gives
scores like 4 _._ 5 . In such cases, we do not drop the
0 _._ 5 since in our instruction, we do not explicitly tell
it the rating should be 1-incremental. We find the
LLM results merely difference between dropping
the 0 _._ 5 and keeping the 0 _._ 5, and the results in the
paper do not drop the 0 _._ 5 . Again, these cases are
really rare.


Attribute Rater Response



**Grammar**


**Cohesive**


**Likable**


**Relevant**



ChatGPT _I would rate the grammatical correctness of the text of the story fragment_
_as a_ _**4**_ _. There are a few minor errors and awkward phrasings, but overall_
_the text is well-constructed and easy to understand._
T1 **Agree**
T2 **Agree**
T3 **Agree**

ChatGPT _I would rate the fit of the sentences in the story fragment as a_ _**5**_ _. The_
_sentences flow well together, building a clear and cohesive narrative, and_
_providing a sense of progression and development in the main character’s_
_thoughts and emotions._
T1 **Disagree** : _I think it’s a_ _**4**_ _for me, not a 5, because there was one sentence_
_that didn’t seem to fit with the others._
T2 **Agree**
T3 **Disagree** : _I find disjointedness in the passage, which serves to confuse_
_me as a reader. The opening sentence is about the first-person narrator_
_being found dead, but then the next sentences are about the narrator’s_
_inner thoughts. Also, this passage: "The loneliness. It never felt like such_
_a loss." So what is the ’it’ that felt like a loss? The loneliness? If so, how_
_can that feel like a loss? If not, what is this ’it’? So, its juxtaposition to_
_the previous sentence makes it confusing._

ChatGPT _I would rate the story fragment as a_ _**2**_ _in terms of enjoyment. The_
_fragment is written in a somber and introspective tone, and it deals with_
_themes of isolation and loneliness, which may not be enjoyable for some_
_readers. The protagonist’s state of mind is also portrayed as being in a_
_state of shock and confusion, which adds to the overall sense of unease_
_and lack of enjoyment._
T1 **Disagree** : _I would rate it a_ _**3**_ _, not a 2, since the enjoyment piece is subjec-_

_tive._

T2 **Disagree** : _While the rating of the freelancer find that the overall theme_
_while somber has a glimmer of hope to it. It is this that makes it more_
_enjoyable to me. The underdog story. Death is not an issue that should_
_be shied away from in writing. We all face it and I enjoyed the honesty in_
_the piece. :"Why did I keep walking?". Something I feel many of us face_
_and may have asked ourselves. I believe that this would be relevant to_
_many readers._
T3 **Agree**

ChatGPT _I would rate the story fragment as a_ _**5**_ _. It is highly relevant to the prompt_
_as it explores the theme of death and how it can be seen as the death of an_
_entire world. The story fragment also touches on the ideas of loneliness_
_and longing, which are related to the loss of someone or something. The_
_concept of the protagonist being frozen in shock and unable to express_
_themselves, also supports the idea of how death can be seen as the death_
_of an entire world._
T1 **Agree**
T2 **Agree**
T3 **Disagree** : _I do not see the link between the prompt and the story._


Table 7: Story 1: a GPT-2-generated story that is highly-rated by ChatGPT.


Attribute Rater Response



**Grammar**


**Cohesive**


**Likable**


**Relevant**



ChatGPT _I would rate the grammatical correctness of the story fragment as a_ _**3**_ _._
_The text is generally understandable, but there are a few punctuation_
_errors and awkward phrasing that make it somewhat difficult to follow._
T1 **Disagree** : _I would rate the grammar as a_ _**2**_ _, not a 3–there were signifi-_
_cantly more grammatical errors than the average within this set of story_
_fragments._
T2 **Disagree** : _The issues with the grammar and coherence creates a stop_
_start feeling when reading. I am unsure as to who is speaking in the text_
_and this leaves me with many questions._
T3 **Disagree** : _The grammar is_ _**poorer than a 3**_ _, as is evident in for example_
_what is supposed to be the second sentence: "He has been ... (incomplete,_
_and another segment is there "People stare ...); cant; couldnt; doesnt;_

_and sentences that do not make sense like this one: " A man I once was_

_won’t be able to marry me "_

ChatGPT _I would rate the cohesiveness of the sentences in the story fragment as_
_a_ _**2**_ _. The sentences in the fragment are disconnected and jump between_
_different thoughts and ideas, making it difficult to understand the overall_
_story or message. There is no clear progression of events or logical_
_flow between the sentences. It appears that the fragment may be missing_
_some important information or context that would help tie the sentences_
_together._
T1 **Agree**
T2 **Agree**
T3 **Disagree** : _very incoherent passage in which it is not very clear who says_
_what, what the topic is, thus rendering a disjointed narrative._

ChatGPT _I would rate the enjoyability of the story fragment as a_ _**1**_ _. The fragment_
_is confusing and difficult to understand, the sentences are disconnected_
_and jumps between different thoughts and ideas. The story seems to_
_be incomplete and lacks context, making it hard to follow the plot or_
_characters. Additionally, the fragment has a sad and negative tone and it_
_does not seem to be enjoyable to read._
T1 **Agree**
T2 **Agree**
T3 **Agree**

ChatGPT _I would rate the relevance of the story fragment to the prompt as a_ _**1**_ _._
_While the story fragment does mention someone being gone, it doesn’t_
_seem to be directly related to the idea of filling out a form to justify killing_
_someone. It’s unclear if the son is the person being proposed to be killed,_
_or if the story is even taking place in the context of the prompt. There is_
_no clear indication that the fragment is related to the prompt._
T1 **Agree**
T2 **Agree**
T3 **Agree**


Table 8: Story 2: a GPT-2-generated story that is lowly-rated by ChatGPT.


Attribute Rater Response



**Grammar**


**Cohesive**


**Likable**


**Relevant**



ChatGPT _The text of the story fragment appears to be grammatically correct, with_
_no major errors or awkward phrasing. On a scale of 1-5, with 1 being_
_the lowest, I would rate the grammatical correctness of the text as a_ _**5**_ _._
T1 **Agree**
T2 **Agree**
T3 **Disagree** : _I would rate this a_ _**4**_ _because of for example ’no where’ =_
_nowhere, although it might seem like a minor error, it does take away_
_some correctness of the text._

ChatGPT _The sentences in the story fragment fit together well, creating a clear and_
_coherent image of the scene and the characters’ actions and thoughts._
_The language is descriptive and evocative, and the dialogue effectively_
_conveys the characters’ emotions and motivations. On a scale of 1-5,_
_with 1 being the lowest, I would rate how well the sentences fit together_

_as a_ _**4**_ _._

T1 **Agree**
T2 **Agree**
T3 **Partially agree** : _To me, all the sentences fit well together to create a_
_captivating narrative. I will give a_ _**4 or 5**_ _._

ChatGPT _Based on the structure, writing style and plot of the story fragment, I_
_would rate it as_ _**3**_ _in terms of enjoyability. The fragment sets up an_
_intriguing situation with the mysterious little black box and the protago-_
_nist’s belief that it holds the "secret to the universe," but it ends abruptly_
_and leaves many questions unanswered, which may not be satisfying for_

_readers._

T1 **Disagree** : _I liked it and it was very clear, so I’d rate it a_ _**4 or 5**_ _._
T2 **Disagree** : _While the story ends suddenly, I am remined that it is just a_
_fragment and that their may be more to this piece. I felt the piece written_
_about a little black box was intriguing and enjoyed the introduction of_
_Norse mythology. This would attract the interest of many readers._
T3 **Disagree** : _I enjoyed the fragment, as it piqued my interest to want to_
_know more about how the story evolves and what it actually was about._

ChatGPT _I would rate the story fragment as a_ _**5**_ _in relevance to the prompt "The_
_Little Black Box." The story fragment centers around a little black box_
_and its perceived importance, which aligns with the prompt. Additionally,_
_the main character, Heaven, is holding the box and describing it as "the_
_secret to the universe," further solidifying the connection to the prompt._
T1 **Agree**
T2 **Agree**
T3 **Agree** : _the story is totally relevant to the prompt._


Table 9: Story 3: a human-written story that is highly-rated by ChatGPT.


Attribute Rater Response



**Grammar**


**Cohesive**


**Likable**


**Relevant**



ChatGPT _I would rate the grammatical correctness of the text as a_ _**3**_ _. There are_
_a few errors such as "Many displays of various vice" which should_
_be "Many displays of various vices" and "I so small flakes of snow"_
_which should be "I saw small flakes of snow." Additionally, some of_
_the sentences are slightly awkward and could be rephrased for better_
_clarity. Overall, the text is understandable, but there are a few errors_
_and awkward phrasings that could be improved._
T1 **Partially agree** : _I rated the grammar lower than this–maybe a_ _**2 or a 3**_ _._
_There were above-average numbers of errors._
T2 **Disagree** : _While the text is understandable, I found myself being dis-_
_tracted by the errors in the text and coherency._
T3 **Agree**

ChatGPT _I would rate the cohesiveness of the sentences in the story fragment as_
_a_ _**2**_ _. The story fragment jumps abruptly between different scenes and_
_characters without clear transition, making it difficult to follow and_
_understand the narrative. Additionally, the sentences are often disjointed_
_and lack a clear flow. The story fragment also lacks a clear plot or_
_purpose, making it difficult to understand the overall story or the main_
_character’s intentions. The story fragment can be improved by providing_
_more context, and a clearer narrative arc._
T1 **Agree** : _it was jumpy_
T2 **Agree**
T3 **Disagree** : _The story is about a sniper looking through his rifle’s telescope,_
_thus seeing changing scenes based on each room he is seeing through_
_the windows, which are then subsequently described. These descriptions_
_account for the seeming lack of cohesion._

ChatGPT _I would rate the enjoyability of the story fragment as a_ _**1**_ _. The fragment_
_lacks a clear plot, purpose or character development, making it difficult to_
_become invested in the story or the characters. Additionally, the fragment_
_includes disturbing and graphic content such as violence and prostitution,_
_which may make it uncomfortable for some readers. Additionally, the_
_fragment contains several grammatical errors and awkward phrasings_
_that can make it difficult to follow the story. Overall, the story fragment_
_is not enjoyable to read._
T1 **Agree** : _: it was not just graphic, but also showed really harmful and_
_negative representations of characters based on their weight, gender, etc._
T2 **Agree**
T3 **Disagree** : _The story did hold my attention to a degree because I wanted_
_to know whom he wanted to target and why._

ChatGPT _I would rate the relevance of the story fragment to the prompt as a_ _**1**_ _. The_
_story fragment doesn’t contain any elements that relate to the prompt such_
_as the protagonist and antagonist, their actions, or the moral ambiguities_
_the antagonist faces. The story fragment mainly describes a person with_
_a sniper rifle looking into windows of a hotel and doesn’t present any_
_clear plot or purpose. Additionally, the story fragment does not offer any_
_indication of moral ambiguity on the part of the person with the sniper_
_rifle. Overall, the story fragment is not relevant to the prompt._
T1 **Agree**
T2 **Agree**
T3 **Agree**


Table 10: Story 4: a human-written story that is lowly-rated by ChatGPT.


We do not parse the output of ChatGPT using
any rules since we the authors read the response
and extract the score by ourselves.
During the experiments, ChatGPT refuses to answer the questions about 2 stories and 3 news titles
since ChatGPT find those contents to violate the
[OpenAI content policy. We find that those samples](https://beta.openai.com/docs/usage-policies)
contain discrimination to some protected groups,
or contain sexual or violent descriptions. Hence,
the results of ChatGPT are calculated without those

samples.


**D.2.2** **Open-Ended Story Generation**


For T0 and the two InstructGPT models, we query
the four attributes **separately** using the queries
shown as follows:


**Grammaticality**
Please rate the story fragment
The goal of this task is to rate story
fragment.
Note: Please take the time to fully read
and understand the story fragment. We
will reject submissions from workers that
are clearly spamming the task.
Story fragment:

[STORY]

(End of story fragment)
How grammatically correct is the text of
the story fragment? (on a scale of 1-5,
with 1 being the lowest)


**Cohesiveness**

Please rate the story fragment
The goal of this task is to rate story
fragment.
Note: Please take the time to fully read
and understand the story fragment. We
will reject submissions from workers that
are clearly spamming the task.
Story fragment:

[STORY]

(End of story fragment)
How well do the sentences in the story
fragment fit together? (on a scale of
1-5, with 1 being the lowest)


**Likability**
Please rate the story fragment
The goal of this task is to rate story
fragment.
Note: Please take the time to fully read
and understand the story fragment. We



will reject submissions from workers that
are clearly spamming the task.
Story fragment:

[STORY]

(End of story fragment)
How enjoyable do you find the story
fragment? (on a scale of 1-5, with 1
being the lowest)


**Relevance**

Please rate the story fragment
The goal of this task is to rate story
fragment.
Note: Please take the time to fully read
and understand the story fragment. We
will reject submissions from workers that
are clearly spamming the task.
Story fragment:

[STORY]

(End of story fragment)

Now read the PROMPT based on which the

story fragment was written.

PROMPT: [PROMPT]

(End of PROMPT)
How relevant is the story fragment to the
prompt? (on a scale of 1-5, with 1 being
the lowest)
The [STORY] and [PROMPT] are to be filled in
with the story and the prompt. We show the newlines for better readability. When we query the
models, we use the token \n to represent the new
line.

When querying ChatGPT, we query the four attributes of the same story in one conversation; this
is similar to asking the teachers to rate the same
story on the same page of the Google Form. We use
the same queries shown above to query ChatGPT
and the order of queries is the same as the order
shown above.


**D.2.3** **Adversarial Attack Quality Evaluation**

When querying all the LLMs in this task, we query
the _fluency_ and the _meaning preserving_ of the same
news title independently. This means that each
conversation with ChatGPT will only have one
question, asking about the fluency or the meaning
preserving of news title(s). All the parameters for
generation are the same as the default parameters
in Section 3.2.

The exact query we use are:


**Fluency**
You are given a news title. Please read


the news title and answer the question.

News title:

[NEWS_TITLE]

(End of news title)
Question: How natural and fluent is the

text of the news title? (on a scale of
1-5, with 1 being the lowest

The [NEWS_TITLE] will be filled in with either
a benign or adversarial-attacked news title.


**Meaning Preserving** You are given two news

titles. Please read the news titles and

answer the question.

News title 1:

[BENIGN_TITLE]

(End of news title 1)

News title 2:

[ADVERSARIAL_TITLE]

(End of news title 2)
Question: Do you agree that the meaning
(or semantics) of news title 1 is
preserved in news title 2? (on a scale of
1-5, with 1 being the strongly disagree
and 5 being strongly agree.)

The [BENIGN_TITLE] will be filled in with
the news title before the attack and the

[ADVERSARIAL_TITLE] will be filled in with the
news title after an adversarial attack.


**E** **Experiment Details on Adversarial**
**Attacks**


The adversarial samples used in Section 4 are
from Yoo et al. (2022). Yoo et al. (2022) generates different sets of adversarial samples using different adversarial attacks against different victim
models. We use the adversarial samples generated against a bert-base-uncased text classifier
trained on AG-News, using three different adversarial attacks: Textfooler, PWWS, and BAE. The

intent of the dataset is to facilitate the research in

SSA, which we do not violate.

Here, we show the supplementary results of using text-davinci-003 as the LLM evaluation for
evaluating the quality of adversarial samples in
Table 11. We can see that the result of using
text-davinci-003 is similar to ChatGPT in the

sense that text-davinci-003 also rates adversar
ial samples higher than humans while still significantly lower than the benign samples. As already
seen in Section 3.3, text-davinci-003 tends to
give a higher rating.



Human evaluate LLM evaluate

**Fluent** **Mean.** **Fluent** **Mean.**

Benign 4 _._ 55  - 4 _._ 33 4 _._ 56 ~~_[†]_~~

Textfooler 2 _._ 17 1 _._ 88 3 _._ 71 2 _._ 37

PWWS 2 _._ 16 1 _._ 85 3 _._ 62 3 _._ 21

BAE 3 _._ 01 3 _._ 02 4 _._ 16 3 _._ 69


Table 11: LLM evaluation (text-davinci-003) and
human evaluation result on fluency ( **Fluent** ) of the benign and adversarial samples and meaning preserving
( **Mean.** ) between the news title before and after adversarial attacks.


Rater Textfooler PWWS BAE

T1 3 _._ 36 3 _._ 68 4 _._ 2

T2 1 _._ 80 1 _._ 40 2 _._ 96

T3 1 _._ 36 1 _._ 40 1 _._ 88


Table 12: The rating on three adversarial attacks of the
three teachers T1, T2, and T3.


As mentioned in Section 4.3, one teacher rates
the _fluency_ of Textfooler significantly higher than
PWWS while the other two teachers do not. We

show the rating on _fluency_ on the three adversarial
attacks by each teacher in Table 12.


