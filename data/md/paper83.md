**THE RANDOM FEATURE MODEL FOR INPUT-OUTPUT MAPS**

**BETWEEN BANACH SPACES** _[∗]_


NICHOLAS H. NELSEN _[†]_ AND ANDREW M. STUART _[†]_


**Abstract.** Well known to the machine learning community, the random feature model is a
parametric approximation to kernel interpolation or regression methods. It is typically used to
approximate functions mapping a finite-dimensional input space to the real line. In this paper, we
instead propose a methodology for use of the random feature model as a data-driven surrogate for
operators that map an input Banach space to an output Banach space. Although the methodology is
quite general, we consider operators defined by partial differential equations (PDEs); here, the inputs
and outputs are themselves functions, with the input parameters being functions required to specify
the problem, such as initial data or coefficients, and the outputs being solutions of the problem. Upon
discretization, the model inherits several desirable attributes from this infinite-dimensional viewpoint,
including mesh-invariant approximation error with respect to the true PDE solution map and the
capability to be trained at one mesh resolution and then deployed at different mesh resolutions.
We view the random feature model as a non-intrusive data-driven emulator, provide a mathematical
framework for its interpretation, and demonstrate its ability to efficiently and accurately approximate
the nonlinear parameter-to-solution maps of two prototypical PDEs arising in physical science and
engineering applications: viscous Burgers’ equation and a variable coefficient elliptic equation.


**Key words.** random feature, surrogate model, emulator, parametric PDE, solution map, highdimensional approximation, model reduction, supervised learning, data-driven scientific computing


**AMS subject classifications.** 65D15, 65D40, 62M45, 35R60


**1. Introduction.** The _random feature model_, an architecture for the data-driven
approximation of maps between finite-dimensional spaces, was formalized in [70, 71,
72], building on earlier precursors in [6, 64, 89]. The goal of this paper is to extend the
random feature model to a methodology for the data-driven approximation of maps
between infinite-dimensional spaces. Canonical examples of such maps include the
semigroup generated by a time-dependent partial differential equation (PDE) mapping the initial condition (an input parameter) to the solution at a later time and
the operator mapping a coefficient function (an input parameter) appearing in a PDE
to its solution. Obtaining efficient and potentially low-dimensional representations
of PDE solution maps is not only conceptually interesting, but also practically useful. Many applications in science and engineering require repeated evaluations of a
complex and expensive forward model for different configurations of a system parameter. The model often represents a discretized PDE and the parameter, serving
as input to the model, often represents a high-dimensional discretized quantity such
as an initial condition or uncertain coefficient field. These _outer loop_ applications
commonly arise in inverse problems or uncertainty quantification tasks that involve
control, optimization, or inference [69]. Full order forward models do not perform well
in such many-query contexts, either due to excessive computational cost (requiring
the most powerful high performance computing architectures) or slow evaluation time
(unacceptable in real-time contexts such as on-the-fly optimal control). In contrast to
that of the _big data_ regime that dominates computer vision and other technological


_∗_ Submitted to the editors May 20, 2020; accepted for publication (in revised form) May 20, 2021.
**Funding:** NHN is supported by the National Science Foundation (NSF) Graduate Research
Fellowship Program under award DGE-1745301. AMS is supported by NSF (award DMS-1818977)
and by the Office of Naval Research (ONR) (award N00014-17-1-2079). Both authors are supported
by NSF (award AGS-1835860) and ONR (award N00014-19-1-2408).

_†_ Division of Engineering and Applied Science, California Institute of Technology, Pasadena, CA
[91125, USA (nnelsen@caltech.edu, astuart@caltech.edu).](mailto:nnelsen@caltech.edu)


1


2 N. H. NELSEN AND A. M. STUART


fields, only a relatively small amount of high resolution data can be generated from
computer simulations or physical experiments in scientific applications. Fast approximate solvers built from this limited available data that can efficiently and accurately
emulate the full order model would be highly advantageous.
In this work, we demonstrate that the random feature model holds considerable
potential for such a purpose. Resembling [58, 92] and the contemporaneous work
in [13, 51, 56, 65], we present a methodology for true function space learning of blackbox input-output maps between a Banach space and separable Hilbert space. We
formulate the approximation problem as supervised learning in infinite dimensions
and show that the natural hypothesis space is a reproducing kernel Hilbert space
associated with an operator-valued kernel. For a suitable loss functional, training the
random feature model is equivalent to solving a finite-dimensional convex optimization
problem. As a consequence of our careful construction of the method as mapping
between Banach spaces, the resulting emulator naturally scales favorably with respect
to the high input and output dimensions arising in practical, discretized applications;
furthermore, it is shown to achieve small relative test error for two model problems
arising from approximation of a semigroup and of the solution map corresponding to
an elliptic PDE exhibiting parametric dependence on a coefficient function.


**1.1. Literature Review.** In recent years, two different lines of research have
emerged that address PDE approximation problems with machine learning techniques.
The first perspective takes a more traditional approach akin to point collocation methods from the field of numerical analysis. Here, the goal is to use a deep neural network
(NN) to solve a prescribed initial boundary value problem with as high accuracy as
possible. Given a point cloud in a spatio-temporal domain _D_ [˜] as input data, the prevailing approach first directly parametrizes the PDE solution field as a NN and then
optimizes the NN parameters by minimizing the PDE residual with respect to (w.r.t.)
some loss functional (see [73, 79, 87] and the references therein). To clarify, the object
approximated with this novel method is a _low-dimensional_ input-output map _D_ [˜] _→_ R,
i.e., the real-valued function that solves the PDE. This approach is mesh-free by definition but highly intrusive as it requires full knowledge of the specified PDE. Any
change to the original formulation of the initial boundary value problem or related
PDE problem parameters necessitates an (expensive) re-training of the NN solution.
We do not explore this first approach any further in this article.
The second direction is arguably more ambitious: use a NN as an emulator for the
infinite-dimensional mapping between an input parameter and the PDE solution itself
or a functional of the solution, i.e., a quantity of interest; the latter is widely prevalent
in uncertainty quantification problems. We emphasize that the object approximated
in this setting, unlike in the aforementioned first approach, is an input-output map
_X →Y_, i.e., the PDE solution operator, where _X_ _, Y_ are infinite-dimensional Banach
spaces; this map is generally nonlinear. For an approximation-theoretic treatment
of parametric PDEs in general, we refer the reader to the article of Cohen and DeVore [23]. In applications, the solution operator is represented by a discretized forward
model R _[K]_ _→_ R _[K]_, where _K_ is the mesh size, and hence represents a _high-dimensional_
object. It is this second line of research that inspires our work.
Of course, there are many approaches to forward model reduction that do not
explicitly involve machine learning ideas. The reduced basis method (see [5, 9, 29]
and the references therein) is a classical idea based on constructing an empirical basis
from data snapshots and solving a cheaper variational problem; it is still widely used
in practice due to computationally efficient offline-online decompositions that elim

THE RANDOM FEATURE MODEL ON BANACH SPACE 3


inate dependence on the full order degrees of freedom. Recently, machine learning
extensions to the reduced basis methodology, of both intrusive (e.g., projection-based
reduced order models) and non-intrusive (e.g., model-free data only) type, have further improved the applicability of these methods [21, 36, 43, 53, 77]. However, the
input-output maps considered in these works involve high dimension in only one of
the input or output space, not both. Other popular surrogate modeling techniques
include Gaussian processes [90], polynomial chaos expansions [80], and radial basis
functions [88]; yet, these are only practically suitable for problems with input space of
low to moderate dimension. Classical numerical methods for PDEs may also represent
the forward model R _[K]_ _→_ R _[K]_, albeit implicitly in the form a computer code (e.g.:
finite element, finite difference, finite volume methods). However, the approximation
error is sensitive to _K_ and repeated evaluations of this forward model often becomes
cost prohibitive due to poor scaling with input dimension _K_ .
Instead, deep NNs have been identified as strong candidate surrogate models for
parametric PDE problems due to their empirical ability to emulate high-dimensional
nonlinear functions with minimal evaluation cost once trained. Early work in the
use of NNs to learn the solution operator, or vector field, defining ODEs and timedependent PDEs, may be found in the 1990s [20, 39, 74]. There are now more theoretical justifications for NNs breaking the _curse of dimensionality_ [51, 52, 61], leading to
increased interest in PDE applications [1, 37, 66, 78]. A suite of work on data-driven
discretizations of PDEs has surfaced that allow for identification of the governing
model [4, 14, 57, 68, 81, 83]; however, we note that only the operators appearing
in the equation itself are approximated with these approaches, not the solution operator of the PDE. More in line with our focus in this article, architectures based
on deep convolutional NNs have proven quite successful for learning elliptic PDE solution maps (for example, see [84, 91, 93], which take an image-to-image regression
approach). Other NNs have been used in similar elliptic problems for quantity of
interest prediction [49], error estimation [19], or unsupervised learning [54]. Yet in
all the approaches above, the architectures and resulting error are dependent on the
mesh resolution. To circumvent this issue, the surrogate map must be well-defined
on function space and independent of any finite-dimensional realization of the map
that arises from discretization. This is not a new idea (see [20, 75] or for functional
data analysis, [46, 63]). The aforementioned reduced basis method is an example, as
is the method of [22, 23], which approximates the solution map with sparse Taylor
polynomials and is proved to achieve optimal convergence rates in idealized settings.
However, it is only recently that machine learning methods have been explicitly designed to operate in an infinite-dimensional setting, and there is little work in this
direction [13, 56]. Here we propose the random feature model as another such method.
The random feature model (RFM) [70, 71, 72], detailed in Subsection 2.3, is in
some sense the simplest possible machine learning model; it may be viewed as an
ensemble average of randomly parametrized functions: an expansion in a randomized basis. These _random features_ could be defined, for example, by randomizing the
internal parameters of a NN. Compared to NN emulators with enormous learnable
parameter counts (e.g., _O_ (10 [5] ) to _O_ (10 [6] ), see [33, 34, 54]) and methods that are intrusive or lead to nontrivial implementations [22, 53, 77], the RFM is one of the simplest
models to formulate and train (often _O_ (10 [3] ) parameters, or fewer, suffice). The theory
of the RFM for real-valued outputs is well developed, partly due to its close connection to kernel methods [3, 16, 45, 70, 88] and Gaussian processes [64, 89], and includes
generalization rates and dimension-free estimates [61, 71, 82]. A quadrature viewpoint
on the RFM provides further insight and leads to Monte Carlo sampling ideas [3]; we


4 N. H. NELSEN AND A. M. STUART


remark on this further in Subsection 2.3. As in modern deep learning practice, the
RFM has also been shown to perform best when the model is over-parametrized [8].
In a similar high-dimensional setting of relevance in this paper, the authors of [40, 48]
theoretically investigated nonparametric kernel regression for parametric PDEs with
real-valued solution map outputs. The specific random Fourier feature approach of
Rahimi and Recht [70] was generalized in [15] to the finite-dimensional matrix-valued
kernel setting with vector-valued random Fourier features. However, most of these
works require explicit knowledge of the kernel itself. Here our viewpoint is to work
directly with random features as the basis for a standalone method, choosing them
for their properties and noting that they implicitly define a kernel, but not working
directly with this kernel; furthermore, our work considers both infinite-dimensional input _and_ output spaces, not just one or the other. A key idea underlying our approach
is to formulate the proposed random feature algorithm on infinite-dimensional space
and only then discretize. This philosophy in algorithm development has been instructive in a number of areas in scientific computing, such as optimization [44] and the
development of Markov chain Monte Carlo methodology [25]. It has recently been
promoted as a way of designing and analyzing algorithms within machine learning

[41, 60, 76, 85, 86], and our work may be understood within this general framework.


**1.2. Contributions.** Our primary contributions in this paper are now listed.

1. We develop the random feature model, directly formulated on the function
space level, for learning input-output maps between Banach spaces purely
from data. As a method for parametric PDEs, the methodology is nonintrusive but also has the additional advantage that it may be used in settings
where only data is available and no model is known.
2. We show that our proposed method is more computationally tractable to
both train and evaluate than standard kernel methods in infinite dimensions.
Furthermore, we show that the method is equivalent to kernel ridge regression
performed in a finite-dimensional space spanned by random features.
3. We apply our methodology to learn the semigroup defined by the solution
operator for viscous Burgers’ equation and the coefficient-to-solution operator
for the Darcy flow equation.
4. We demonstrate, by means of numerical experiments, two mesh-independent
approximation properties that are built into the proposed methodology: invariance of relative error to mesh resolution and evaluation ability on any
mesh resolution.

This paper is structured as follows. In Section 2, we communicate the mathematical framework required to work with the random feature model in infinite dimensions,
identify an appropriate approximation space, and explain the training procedure.
We introduce two instantiations of random feature maps that target physical science
applications in Section 3 and detail the corresponding numerical results for these
applications in Section 4. We conclude in Section 5 with discussion and future work.


**2. Methodology.** In this work, the overarching problem of interest is the approximation of a map _F_ _[†]_ : _X →Y_, where _X_ _, Y_ are infinite-dimensional spaces of
real-valued functions defined on some bounded open subset of R _[d]_, and _F_ _[†]_ is defined
by _a �→_ _F_ _[†]_ ( _a_ ) := _u_, where _u_ is the solution of a (possibly time-dependent) PDE and _a_
is an input function required to make the problem well-posed. Our proposed approach
for this approximation, constructing a surrogate map _F_ for the true map _F_ _[†]_, is datadriven, non-intrusive, and based on least squares. Least squares-based methods are
integral to the random feature methodology as proposed in low dimensions [70, 71] and


THE RANDOM FEATURE MODEL ON BANACH SPACE 5


generalized here to the infinite-dimensional setting; they have also been shown to work
well in other algorithms for high-dimensional numerical approximation [12, 24, 30].
Within the broader scope of reduced order modeling techniques [9], the approach we
adopt in this paper falls within the class of data-fit emulators. In its essence, our
method interpolates the solution manifold


(2.1) _M_ = _{u ∈Y_ : _u_ = _F_ _[†]_ ( _a_ ) _, a ∈X} ._


The solution map _F_ _[†]_, as the inverse of a differential operator, is often smoothing and
admits a notion of compactness, i.e., the output space compactly embeds into the
input space. Then, the idea is that _M_ should have some compact, low-dimensional
structure (intrinsic dimension). However, actually finding a model _F_ that exploits
this structure despite the high dimensionality of the truth map _F_ _[†]_ is quite difficult.
Further, the effectiveness of many model reduction techniques, such as those based on
the reduced basis method, are dependent on inherent properties of the map _F_ _[†]_ itself
(e.g., analyticity), which in turn may influence the decay rate of the Kolmogorov width
of the manifold _M_ [23]. While such subtleties of approximation theory are crucial to
developing rigorous theory and provably convergent algorithms, we choose to work in
the non-intrusive setting where knowledge of the map _F_ _[†]_ and its associated PDE are
only obtained through measurement data, and hence detailed characterizations such
as those aforementioned are essentially unavailable.
The remainder of this section introduces the mathematical preliminaries for our
methodology. With the goal of operator approximation in mind, in Subsection 2.1 we
formulate a supervised learning problem in an infinite-dimensional setting. We provide
the necessary background on reproducing kernel Hilbert spaces in Subsection 2.2
and then define the RFM in Subsection 2.3. In Subsection 2.4, we describe the
optimization principle which leads to algorithms for the RFM and an example problem
in which _X_ and _Y_ are one-dimensional vector spaces.


**2.1. Problem Formulation.** Let _X_ _, Y_ be real Banach spaces and _F_ _[†]_ : _X →_
_Y_ be a (possibly nonlinear) map. It is natural to frame the approximation of _F_ _[†]_

as a supervised learning problem. Suppose we are given training data in the form
of input-output pairs _{a_ _i_ _, y_ _i_ _}_ _[n]_ _i_ =1 _[⊂X × Y]_ [, where] _[ a]_ _[i]_ _[ ∼]_ _[ν]_ [ i.i.d.,] _[ ν]_ [ is a probability]
measure supported on _X_, and _y_ _i_ = _F_ _[†]_ ( _a_ _i_ ) _∼_ _F_ _♯_ _[†]_ _[ν]_ [ with, potentially, noise added]
to the evaluations of _F_ _[†]_ ( _·_ ). In the examples in this paper, the noise is viewed as
resulting from model error (the PDE does not perfectly represent the physics) or
from discretization error (in approximating the PDE); situations in which the data
acquisition process is inherently noisy can also be envisioned but are not studied here.
We aim to build a parametric reconstruction of the true map _F_ _[†]_ from the data, that
is, construct a model _F_ : _X × P →Y_ and find _α_ _[†]_ _∈P ⊆_ R _[m]_ such that _F_ ( _·, α_ _[†]_ ) _≈_ _F_ _[†]_

are close as maps from _X_ to _Y_ in some suitable sense. The natural number _m_ here
denotes the total number of model parameters. The standard approach to determine
parameters in supervised learning is to first define a loss functional _ℓ_ : _Y × Y →_ R _≥_ 0
and then minimize the expected risk,


(2.2) _α_ min _∈P_ [E] _[a][∼][ν]_ [�] _ℓ_ � _F_ _[†]_ ( _a_ ) _, F_ ( _a, α_ )�� _._


With only the data _{a_ _i_ _, y_ _i_ _}_ _[n]_ _i_ =1 [at our disposal, we approximate problem][ (2.2)][ by re-]

_n_

placing _ν_ with the empirical measure _ν_ [(] _[n]_ [)] := _n_ [1] � _j_ =1 _[δ]_ _[a]_ _j_ [, which leads to the empirical]


6 N. H. NELSEN AND A. M. STUART


risk minimization problem



1
(2.3) min
_α∈P_ _n_



_n_
� _ℓ_ � _y_ _j_ _, F_ ( _a_ _j_ _, α_ )� _._

_j_ =1



The hope is that given minimizer _α_ [(] _[n]_ [)] of (2.3) and _α_ _[†]_ of (2.2), _F_ ( _·, α_ [(] _[n]_ [)] ) well approximates _F_ ( _·, α_ _[†]_ ), that is, the learned model _generalizes_ well; these ideas may be
made rigorous with results from statistical learning theory [42]. Solving problem (2.3)
is called _training_ the model _F_ . Once trained, the model is then validated on a new
set of i.i.d. input-output pairs previously unseen during the training process. This
_testing_ phase indicates how well _F_ approximates _F_ _[†]_ . From here on out, we assume
that ( _Y, ⟨·, ·⟩_ _Y_ _, ∥·∥_ _Y_ ) is a real separable Hilbert space and focus on the squared loss

(2.4) _ℓ_ ( _y, y_ _[′]_ ) := [1] 2 _[∥][y][ −]_ _[y]_ _[′]_ _[∥]_ _Y_ [2] _[.]_


We stress that our entire formulation is in an infinite-dimensional setting and we will
remain in this setting throughout the paper; as such, the random feature methodology
we propose will inherit desirable discretization-invariant properties, to be observed in
the numerical experiments of Section 4.


_Notation_ 2.1. For a Borel measurable map _G_ : _U →V_ between two Banach spaces
_U_, _V_ and a probability measure _π_ supported on _U_, we denote the expectation of _G_
under _π_ by


(2.5) E _[u][∼][π]_ [�] _G_ ( _u_ )� = _G_ ( _u_ ) _π_ ( _du_ )
� _U_


in the sense of Bochner integration (see, e.g., [27], Sec. A.2). We will drop the domain
of integration in situations where no confusion is caused by doing so. ♦


**2.2. Operator-Valued Reproducing Kernels.** The random feature model is
naturally formulated in a reproducing kernel Hilbert space (RKHS) setting, as our
exposition will demonstrate in Subsection 2.3. However, the usual RKHS theory is
concerned with real-valued functions [2, 10, 26, 88]. Our setting, with the output
space _Y_ a separable Hilbert space, requires several ideas that generalize the realvalued case. We now outline these ideas with a review of operator-valued kernels;
parts of the presentation that follow may be found in the references [3, 18, 63].
We first consider the special case _Y_ := R for ease of exposition. A real RKHS is
a Hilbert space ( _H, ⟨·, ·⟩_ _H_ _, ∥·∥_ _H_ ) comprised of real-valued functions _f_ : _X →_ R such
that the pointwise evaluation functional _f �→_ _f_ ( _a_ ) is bounded for every _a ∈X_ . It
then follows that there exists a unique, symmetric, positive definite kernel function
_k_ : _X × X →_ R such that for every _a ∈X_, _k_ ( _·, a_ ) _∈H_ and the _reproducing kernel_
_property f_ ( _a_ ) = _⟨k_ ( _·, a_ ) _, f_ _⟩_ _H_ holds. These two properties are often taken as the
definition of a RKHS. The converse direction is also true: every symmetric, positive
definite kernel defines a unique RKHS [2].
We now introduce the needed generalization of the reproducing property to the
case of arbitrary real Hilbert spaces _Y_, as this result will motivate the construction
of the RFM. Kernels in this setting are now operator-valued.


Definition 2.2. _Let X be a real Banach space and Y a real separable Hilbert_
_space. An_ **operator-valued kernel** _is a map_


(2.6) _k_ : _X × X →L_ ( _Y, Y_ ) _,_


THE RANDOM FEATURE MODEL ON BANACH SPACE 7


_where L_ ( _Y, Y_ ) _denotes the Banach space of all bounded linear operators on Y, such_
_that its adjoint satisfies k_ ( _a, a_ _[′]_ ) _[∗]_ = _k_ ( _a_ _[′]_ _, a_ ) _for all a, a_ _[′]_ _∈X and for every N ∈_ N _,_



(2.7)



_N_
� _⟨y_ _i_ _, k_ ( _a_ _i_ _, a_ _j_ ) _y_ _j_ _⟩_ _Y_ _≥_ 0

_i,j_ =1



_for all pairs {_ ( _a_ _i_ _, y_ _i_ ) _}_ _[N]_ _i_ =1 _[⊂X × Y][.]_


Paralleling the development for the real-valued case, an operator-valued kernel
_k_ also uniquely (up to isomorphism) determines an associated real RKHS _H_ _k_ =
_H_ _k_ ( _X_ ; _Y_ ). Now, choosing a probability measure _ν_ supported on _X_, we define a kernel
integral operator _T_ _k_ associated to _k_ by


_T_ _k_ : _L_ [2] _ν_ [(] _[X]_ [;] _[ Y]_ [)] _[ →]_ _[L]_ [2] _ν_ [(] _[X]_ [;] _[ Y]_ [)]

(2.8)
_F �→_ _T_ _k_ _F_ := _k_ ( _·, a_ _[′]_ ) _F_ ( _a_ _[′]_ ) _ν_ ( _da_ _[′]_ ) _,_
�


which is non-negative, self-adjoint, and compact (provided _k_ ( _a, a_ ) _∈L_ ( _Y, Y_ ) is compact for all _a ∈X_ [18]). Let us further assume that all conditions needed for _T_ _k_ [1] _[/]_ [2] to
be an isometry from _L_ [2] _ν_ [into] _[ H]_ _[k]_ [are satisfied, i.e.,] _[ H]_ _[k]_ [= im(] _[T]_ [ 1] _k_ _[/]_ [2] ). Generalizing the
standard Mercer theory (see, e.g., [3, 10]), we may write the RKHS inner product as


(2.9) _⟨F, G⟩_ _H_ _k_ = _⟨F, T_ _k_ _[−]_ [1] _[G][⟩]_ _[L]_ [2] _ν_ for all _F, G ∈H_ _k_ _._


Note that while (2.9) appears to depend on the measure _ν_ on _X_, the RKHS _H_ _k_ is
itself determined by the kernel without any reference to a measure (see [26], Chp. 3,
Thm. 4). With the inner product now explicit, we may directly deduce a reproducing
property. A fully rigorous justification of the methodology is outside the scope of this
article; however, we perform formal computations which provide intuition underpinning the methodology. To this end we fix _a ∈X_ and _y ∈Y_ . Then



_⟨k_ ( _·, a_ ) _y, T_ _k_ _[−]_ [1] _[F]_ _[⟩]_ _[L]_ [2] _ν_ [=] _k_ ( _a_ _[′]_ _, a_ ) _y,_ ( _T_ _k_ _[−]_ [1] _[F]_ [)(] _[a]_ _[′]_ [)] �
��


= _y, k_ ( _a, a_ _[′]_ )( _T_ _k_ _[−]_ [1] _[F]_ [)(] _[a]_ _[′]_ [)] �
��



_Y_ _[ν]_ [(] _[da]_ _[′]_ [)]


_Y_ _[ν]_ [(] _[da]_ _[′]_ [)]



= _y,_ _k_ ( _a, a_ _[′]_ )( _T_ _k_ _[−]_ [1] _[F]_ [)(] _[a]_ _[′]_ [)] _[ ν]_ [(] _[da]_ _[′]_ [)]
� � � _Y_

= _⟨y, F_ ( _a_ ) _⟩_ _Y_ _,_



by using Definition 2.2 of operator-valued kernel and the fact that _k_ ( _·, a_ ) _y ∈H_ _k_ ([18]).
So, we deduce the following:


Result 2.3 (Reproducing property for operator-valued kernels). _Let F ∈H_ _k_ _be_
_given. Then for every a ∈X and y ∈Y,_


(2.10) _⟨y, F_ ( _a_ ) _⟩_ _Y_ = _⟨k_ ( _·, a_ ) _y, F_ _⟩_ _H_ _k_ _._


This identity, paired with a special choice of _k_, is the basis of the random feature
model in our abstract infinite-dimensional setting.


8 N. H. NELSEN AND A. M. STUART


**2.3. Random Feature Model.** One could approach the approximation of target map _F_ _[†]_ : _X →Y_ from the perspective of kernel methods. However, it is generally
a difficult task to explicitly design operator-valued kernels of the form (2.6) since the
spaces _X_ _, Y_ may be of different regularity, for example. Example constructions of
operator-valued kernels studied in the literature include those taking value as diagonal operators, multiplication operators, or composition operators [46, 63], but these
all involve some simple generalization of scalar-valued kernels. Instead, the random
feature model allows one to implicitly work with operator-valued kernels through the
use of a _random feature map ϕ_ : _X ×_ Θ _→Y_ and a probability measure _µ_ supported
on Banach space Θ. The map _ϕ_ is assumed to be square integrable w.r.t. the product
measure _ν ×_ _µ_, i.e., _ϕ ∈_ _L_ [2] _ν×µ_ [(] _[X ×]_ [Θ;] _[ Y]_ [), where] _[ ν]_ [ is the (sometimes a modeling choice]
at our discretion, sometimes unknown) data distribution on _X_ . Together, ( _ϕ, µ_ ) form
a _random feature pair_ . With this setup in place, we now describe the connection between random features and kernels; to this end, recall the following standard notation:


_Notation_ 2.4. Given a Hilbert space ( _H, ⟨·, ·⟩, ∥·∥_ ), the _outer product a ⊗_ _b ∈_
_L_ ( _H, H_ ) is defined by ( _a ⊗_ _b_ ) _c_ = _⟨b, c⟩a_ for any _a, b, c ∈_ _H_ . ♦


Given the pair ( _ϕ, µ_ ), consider maps _k_ _µ_ : _X × X →L_ ( _Y, Y_ ) of the form


(2.11) _k_ _µ_ ( _a, a_ _[′]_ ) := _ϕ_ ( _a_ ; _θ_ ) _⊗_ _ϕ_ ( _a_ _[′]_ ; _θ_ ) _µ_ ( _dθ_ ) _._
�


Such representations need not be unique; different pairs ( _ϕ, µ_ ) may induce the same
kernel _k_ = _k_ _µ_ in (2.11). Since _k_ _µ_ may readily be shown to be an operator-valued kernel
via Definition 2.2, it defines a unique real RKHS _H_ _k_ _µ_ _⊂_ _L_ [2] _ν_ [(] _[X]_ [;] _[ Y]_ [). Our approximation]
theory will be based on this space or finite-dimensional approximations thereof. We
now perform a purely formal but instructive calculation, following from application of
the reproducing property (2.10) to operator-valued kernels of the form (2.11). Doing
so leads to an integral representation of any _F ∈H_ _k_ _µ_ : for all _a ∈X_ _, y ∈Y_,



_⟨y, F_ ( _a_ ) _⟩_ _Y_ = _⟨k_ _µ_ ( _·, a_ ) _y, F_ _⟩_ _H_ _kµ_ = � [�] _⟨ϕ_ ( _a_ ; _θ_ ) _, y⟩_ _Y_ _ϕ_ ( _·_ ; _θ_ ) _µ_ ( _dθ_ ) _, F_ �



_H_ _kµ_



= _⟨ϕ_ ( _a_ ; _θ_ ) _, y⟩_ _Y_ _⟨ϕ_ ( _·_ ; _θ_ ) _, F_ _⟩_ _H_ _kµ_ _µ_ ( _dθ_ )
�


= _c_ _F_ ( _θ_ ) _⟨y, ϕ_ ( _a_ ; _θ_ ) _⟩_ _Y_ _µ_ ( _dθ_ )
�



= _y,_ _c_ _F_ ( _θ_ ) _ϕ_ ( _a_ ; _θ_ ) _µ_ ( _dθ_ )
� � �


where the coefficient function _c_ _F_ : Θ _→_ R is defined by


(2.12) _c_ _F_ ( _θ_ ) := _⟨ϕ_ ( _·_ ; _θ_ ) _, F_ _⟩_ _H_ _kµ_ _._



_Y_ _[,]_



Since _Y_ is Hilbert, the above holding for all _y ∈Y_ implies the integral representation


(2.13) _F_ = _c_ _F_ ( _θ_ ) _ϕ_ ( _·_ ; _θ_ ) _µ_ ( _dθ_ ) _._
�


The formal expression (2.12) for _c_ _F_ ( _θ_ ) needs careful interpretation (provided in Appendix B). For instance, if _ϕ_ ( _·_ ; _θ_ ) is a realization of a Gaussian process as in Example 2.9, then _ϕ_ ( _·_ ; _θ_ ) _/∈H_ _k_ _µ_ with probability one; indeed, in this case _c_ _F_ is defined


THE RANDOM FEATURE MODEL ON BANACH SPACE 9


only as an _L_ [2] _µ_ [limit. Nonetheless, the RKHS may be completely characterized by this]
integral representation. Define the map


_A_ : _L_ [2] _µ_ [(Θ;][ R][)] _[ →]_ _[L]_ _ν_ [2] [(] _[X]_ [;] _[ Y]_ [)]

(2.14)
_c �→Ac_ := _c_ ( _θ_ ) _ϕ_ ( _·_ ; _θ_ ) _µ_ ( _dθ_ ) _._
�


_A_ may be shown to be a bounded linear operator that is a particular square root of
_T_ _k_ _µ_ (Appendix B). We have the following result whose proof, provided in Appendix A,
is a straightforward generalization of the real-valued case given in [3], Sec. 2.2:

Result 2.5. _Under the assumption that ϕ ∈_ _L_ [2] _ν×µ_ [(] _[X ×]_ [ Θ;] _[ Y]_ [)] _[, the RKHS defined]_
_by the kernel k_ _µ_ _in_ (2.11) _is precisely_


(2.15) _H_ _k_ _µ_ = im( _A_ ) = _c_ ( _θ_ ) _ϕ_ ( _·_ ; _θ_ ) _µ_ ( _dθ_ ) : _c ∈_ _L_ [2] _µ_ [(Θ;][ R][)] _._
�� �


We stress that the integral representation of mappings in RKHS (2.15) is not
unique since _A_ is not injective in general. However, the particular choice _c_ = _c_ _F_ (2.12)
in representation (2.13) does enjoy a sense of uniqueness as described in Appendix B.
A central role in what follows is the approximation of measure _µ_ by the empirical

measure



(2.16) _µ_ [(] _[m]_ [)] := [1]

_m_



_m_

iid

� _δ_ _θ_ _j_ _,_ _θ_ _j_ _∼_ _µ ._

_j_ =1



Given this, define _k_ [(] _[m]_ [)] := _k_ _µ_ ( _m_ ) to be the empirical approximation to _k_ _µ_ :



(2.17) _k_ [(] _[m]_ [)] ( _a, a_ _[′]_ ) = E _[θ][∼][µ]_ [(] _[m]_ [)] [�] _ϕ_ ( _a_ ; _θ_ ) _⊗_ _ϕ_ ( _a_ _[′]_ ; _θ_ )� = [1]

_m_



_m_
� _ϕ_ ( _a_ ; _θ_ _j_ ) _⊗_ _ϕ_ ( _a_ _[′]_ ; _θ_ _j_ ) _._

_j_ =1



Then we let _H_ _k_ ( _m_ ) be the unique RKHS induced by the kernel _k_ [(] _[m]_ [)] ; note that _k_ [(] _[m]_ [)]

and hence _H_ _k_ ( _m_ ) are themselves random variables. The following characterization of
_H_ _k_ ( _m_ ) is proved in Appendix A:

Result 2.6. _Assume that ϕ ∈_ _L_ [2] _ν×µ_ [(] _[X ×]_ [ Θ;] _[ Y]_ [)] _[ and that the random features]_
_{ϕ_ ( _·_ ; _θ_ _j_ ) _}_ _[m]_ _j_ =1 _[are linearly independent in][ L]_ _ν_ [2] [(] _[X]_ [;] _[ Y]_ [)] _[. Then, the RKHS][ H]_ _k_ [(] _[m]_ [)] _[ is equal]_
_to the linear span of the {ϕ_ _j_ := _ϕ_ ( _·_ ; _θ_ _j_ ) _}_ _[m]_ _j_ =1 _[.]_

Applying a simple Monte Carlo sampling approach to elements in RKHS (2.15)
by replacing probability measure _µ_ by empirical measure _µ_ [(] _[m]_ [)] gives, for _c ∈_ _L_ [2] _µ_ [,]



1
(2.18)
_m_



_m_
� _c_ ( _θ_ _j_ ) _ϕ_ ( _·_ ; _θ_ _j_ ) _≈_ _c_ ( _θ_ ) _ϕ_ ( _·_ ; _θ_ ) _µ_ ( _dθ_ ) _._

�
_j_ =1



This approximation achieves the Monte Carlo rate _O_ ( _m_ _[−]_ [1] _[/]_ [2] ) and, by virtue of Result 2.6, is in _H_ _k_ ( _m_ ) . However, in the setting of this work, the Monte Carlo approach
does not give rise to a practical method for learning a target map _F_ _[†]_ _∈H_ _k_ _µ_ because
_F_ _[†]_, _k_ _µ_, and _H_ _k_ _µ_ are all unknown; only the random feature pair ( _ϕ, µ_ ) is assumed to
be given. Hence one cannot apply (2.12) (or (B.2)) to evaluate _c_ = _c_ _F_ _†_ in (2.18).
Furthermore, in realistic settings it may be that _F_ _[†]_ _̸∈H_ _k_ _µ_, which leads to an additional approximation gap not accounted for by the Monte Carlo method. To sidestep
these difficulties, the RFM adopts a data-driven optimization approach to determine
a different approximation to _F_ _[†]_, also from the space _H_ _k_ ( _m_ ) . We now define the RFM:


10 N. H. NELSEN AND A. M. STUART


Definition 2.7. _Given probability spaces_ ( _X_ _, B_ ( _X_ ) _, ν_ ) _and_ (Θ _, B_ (Θ) _, µ_ ) _with X_ _,_
Θ _being real finite- or infinite-dimensional Banach spaces, real separable Hilbert space_
_Y, and ϕ ∈_ _L_ [2] _ν×µ_ [(] _[X ×]_ [ Θ;] _[ Y]_ [)] _[, the]_ **[ random feature model]** _[ is the parametric map]_


_F_ _m_ : _X ×_ R _[m]_ _→Y_



(2.19) ( _a_ ; _α_ ) _�→_ _F_ _m_ ( _a_ ; _α_ ) := _m_ [1] � _α_ _j_ _ϕ_ ( _a_ ; _θ_ _j_ ) _,_ _θ_ _j_ iid _∼_ _µ ._
_j_ =1



( _a_ ; _α_ ) _�→_ _F_ _m_ ( _a_ ; _α_ ) := [1]



_m_



_m_
�



We use the Borel _σ_ -algebras _B_ ( _X_ ) and _B_ (Θ) to define the probability spaces in the
preceding definition. Our goal with the RFM is to choose parameters _α ∈_ R _[m]_ so as to
approximate mappings _F_ _[†]_ _∈H_ _k_ _µ_ (in the ideal setting) by mappings _F_ _m_ ( _·_ ; _α_ ) _∈H_ _k_ ( _m_ ) .
The RFM is itself a random variable and may be viewed as a _spectral method_ since
the randomized basis _ϕ_ ( _·_ ; _θ_ ) in the linear expansion (2.19) is defined on all of _X ν_ -a.e.
Determining the coefficient vector _α_ from data obviates the difficulties associated with
the Monte Carlo approach since the method only requires knowledge of the pair ( _ϕ, µ_ )
and knowledge of sample input-output pairs from target operator _F_ _[†]_ .
As written, Equation (2.19) is incredibly simple. It is clear that the choice of random feature map and measure pair ( _ϕ, µ_ ) will determine the quality of approximation.
Most papers deploying these methods, including [15, 70, 71], take a kernel-oriented
perspective by first choosing a kernel and then finding a random feature map to estimate this kernel. Our perspective, more aligned with [72, 82], is the opposite in that
we allow the choice of random feature map _ϕ_ to implicitly _define_ the kernel via the
formula (2.11) instead of picking the kernel first. This methodology also has implications for numerics: the kernel never explicitly appears in any computations, which
leads to memory savings. It does, however, leave open the question of characterizing
the universality [82] of such kernels and the RKHS _H_ _k_ _µ_ of mappings from _X_ to _Y_ that
underlies the approximation method; this is an important avenue for future work.
The close connection to kernels explains the origins of the RFM in the machine
learning literature. Moreover, the RFM may also be interpreted in the context of
neural networks [64, 82, 89]. To see this explicitly, consider the setting where _X_ _, Y_
are both equal to the Euclidean space R and choose _ϕ_ to be a family of hidden
neurons _ϕ_ NN ( _a_ ; _θ_ ) := _σ_ ( _θ_ [(1)] _· a_ + _θ_ [(2)] ). A single hidden layer NN would seek to find
_{_ ( _α_ _j_ _, θ_ _j_ ) _}_ _[m]_ _j_ =1 [in][ R] _[ ×]_ [ R] [2] [ so that]



1
(2.20)
_m_



_m_
� _α_ _j_ _ϕ_ NN ( _·_ ; _θ_ _j_ )

_j_ =1



matches the given training data _{a_ _i_ _, y_ _i_ _}_ _[n]_ _i_ =1 _[⊂X ×Y]_ [. More generally, and in arbitrary]
Euclidean spaces, one may allow _ϕ_ NN ( _·_ ; _θ_ ) to be any deep NN. However, while the
RFM has the same _form_ as (2.20), there is a difference in the _training_ : the _θ_ _j_ are drawn
i.i.d. from a probability measure and then fixed, and only the _α_ _j_ are chosen to fit the
training data. This connection is quite profound: given any deep NN with randomly
initialized parameters _θ_, studies of the lazy training regime and neural tangent kernel

[16, 45] suggest that adopting a RFM approach and optimizing over only _α_ is quite
natural, as it is observed that in this regime the internal NN parameters do not stray
far from their random initialization during gradient descent whilst the last layer of
parameters _{α_ _j_ _}_ _[m]_ _j_ =1 [adapt considerably.]
Once the feature parameters _{θ_ _j_ _}_ _[m]_ _j_ =1 [are chosen at random and fixed, training]
the RFM _F_ _m_ only requires optimizing over _α ∈_ R _[m]_ which, due to linearity of _F_ _m_ in
_α_, is a straightforward task to which we now turn our attention.


THE RANDOM FEATURE MODEL ON BANACH SPACE 11


**2.4. Optimization.** One of the most attractive characteristics of the RFM is
its training procedure. With the _L_ [2] -type loss (2.4) as in standard regression settings,
optimizing the coefficients of the RFM with respect to the empirical risk (2.3) is a convex optimization problem, requiring only the solution of a finite-dimensional system of
linear equations; the convexity also suggests the possibility of appending convex constraints (such as linear inequalities), although we do not pursue this here. Further,
the kernels _k_ _µ_ or _k_ [(] _[m]_ [)] are not required anywhere in the algorithm. We emphasize
the simplicity of the underlying optimization tasks as they suggest the possibility of
numerical implementation of the RFM into complicated black-box computer codes.
We now proceed to show that a regularized version of the optimization problem (2.3)–(2.4) arises naturally from approximation of a nonparametric regression
problem defined over the RKHS _H_ _k_ _µ_ _._ To this end, recall the supervised learning formulation in Subsection 2.1. Given _n_ i.i.d. input-output pairs _{a_ _i_ _, y_ _i_ = _F_ _[†]_ ( _a_ _i_ ) _}_ _[n]_ _i_ =1 _[⊂]_
_X × Y_ as data, with the _a_ _i_ drawn from (possibly unknown) probability measure _ν_ on
_X_, the objective is to find an approximation _F_ [ˆ] to the map _F_ _[†]_ . Let _H_ _k_ _µ_ be the hypothesis space and _k_ _µ_ its operator-valued reproducing kernel of the form (2.11). The
most straightforward learning algorithm in this RKHS setting is kernel ridge regression, also known as penalized least squares. This method produces a nonparametric
model by finding a minimizer _F_ [ˆ] of



2
�� _y_ _j_ _−_ _F_ ( _a_ _j_ )�� _Y_ [+] _[ λ]_ 2



2
�� _y_ _j_ _−_ _F_ ( _a_ _j_ )�� _Y_ [+] _[ λ]_



2
�� _F_ �� _H_ _kµ_



_,_
�



(2.21) min
_F ∈H_ _kµ_



_n_
�
� _j_ =1



1

2



where _λ ≥_ 0 is a penalty parameter. By the representer theorem for operator-valued
kernels ([63], Theorems 2 and 4), the minimizer has the form



(2.22) _F_ ˆ =



_n_
� _k_ _µ_ ( _·, a_ _j_ ) _β_ _j_

_j_ =1



for some functions _{β_ _j_ _}_ _[n]_ _j_ =1 _[⊂Y]_ [. In practice, finding these] _[ n]_ [ functions in the output]
space requires solving a block linear operator equation. For the high-dimensional PDE
problems we consider in this work, solving such an equation may become prohibitively
expensive from both operation count and memory required. A few workarounds were
proposed in [46] such as certain diagonalizations, but these rely on simplifying assumptions that are somewhat limiting. More fundamentally, the representation of
the solution in (2.22) requires knowledge of the kernel _k_ _µ_ ; in our setting we assume
access only to the random feature pair ( _ϕ, µ_ ) which defines _k_ _µ_ and not _k_ _µ_ itself.
We thus explain how to make progress with this problem given knowledge only of
random features. Recall the empirical kernel given by (2.17), the RKHS _H_ _k_ ( _m_ ), and
Result 2.6. The following result, proved in Appendix A, shows that a RFM hypothesis
class with a penalized least squares empirical loss function in optimization problem
(2.3)–(2.4) is equivalent to kernel ridge regression (2.21) restricted to _H_ _k_ ( _m_ ) .


Result 2.8. _Assume that ϕ ∈_ _L_ [2] _ν×µ_ [(] _[X ×]_ [ Θ;] _[ Y]_ [)] _[ and that the random features]_
_{ϕ_ ( _·_ ; _θ_ _j_ ) _}_ _[m]_ _j_ =1 _[are linearly independent in][ L]_ _ν_ [2] [(] _[X]_ [;] _[ Y]_ [)] _[. Fix][ λ][ ≥]_ [0] _[. Let]_ [ ˆ] _[α][ ∈]_ [R] _[m]_ _[ be the]_
_unique minimum norm solution of the following problem:_



2



+ _[λ]_

2

_Y_



2 _m_ _[∥][α][∥]_ 2 [2]



_m_
�



� _α_ _ℓ_ _ϕ_ ( _a_ _j_ ; _θ_ _ℓ_ )

_ℓ_ =1 ����



1
_y_ _j_ _−_
���� _m_



_._
�



(2.23) min
_α∈_ R _[m]_



_n_
�
� _j_ =1



1

2


12 N. H. NELSEN AND A. M. STUART


_Then, the RFM defined by this choice α_ = ˆ _α satisfies_



2
�� _F_ �� _H_ _k_ ( _m_ )



_._
�



(2.24) _F_ _m_ ( _·_ ; ˆ _α_ ) = argmin
_F ∈H_ _k_ ( _m_ )



_n_
�
� _j_ =1



1

2



2
�� _y_ _j_ _−_ _F_ ( _a_ _j_ )�� _Y_ [+] _[ λ]_ 2



2
�� _y_ _j_ _−_ _F_ ( _a_ _j_ )�� _Y_ [+] _[ λ]_



Solving the convex problem (2.23) trains the RFM. The first order condition for
a global minimizer leads to the normal equations



_n_
� _α_ _i_ � _ϕ_ ( _a_ _j_ ; _θ_ _i_ ) _, ϕ_ ( _a_ _j_ ; _θ_ _ℓ_ )�

_j_ =1



1
(2.25)
_m_



_m_
�


_i_ =1



_Y_ [+] _[ λα]_ _[ℓ]_ [=]



_n_
�� _y_ _j_ _, ϕ_ ( _a_ _j_ ; _θ_ _ℓ_ )�

_j_ =1



_Y_



for each _ℓ_ _∈{_ 1 _, . . ., m}_ . This is an _m_ -by- _m_ linear system of equations for _α ∈_ R _[m]_

that is standard to solve. In the case _λ_ = 0, the minimum norm solution may be
written in terms of a pseudoinverse operator (see [59], Sec. 6.11).


_Example_ 2.9 (Brownian bridge). We now provide a one-dimensional instantiation of the random feature model to illustrate the methodology. Take the input space
as _X_ := (0 _,_ 1), output space _Y_ := R, input space measure _ν_ := _U_ (0 _,_ 1), and random
parameter space Θ := R _[∞]_ . Denote the input by _a_ = _x ∈X_ . Then, consider the
random feature map _ϕ_ : (0 _,_ 1) _×_ R _[∞]_ _→_ R defined by the _Brownian bridge_



(2.26) _ϕ_ ( _x_ ; _θ_ ) := � _θ_ [(] _[j]_ [)] ( _jπ_ ) _[−]_ [1] _[√]_

_j∈_ N



2 sin( _jπx_ ) _,_ _θ_ [(] _[j]_ [) iid] _∼_ _N_ (0 _,_ 1) _,_



where _θ_ := _{θ_ [(] _[j]_ [)] _}_ _j∈_ N and _µ_ := _N_ (0 _,_ 1) _× N_ (0 _,_ 1) _× · · ·_ . For any realization of _θ ∼_ _µ_,
the function _ϕ_ ( _·_ ; _θ_ ) is a Brownian motion constrained to zero at _x_ = 0 and _x_ = 1.
The induced kernel _k_ _µ_ : (0 _,_ 1) _×_ (0 _,_ 1) _→_ R is then simply the covariance function of
this stochastic process:


(2.27) _k_ _µ_ ( _x, x_ _[′]_ ) = E _[θ][∼][µ]_ [�] _ϕ_ ( _x_ ; _θ_ ) _ϕ_ ( _x_ _[′]_ ; _θ_ )� = min _{x, x_ _[′]_ _} −_ _xx_ _[′]_ _._


Note that _k_ _µ_ is the Green’s function for the negative Laplacian on (0 _,_ 1) with Dirichlet
boundary conditions. Using this fact, we may explicitly characterize the associated
RKHS _H_ _k_ _µ_ as follows. First, we have



1
(2.28) _T_ _k_ _µ_ _f_ =
� 0



1

_k_ _µ_ ( _·, y_ ) _f_ ( _y_ ) _dy_ = _−_ _[d]_ [2]
0 � _dx_ [2]



_dx_ [2]



_−_ 1
_f,_
�



where the the negative Laplacian has domain _H_ [2] ((0 _,_ 1); R) _∩_ _H_ 0 [1] [((0] _[,]_ [ 1);][ R][).] Viewing _T_ _k_ _µ_ as an operator from _L_ [2] ((0 _,_ 1); R) into itself, from (2.9) we conclude, upon
integration by parts, that



_df_
(2.29) _⟨f, g⟩_ _H_ _kµ_ = _⟨f, T_ _k_ _[−]_ _µ_ [1] _[g][⟩]_ _[L]_ [2] [ =] �



_dx_



_df_ _[d][g]_

_dx_ _[,]_ _dx_



_L_ [2] [ =] _[ ⟨][f, g][⟩]_ _[H]_ 0 [1] for all _f, g ∈H_ _k_ _µ_ _._



�



Note that the last identity does indeed define an inner product on _H_ 0 [1] _[.]_ [ By this formal]
argument we identify the RKHS _H_ _k_ _µ_ as the Sobolev space _H_ 0 [1] [((0] _[,]_ [ 1);][ R][). Furthermore,]
Brownian bridge may be viewed as the Gaussian measure _N_ (0 _, T_ _k_ _µ_ ). Approximation
using the RFM with the Brownian bridge random features is illustrated in Figure 1.
Since _k_ _µ_ ( _·, x_ ) is a piecewise linear function, a kernel interpolation or regression method
will produce a piecewise linear approximation. Indeed, the figure indicates that the
RFM with _n_ training points fixed approaches the optimal piecewise linear kernel
interpolant as _m →∞_ (see [61] for a related theoretical result). ♦


THE RANDOM FEATURE MODEL ON BANACH SPACE 13



2 _._ 5


2 _._ 0


1 _._ 5


1 _._ 0


0 _._ 5


0 _._ 0


_−_ 0 _._ 5

0 _._ 0 0 _._ 2 0 _._ 4 0 _._ 6 0 _._ 8 1 _._ 0

_x_


(a) _m_ = 50


2 _._ 5


2 _._ 0


1 _._ 5


1 _._ 0


0 _._ 5


0 _._ 0


_−_ 0 _._ 5

0 _._ 0 0 _._ 2 0 _._ 4 0 _._ 6 0 _._ 8 1 _._ 0

_x_


(c) _m_ = 5000



1 _._ 5


1 _._ 0


0 _._ 5


0 _._ 0


_−_ 0 _._ 5

0 _._ 0 0 _._ 2 0 _._ 4 0 _._ 6 0 _._ 8 1 _._ 0

_x_


(b) _m_ = 500


2 _._ 5


2 _._ 0


1 _._ 5


1 _._ 0


0 _._ 5


0 _._ 0


_−_ 0 _._ 5

0 _._ 0 0 _._ 2 0 _._ 4 0 _._ 6 0 _._ 8 1 _._ 0

_x_


(d) _m_ = _∞_



2 _._ 5


2 _._ 0



Fig. 1: Brownian bridge random feature model for one-dimensional input-output spaces with _n_ = 32 training points fixed and _λ_ = 0 (Example 2.9): as _m →∞_, the RFM
approaches the nonparametric interpolant given by the representer theorem (Figure 1d), which in this case is a piecewise linear approximation of the true function
(an element of RKHS _H_ _k_ _µ_ = _H_ 0 [1] [, shown in red). Blue lines denote the trained model]
evaluated on test data points and black circles denote evaluation at training points.


The Brownian bridge Example 2.9 illuminates a more fundamental idea. For this
low-dimensional problem, an expansion in a deterministic Fourier sine basis would of
course be more natural. But if we do not have a natural, computable orthonormal
basis, then randomness provides a useful alternative representation; notice that the
random features each include random combinations of the deterministic Fourier sine

basis in this example. For the more complex problems that we study numerically in
the next two sections, we lack knowledge of good, computable bases for general maps
in infinite dimensions. The RFM approach exploits randomness to explore, implicitly
discover the structure of, and represent, such maps. Thus we now turn away from this
example of real-valued maps defined on a subset of the real line and instead consider
the use of random features to represent maps between spaces of functions.


**3. Application to PDE Solution Maps.** In this section, we design the random feature maps _ϕ_ : _X ×_ Θ _→Y_ and measures _µ_ for the RFM approximation of


14 N. H. NELSEN AND A. M. STUART


two particular PDE parameter-to-solution maps: the evolution semigroup of viscous
Burgers’ equation in Subsection 3.1 and the coefficient-to-solution operator for the
Darcy problem in Subsection 3.2. It is well known to kernel method practitioners
that the choice of kernel (which in this work follows from the choice of ( _ϕ, µ_ )) plays a
central role in the quality of the function reconstruction. While our method is purely
data-driven and requires no knowledge of the governing PDE, we take the view that
any prior knowledge can, and should, be introduced into the design of ( _ϕ, µ_ ). However, the question of how to automatically determine good random feature pairs for
a particular problem or dataset, inducing data-adapted kernels, is open. The maps
_ϕ_ that we choose to employ are nonlinear in both arguments. We also detail the
probability measure _ν_ on the input space _X_ for each of the two PDE applications;
this choice is crucial because while we desire the trained RFM to transfer to arbitrary
out-of-distribution inputs from _X_, we can in general only expect the learned map to
perform well when restricted to inputs statistically similar to those sampled from _ν_ .


**3.1. Burgers’ Equation: Formulation.** Viscous Burgers’ equation in one spatial dimension is representative of the advection-dominated PDE problem class in
some regimes; these time-dependent equations are not conservation laws due to the
presence of small dissipative terms, but nonlinear transport still plays a central role
in the evolution of solutions. The initial value problem we consider is



_−_ _ε_ _[∂]_ [2] _[u]_ [=] _[ f]_ in (0 _, ∞_ ) _×_ (0 _,_ 1) _,_
� _∂x_ [2]



_∂u_ _[∂]_

_∂t_ [+]



_∂u_



_u_ (0 _, ·_ ) = _a_ in (0 _,_ 1) _,_



_∂x_



2
_u_
� 2



(3.1)







_∂u_
_u_ ( _·,_ 0) = _u_ ( _·,_ 1) _,_
_∂x_ [(] _[·][,]_ [ 0) =] _[ ∂u]_ _∂x_



in (0 _, ∞_ ) _,_
_∂x_ [(] _[·][,]_ [ 1)]



where _ε >_ 0 is the viscosity (i.e., diffusion coefficient) and we have imposed periodic boundary conditions. The initial condition _a_ serves as the input and is drawn
according to a Gaussian measure defined by


(3.2) _a ∼_ _ν_ := _N_ (0 _, C_ )


with Mat´ern-like covariance operator [31, 62]


(3.3) _C_ := _τ_ [2] _[α][−][d]_ ( _−_ ∆+ _τ_ [2] Id) _[−][α]_ _,_


where _d_ = 1 and the negative Laplacian _−_ ∆is defined over T [1] = [0 _,_ 1] per and restricted
to functions which integrate to zero over T [1] . The hyperparameter _τ ≥_ 0 is an inverse
length scale and _α >_ 1 _/_ 2 controls the regularity of the draw. Such _a_ are almost surely
H¨older and Sobolev regular with exponent up to _α −_ 1 _/_ 2 ([27], Thm. 12, pg. 338), so
in particular _a ∈X_ := _L_ [2] (T [1] ; R). Then for all _ε >_ 0, the unique global solution _u_ ( _t, ·_ )
to (3.1) is real analytic for all _t >_ 0 (see [50], Thm. 1.1). Hence, setting the output
space to be _Y_ := _H_ _[s]_ (T [1] ; R) for any _s >_ 0, we may define the solution map


_F_ _[†]_ : _L_ [2] _→_ _H_ _[s]_

(3.4)
_a �→_ _F_ _[†]_ ( _a_ ) := Ψ _T_ ( _a_ ) = _u_ ( _T, ·_ ) _,_


where _{_ Ψ _t_ _}_ _t>_ 0 forms the solution operator semigroup for (3.1) and we fix the final
time _t_ = _T >_ 0. The map _F_ _[†]_ is smoothing and nonlinear.


THE RANDOM FEATURE MODEL ON BANACH SPACE 15


We now describe a random feature map for use in the RFM (2.19) that we call
_Fourier space random features_ . Let _F_ denote the Fourier transform over spatial domain T [1] and define _ϕ_ : _X ×_ Θ _→Y_ by


(3.5) _ϕ_ ( _a_ ; _θ_ ) := _σ_ � _F_ _[−]_ [1] ( _χFaFθ_ )� _,_


where _σ_ ( _·_ ), the ELU function defined below, is defined as a mapping on R and applied
pointwise to functions. Viewing Θ _⊆_ _L_ [2] (T [1] ; R), the randomness enters through _θ ∼_
_µ_ := _N_ (0 _, C_ _[′]_ ) with _C_ _[′]_ the same covariance operator as in (3.3) but with potentially
different inverse length scale and regularity, and the _wavenumber filter function χ_ :
Z _→_ R _≥_ 0 is


(3.6) _χ_ ( _k_ ) := _σ_ _χ_ (2 _π|k|δ_ ) _,_ _σ_ _χ_ ( _r_ ) := max�0 _,_ min _{_ 2 _r,_ ( _r_ + 1 _/_ 2) _[−][β]_ _}_ � _,_


where _δ, β >_ 0. The map _ϕ_ ( _·_ ; _θ_ ) essentially performs a filtered random convolution with the initial condition. Figure 2a illustrates a sample input and output from
_ϕ_ . Although simply hand-tuned for performance and not optimized, the filter _χ_ is
designed to shuffle energy in low to medium wavenumbers and cut off high wavenumbers (see Figure 2b), reflecting our prior knowledge of solutions to (3.1).
We choose the activation function _σ_ in (3.5) to be the exponential linear unit



(3.7) ELU( _r_ ) :=



_r,_ _r ≥_ 0


_e_ _[r]_ _−_ 1 _,_ _r <_ 0 _._

�



ELU has successfully been used as activation in other machine learning frameworks for
related nonlinear PDE problems [53, 67, 68]. We also find ELU to perform better in

_·_ _·_ _·_
the RFM framework over several other choices including ReLU( ), tanh( ), sigmoid( ),

_·_ _·_ _·_
sin( ), SELU( ), and softplus( ). Note that the pointwise evaluation of ELU in (3.5)
will be well defined, by Sobolev embedding, for _s >_ 1 _/_ 2 sufficiently large in the
definition of _Y_ = _H_ _[s]_ . Since the solution operator maps into _H_ _[s]_ for any _s >_ 0, this
does not constrain the method.


**3.2. Darcy Flow: Formulation.** Divergence form elliptic equations [38] arise
in a variety of applications, in particular, the groundwater flow in a porous medium
governed by Darcy’s law [7]. This linear elliptic boundary value problem reads



(3.8)



_−∇·_ ( _a∇u_ ) = _f_ in _D,_


_u_ = 0 on _∂D,_

�



where _D_ is a bounded open subset in R _[d]_, _f_ represents sources and sinks of fluid, _a_ the
permeability of the porous medium, and _u_ the piezometric head; all three functions
map _D_ into R and, in addition, _a_ is strictly positive almost everywhere in _D_ . We work
in a setting where _f_ is fixed and consider the input-output map defined by _a �→_ _u_ .
The measure _ν_ on _a_ is a high contrast level set prior constructed as the pushforward
of a Gaussian measure:


(3.9) _a ∼_ _ν_ := _ψ_ _♯_ _N_ (0 _, C_ ) _._


Here _ψ_ : R _→_ R is a threshold function defined by


(3.10) _ψ_ ( _r_ ) := _a_ [+] 1 (0 _,∞_ ) ( _r_ ) + _a_ _[−]_ 1 ( _−∞,_ 0) ( _r_ ) _,_ 0 _< a_ _[−]_ _≤_ _a_ [+] _< ∞_ _,_


16 N. H. NELSEN AND A. M. STUART


1 _._ 0


0 _._ 5


0 _._ 0


_−_ 0 _._ 5


0 _._ 0 0 _._ 2 0 _._ 4 0 _._ 6 0 _._ 8 1 _._ 0

_x_


(a) (b)


Fig. 2: Random feature map construction for Burgers’ equation: Figure 2a displays
a representative input-output pair for the random feature _ϕ_ ( _·_ ; _θ_ ), _θ ∼_ _µ_ (3.5), while
Figure 2b shows the filter _k �→_ _χ_ ( _k_ ) for _δ_ = 0 _._ 0025 and _β_ = 4 (3.6).


applied pointwise to functions, and the covariance operator _C_ is given in (3.3) with
_d_ = 2 and homogeneous Neumann boundary conditions on _−_ ∆. That is, the resulting
coefficient _a_ almost surely takes only two values ( _a_ [+] or _a_ _[−]_ ) and, as the zero level set
of a Gaussian random field, exhibits random geometry in the physical domain _D_ . It
follows that _a ∈_ _L_ _[∞]_ ( _D_ ; R _≥_ 0 ) almost surely. Further, the size of the contrast ratio
_a_ [+] _/a_ _[−]_ measures the scale separation of this elliptic problem and hence controls the
difficulty of reconstruction [11]. See Figure 3a for a representative sample.
Given _f ∈_ _L_ [2] ( _D_ ; R), the standard Lax-Milgram theory may be applied to show
that for coefficient _a ∈X_ := _L_ _[∞]_ ( _D_ ; R _≥_ 0 ), there exists a unique weak solution _u ∈_
_Y_ := _H_ 0 [1] [(] _[D]_ [;][ R][) for][ Equation (3.8)][ (see, e.g., Evans [][32][]). Thus, we define the ground]
truth solution map

(3.11) _F_ _[†]_ : _L_ _[∞]_ _→_ _H_ 0 [1]
_a �→_ _F_ _[†]_ ( _a_ ) := _u ._


Although the PDE (3.8) is linear, the solution map _F_ _[†]_ is nonlinear.
We now describe the chosen random feature map for this problem, which we call
_predictor-corrector random features_ . Define _ϕ_ : _X ×_ Θ _→Y_ by _ϕ_ ( _a_ ; _θ_ ) := _p_ 1 such that


(3.12a) _−_ ∆ _p_ 0 = _[f]_ _a_ [+] _[ σ]_ _[γ]_ [(] _[θ]_ [1] [)] _[,]_

(3.12b) _−_ ∆ _p_ 1 = _[f]_ _a_ [+] _[ σ]_ _[γ]_ [(] _[θ]_ [2] [) +] _[ ∇]_ [(log] _[ a]_ [)] _[ · ∇][p]_ [0] _[,]_


where the boundary conditions are homogeneous Dirichlet, _θ_ = ( _θ_ 1 _, θ_ 2 ) _∼_ _µ_ := _µ_ _[′]_ _×_ _µ_ _[′]_

are two Gaussian random fields each drawn from _µ_ _[′]_ := _N_ (0 _, C_ _[′]_ ), _f_ is the source term
in (3.8), and _γ_ = ( _s_ [+] _, s_ _[−]_ _, δ_ ) are parameters for a thresholded sigmoid _σ_ _γ_ : R _→_ R,

(3.13) _σ_ _γ_ ( _r_ ) := 1 + _[s]_ [+] _[ −]_ _e_ _[−]_ _[s]_ _[r/δ][−]_ [+] _[ s]_ _[−]_ _[,]_


THE RANDOM FEATURE MODEL ON BANACH SPACE 17


and extended as a Nemytskii operator when applied to _θ_ 1 ( _·_ ) or _θ_ 2 ( _·_ ). We view Θ _⊆_
_L_ [2] ( _D_ ; R) _× L_ [2] ( _D_ ; R). In practice, since _∇a_ is not well-defined when drawn from the
level set measure, we replace _a_ with _a_ _ε_, where _a_ _ε_ := _v_ (1) is a smoothed version of _a_
obtained by evolving the following linear heat equation for one time unit:



_v_ (0) = _a_ in _D,_



_dv_

_dt_ [=] _[ η]_ [∆] _[v]_ [ in (0] _[,]_ [ 1)] _[ ×][ D,]_



(3.14)







_n · ∇v_ = 0 on (0 _,_ 1) _× ∂D,_



where _n_ is the outward unit normal vector to _∂D_ . An example of the response _ϕ_ ( _a_ ; _θ_ )
to a piecewise constant input _a ∼_ _ν_ is shown in Figure 3 for some _θ ∼_ _µ_ .


(a) _a ∼_ _ν_ (b) _ϕ_ ( _a_ ; _θ_ ) _, θ ∼_ _µ_


Fig. 3: Random feature map construction for Darcy flow: Figure 3a displays a representative input draw _a_ with _τ_ = 3 _, α_ = 2 and _a_ [+] = 12 _, a_ _[−]_ = 3; Figure 3b shows
the output random feature _ϕ_ ( _a_ ; _θ_ ) (Equation (3.12)) taking the coefficient _a_ as input.
Here, _f ≡_ 1, _τ_ _[′]_ = 7 _._ 5 _, α_ _[′]_ = 2, _s_ [+] = 1 _/a_ [+], _s_ _[−]_ = _−_ 1 _/a_ _[−]_, and _δ_ = 0 _._ 15.


We remark that by removing the two random terms involving _θ_ 1 _, θ_ 2 in (3.12),
we obtain a remarkably accurate surrogate model for the PDE. This observation is
representative of a more general iterative method, a predictor-corrector type iteration,
for solving the Darcy equation (3.8), whose convergence depends on the size of _a_ . The
map _ϕ_ is essentially a random perturbation of a single step of this iterative method:
Equation (3.12a) makes a coarse prediction of the output, then (3.12b) improves
this prediction with a correction term derived from expanding the original PDE.
This choice of _ϕ_ falls within an ensemble viewpoint that the RFM may be used to
improve pre-existing surrogate models by taking _ϕ_ ( _·_ ; _θ_ ) to be an existing emulator,
but randomized in a principled way through _θ ∼_ _µ_ .
For this particular example, we are cognizant of the facts that the random feature
map _ϕ_ requires full knowledge of the Darcy equation and a na¨ıve evaluation of _ϕ_ may
be as expensive as solving the original PDE, which is itself a linear PDE; however,
we believe that the ideas underlying the random features used here are intuitive and
suggestive of what is possible in other applications areas. For example, RFMs may be
applied on larger domains with simple geometries, viewed as supersets of the physical


18 N. H. NELSEN AND A. M. STUART


domain of interest, enabling the use of efficient algorithms such as the fast Fourier
transform (FFT) even though these may not be available on the original problem,
either because the operator to be inverted is spatially inhomogeneous or because of
the complicated geometry of the physical domain.


**4. Numerical Experiments.** We now assess the performance of our proposed
methodology on the approximation of operators _F_ _[†]_ : _X →Y_ presented in Section 3.
Practical implementation of the approach on a computer necessitates discretization
of the input-output function spaces _X_, _Y_ . Hence in the numerical experiments that
follow, all infinite-dimensional objects such as the training data, evaluations of random feature maps, and random fields are discretized on an equispaced mesh with _K_
grid points to take advantage of the _O_ ( _K_ log _K_ ) computational speed of the FFT.
The simple choice of equispaced points does not limit the proposed approach, as our
formulation of the RFM on function space allows the method to be implemented numerically with any choice of spatial discretization. Such a numerical discretization
procedure leads to the problem of high- but finite-dimensional approximation of discretized target operators mapping R _[K]_ to R _[K]_ by similarly discretized RFMs. However,
we emphasize the fact that _K_ is allowed to vary, and we study the properties of the
discretized RFM as _K_ varies, noting that since the RFM is defined conceptually on
function space in Section 2 without reference to discretization, its discretized numerical realization has approximation quality consistent with the infinite-dimensional limit
_K →∞_ . This implies that the same trained model can be deployed across the entire
hierarchy of finite-dimensional spaces R _[K]_ parametrized by _K ∈_ N without the need
to be re-trained, provided _K_ is sufficiently large. Thus in this section, our notation
does not make explicit the dependence of the discretized RFM or target operators on
mesh size _K_ . We demonstrate these claimed properties numerically.
The input functions and our chosen random feature maps (3.5) and (3.12) require
i.i.d. draws of Gaussian random fields to be fully defined. We efficiently sample these
fields by truncating a Karhunen-Lo´eve expansion and employing fast summation of
the eigenfunctions with FFT. More precisely, on a mesh of size _K_, denote by _g_ ( _·_ ) a
numerical approximation of a Gaussian random field on domain _D_ = (0 _,_ 1) _[d]_, _d_ = 1 _,_ 2:



(4.1) _g_ = � _ξ_ _k_ �

_k∈Z_ _K_



_λ_ _k_ _φ_ _k_ _≈_ �

_k_ _[′]_ _∈_ Z _[d]_ _≥_ 0



_λ_ _k_ _′_ _φ_ _k_ _′_ _∼_ _N_ (0 _, C_ ) _,_



_ξ_ _k_ _′_ �



where _{ξ_ _j_ _} ∼_ _N_ (0 _,_ 1) i.i.d. and _Z_ _K_ _⊂_ Z _≥_ 0 is a truncated one-dimensional lattice
of cardinality _K_ ordered such that _{λ_ _j_ _}_ is non-increasing. The pairs ( _λ_ _k_ _′_ _, φ_ _k_ _′_ ) are
found by solving the eigenvalue problem _Cφ_ _k_ _[′]_ = _λ_ _k_ _[′]_ _φ_ _k_ _[′]_ for non-negative, symmetric,
trace-class operator _C_ (3.3). Concretely, these solutions are given by
(4.2)



_φ_ _k_ _′_ ( _x_ ) =



_√_ 2 cos( _k_ 1 _[′]_ _[πx]_ [1] [) cos(] _[k]_ 2 _[′]_ _[πx]_ [2] [)] _[,]_ _k_ 1 _[′]_ [or] _[ k]_ 2 _[′]_ [= 0]

_,_ _λ_ _k_ _[′]_ = _τ_ [2] _[α][−]_ [2] ( _π_ [2] _|k_ _[′]_ _|_ [2] + _τ_ [2] ) _[−][α]_

�2 cos( _k_ 1 _[′]_ _[πx]_ [1] [) cos(] _[k]_ 2 _[′]_ _[πx]_ [2] [)] _[,]_ otherwise



for homogeneous Neumann boundary conditions when _d_ = 2, _k_ _[′]_ = ( _k_ 1 _[′]_ _[, k]_ 2 _[′]_ [)] _[ ∈]_ [Z] [2] _≥_ 0 _[\{]_ [0] _[}]_ [,]
_x_ = ( _x_ 1 _, x_ 2 ) _∈_ (0 _,_ 1) [2], and given by



_φ_ 2 _j_ ( _x_ ) = _√_



2 cos(2 _πjx_ ) _,_ _φ_ 2 _j−_ 1 ( _x_ ) = _√_



(4.3a) _φ_ 2 _j_ ( _x_ ) = _√_ 2 cos(2 _πjx_ ) _,_ _φ_ 2 _j−_ 1 ( _x_ ) = _√_ 2 sin(2 _πjx_ ) _,_ _φ_ 0 ( _x_ ) = 1 _,_



(4.3b) _λ_ 2 _j_ = _λ_ 2 _j−_ 1 = _τ_ [2] _[α][−]_ [1] (4 _π_ [2] _j_ [2] + _τ_ [2] ) _[−][α]_ _,_ _λ_ 0 = _τ_ _[−]_ [1]


for periodic boundary conditions when _d_ = 1, _j ∈_ Z _>_ 0, and _x ∈_ (0 _,_ 1). In both cases,
we enforce that _g_ integrate to zero over _D_ by manually setting to zero the Fourier


THE RANDOM FEATURE MODEL ON BANACH SPACE 19


coefficient corresponding to multi-index _k_ _[′]_ = 0. We use such _g_ in all experiments that
follow. Additionally, the _k_ and _k_ _[′]_ used in this section to denote wavenumber indices
should not be confused with our previous notation for kernels.
With the discretization and data generation setup now well-defined, and the pairs
( _ϕ, µ_ ) given in Section 3, the last algorithmic step is to train the RFM by solving (2.25)
and then test its performance. For a fixed number of random features _m_, we only
train and test a single realization of the RFM, viewed as a random variable itself.
In each instance _m_ is varied in the experiments that follow, the draws _{θ_ _j_ _}_ _[m]_ _j_ =1 [are]
re-sampled i.i.d. from _µ_ . To measure the distance between the trained RFM _F_ _m_ ( _·_ ; ˆ _α_ )
and the ground truth map _F_ _[†]_, we employ the _approximate expected relative test error_



(4.4) _e_ _n_ _[′]_ _,m_ := [1]

_n_ _[′]_



_n_ _[′]_
�

_j_ =1



_∥F_ _[†]_ ( _a_ _[′]_ _j_ [)] _[ −]_ _[F]_ _[m]_ [(] _[a]_ _[′]_ _j_ [; ˆ] _[α]_ [)] _[∥]_ _[L]_ [2] _∥F_ _†_ ( _a_ _′_ ) _−_ _F_ _m_ ( _a_ _′_ ; ˆ _α_ ) _∥_ _L_ 2
_≈_ E _[a]_ _[′]_ _[∼][ν]_
_∥F_ _[†]_ ( _a_ _[′]_ _j_ [)] _[∥]_ _[L]_ [2] � _∥F_ _[†]_ ( _a_ _[′]_ ) _∥_ _L_ 2



_,_
�



where the _{a_ _[′]_ _j_ _[}]_ _[n]_ _j_ =1 _[′]_ [are drawn i.i.d. from] _[ ν]_ [ and] _[ n]_ _[′]_ [ denotes the number of input-output]
pairs used for testing. All _L_ [2] ( _D_ ; R) norms on the physical domain are numerically
approximated by composite trapezoid rule quadrature. Since _Y ⊂_ _L_ [2] for both the
PDE solution operators (3.4) and (3.11), we also perform all required inner products
during training in _L_ [2] rather than in _Y_ ; this results in smaller relative test error _e_ _n_ _′_ _,m_ .


**4.1. Burgers’ Equation: Experiment.** We generate a high resolution dataset
of input-output pairs by solving Burgers’ equation (3.1) on an equispaced periodic
mesh of size _K_ = 1025 (identifying the first mesh point with the last) with random
initial conditions sampled from _ν_ = _N_ (0 _, C_ ) using (4.1), where _C_ is given by (3.3)
with parameter choices _τ_ = 7 and _α_ = 2 _._ 5. The full-order solver is a FFT-based pseudospectral method for spatial discretization [35] and a fourth-order Runge-Kutta integrating factor time-stepping scheme for time discretization [47]. All data represented
on mesh sizes _K <_ 1025 used in both training and testing phases are subsampled from
this original dataset, and hence we consider numerical realizations of _F_ _[†]_ (3.4) up to
R [1025] _→_ R [1025] . We fix _n_ = 512 training and _n_ _[′]_ = 4000 testing pairs unless otherwise
noted, and also fix the viscosity to _ε_ = 10 _[−]_ [2] in all experiments. Lowering _ε_ leads to
smaller length scale solutions and more difficult reconstruction; more data (higher _n_ )
and features (higher _m_ ) or a more expressive choice of ( _ϕ, µ_ ) would be required to
achieve comparable error levels due to the slow decaying Kolmogorov width of the
solution map. For simplicity, we set the forcing _f ≡_ 0, although nonzero forcing could
lead to other interesting solution maps such as _f �→_ _u_ ( _T, ·_ ). It is easy to check that
the solution will have zero mean for all time and a steady state of zero. Hence, we
choose _T ≤_ 2 to ensure that the solution is far enough away from steady state. For
the random feature map (3.5), we fix the hyperparameters _α_ _[′]_ = 2, _τ_ _[′]_ = 5, _δ_ = 0 _._ 0025,
and _β_ = 4. The map itself is evaluated efficiently with the FFT and requires no other
tools to be discretized. RFM hyperparameters were hand-tuned but not optimized.
We find that regularization during training had a negligible effect for this problem, so
the RFM is trained with _λ_ = 0 by solving the normal equations (2.25) with the pseudoinverse to deliver the minimum norm least squares solution; we use the truncated
SVD implementation in Python’s `scipy.linalg.pinv2` for this purpose.
Our experiments study the RFM approximation to the viscous Burgers’ equation
evolution operator semigroup (3.4). As a visual aid for the high-dimensional problem
at hand, Figure 4 shows a representative sample input and output along with a trained
RFM test prediction. To determine whether the RFM has actually learned the correct
evolution operator, we test the semigroup property of the map; [92] pursues closely


20 N. H. NELSEN AND A. M. STUART



0 _._ 4


0 _._ 2


0 _._ 0


_−_ 0 _._ 2


_−_ 0 _._ 4





0 _._ 0 0 _._ 2 0 _._ 4 0 _._ 6 0 _._ 8 1 _._ 0

_x_


(b)
(a)


Fig. 4: Representative input-output test sample for the Burgers’ equation solution
map _F_ _[†]_ := Ψ 1 : Here, _n_ = 512, _m_ = 1024, and _K_ = 1025. Figure 4a shows a sample
input, output (truth), and trained RFM prediction (test), while Figure 4b displays
the pointwise error. The relative _L_ [2] error for this single prediction is 0 _._ 0146.


related work also in a Fourier space setting. Denote the ( _j −_ 1)-fold composition of a
function _G_ with itself by _G_ _[j]_ . Then, with _u_ (0 _, ·_ ) = _a_, we have


(4.5) (Ψ _T_ _◦· · · ◦_ Ψ _T_ )( _a_ ) = Ψ _[j]_ _T_ [(] _[a]_ [) = Ψ] _[jT]_ [ (] _[a]_ [) =] _[ u]_ [(] _[jT,][ ·]_ [)]


by definition. We train the RFM on input-output pairs from the map Ψ _T_ with _T_ := 0 _._ 5
to obtain _F_ [ˆ] := _F_ _m_ ( _·_ ; ˆ _α_ ). Then, it should follow from (4.5) that _F_ [ˆ] _[j]_ _≈_ Ψ _jT_, that is,
each application of _F_ [ˆ] should evolve the solution _T_ time units. We test this semigroup
approximation by learning the map _F_ [ˆ] and then comparing _F_ [ˆ] _[j]_ on _n_ _[′]_ = 4000 fixed
inputs to outputs from each of the operators Ψ _jT_, with _j ∈{_ 1 _,_ 2 _,_ 3 _,_ 4 _}_ (the solutions
at time _T_, 2 _T_, 3 _T_, 4 _T_ ). The results are presented in Table 1 for a fixed mesh size
_K_ = 129. We observe that the composed RFM map _F_ [ˆ] _[j]_ accurately captures Ψ _jT_,


Train on: _T_ = 0 _._ 5 Test on: 2 _T_ = 1 _._ 0 3 _T_ = 1 _._ 5 4 _T_ = 2 _._ 0


0.0360 0.0407 0.0528 0.0788


Table 1: Expected relative error _e_ _n_ _′_ _,m_ for time upscaling with the learned RFM
operator semigroup for Burgers’ equation: Here, _n_ _[′]_ = 4000, _m_ = 1024, _n_ = 512, and
_K_ = 129. The RFM is trained on data from the evolution operator Ψ _T_ =0 _._ 5, and then
tested on input-output samples generated from Ψ _jT_, where _j_ = 2 _,_ 3 _,_ 4, by repeated
composition of the learned model. The increase in error is small even after three
compositions, reflecting excellent out-of-distribution performance.


though this accuracy deteriorates as _j_ increases due to error propagation in time as
is common with any traditional integrator. However, even after three compositions


THE RANDOM FEATURE MODEL ON BANACH SPACE 21


corresponding to 1.5 time units past the training time _T_ = 0 _._ 5, the relative error only
increases by around 0 _._ 04. It is remarkable that the RFM learns time evolution without
explicitly time-stepping the PDE (3.1) itself. Such a procedure is coined _time upscaling_
in the PDE context and in some sense breaks the CFL stability barrier [28]. Table 1 is
evidence that the RFM has excellent out-of-distribution performance: although only
trained on inputs _a ∼_ _ν_, the model outputs accurate predictions given new input
samples Ψ _jT_ ( _a_ ) _∼_ (Ψ _jT_ ) _♯_ _ν_ .
We next study the ability of the RFM to transfer its learned coefficients ˆ _α_ obtained from training on mesh size _K_ to different mesh resolutions _K_ _[′]_ in Figure 5a.
We fix _T_ := 1 from here on and observe that the lowest test error occurs when
_K_ = _K_ _[′]_, that is, when the train and test resolutions are identical; this behavior was
also observed in the contemporaneous work [56]. At very low resolutions, such as
_K_ = 17 here, the test error is dominated by discretization error which can become
quite large; for example, resolving conceptually infinite-dimensional objects such as
the Fourier space-based feature map in (3.5) or the _L_ [2] norms in (4.4) with only 17
grid points gives bad accuracy. But outside this regime, the errors are essentially
constant across resolution regardless of the training resolution _K_, indicating that the
RFM learns its optimal coefficients independently of the resolution and hence generalizes well to any desired mesh size. In fact, the trained model could be deployed
on different discretizations of the domain _D_ (e.g.: various choices of finite elements,
graph-based/particle methods), not just with different mesh sizes. Practically speaking, this means that high resolution training sets can be subsampled to smaller mesh
sizes _K_ (yet still large enough to avoid large discretization error) for faster training,
leading to a trained model with nearly the same accuracy at all higher resolutions.



0 _._ 25


0 _._ 20


0 _._ 15


0 _._ 10


0 _._ 05





0 _._ 060


0 _._ 055


0 _._ 050


0 _._ 045


0 _._ 040


0 _._ 035


0 _._ 030





0 200 400 600 800 1000

Resolution


(a)



0 200 400 600 800 1000

_m_


(b)



Fig. 5: Expected relative test error of a trained RFM for the Burgers’ evolution
operator _F_ _[†]_ = Ψ 1 with _n_ _[′]_ = 4000 test pairs: Figure 5a displays the invariance of test
error w.r.t. training and testing on different resolutions for _m_ = 1024 and _n_ = 512
fixed; the RFM can train and test on different mesh sizes without loss of accuracy.
Figure 5b shows the decay of the test error for resolution _K_ = 129 fixed as a function
of _m_ and _n_ ; the smallest error achieved is 0 _._ 0303 for _n_ = 1000 and _m_ = 1024.


The smallest expected relative test error achieved by the RFM is 0 _._ 0303 for the


22 N. H. NELSEN AND A. M. STUART


configuration detailed in Figure 5b. This excellent performance is encouraging because
the error we report is of the same order of magnitude as that reported in Sec. 5.1 of

[55] for the same Burgers’ solution operator that we study, but with slightly different
problem parameter choices. We emphasize that the Neural Operator methods in that
work are based on deep learning, which involves training neural networks by solving a
non-convex optimization problem with stochastic gradient descent, while our random
feature methods have orders of magnitude fewer trainable parameters that are easily
optimized through convex optimization. In Figure 5b, we also note that for a small
number of training data _n_, the error does not always decrease as the number of
random features _m_ increases. This indicates a delicate dependence of _m_ as a function
of _n_, in particular, _n_ must increase with _m_ as is expected from parametric estimation;
we observe the desired monotonic decrease in error with _m_ when _n_ is increased to
100 or 1000. In the over-parametrized regime, the authors in [61] present a loose
bound for this dependence for real-valued outputs. We leave a detailed account of the
dependence of _m_ on _n_ required to achieve a certain error tolerance to future work and
refer the interested reader to [17] for detailed statistical analysis in a related setting.



0 _._ 060


0 _._ 055


0 _._ 050


0 _._ 045


0 _._ 040


0 _._ 035


0 _._ 030





10 _[−]_ [1]


10 _[−]_ [2]


10 _[−]_ [3]





0 200 400 600 800 1000

Resolution


(a)



0 100 200 300 400 500

Resolution


(b)



Fig. 6: Results of a trained RFM for the Burgers’ equation evolution operator
_F_ _[†]_ = Ψ 1 : Here, _n_ = 512 training and _n_ _[′]_ = 4000 testing pairs were used. Figure 6a
shows resolution-invariant test error for various _m_ ; the error follows the _O_ ( _m_ _[−]_ [1] _[/]_ [2] )
Monte Carlo rate remarkably well. Figure 6b displays the relative error of the learned
coefficient _α_ w.r.t. the coefficient learned on the highest mesh size ( _K_ = 1025).


Finally, Figure 6 demonstrates the invariance of the expected relative test error
to the mesh resolution used for training and testing. This result is a consequence of
framing the RFM on function space; other machine learning-based surrogate methods
defined in finite-dimensions exhibit an _increase_ in test error as mesh resolution is
increased (see [13], Sec. 4, for a numerical account of this phenomenon). The first
panel, Figure 6a, shows the error as a function of mesh resolution for three values
of _m_ . For very low resolution, the error varies slightly but then flattens out to a
constant value as _K →∞_ . More interestingly, these constant values of error, _e_ _n_ _′_ _,m_ =
0 _._ 063, 0 _._ 043, and 0 _._ 031 corresponding to _m_ = 256, 512, and 1024, respectively, closely
match the Monte Carlo rate _O_ ( _m_ _[−]_ [1] _[/]_ [2] ). While more theory is required to understand


THE RANDOM FEATURE MODEL ON BANACH SPACE 23


this behavior, it suggests that the optimization process finds coefficients close to
those arising from a Monte Carlo approximation of _F_ _[†]_ as discussed in Subsection 2.3.
The second panel, Figure 6b, indicates that the learned coefficient _α_ [(] _[K]_ [)] for each _K_
converges to some _α_ [(] _[∞]_ [)] as _K →∞_, again reflecting the design of the RFM as a
mapping between infinite-dimensional spaces.


**4.2. Darcy Flow: Experiment.** In this section, we consider Darcy flow on the
physical domain _D_ := (0 _,_ 1) [2], the unit square. We generate a high resolution dataset of
input-output pairs for _F_ _[†]_ (3.11) by solving Equation (3.8) on an equispaced 257 _×_ 257
mesh (size _K_ = 257 [2] ) using a second order finite difference scheme. All mesh sizes
_K <_ 257 [2] are subsampled from this original dataset and hence we consider numerical
realizations of _F_ _[†]_ up to R [66049] _→_ R [66049] . We denote _resolution_ by _r_ such that _K_ = _r_ [2] .
We fix _n_ = 128 training and _n_ _[′]_ = 1000 testing pairs unless otherwise noted. The input
data are drawn from the level set measure _ν_ (3.9) with _τ_ = 3 and _α_ = 2 fixed. We
choose _a_ [+] = 12 and _a_ _[−]_ = 3 in all experiments that follow and hence the contrast ratio
_a_ [+] _/a_ _[−]_ = 4 is fixed. The source is fixed to _f ≡_ 1, the constant function. We evaluate
the predictor-corrector random features _ϕ_ (3.12) using an FFT-based fast Poisson
solver corresponding to an underlying second order finite difference stencil at a cost
of _O_ ( _K_ log _K_ ) per solve. The smoothed coefficient _a_ _ε_ in the definition of _ϕ_ is obtained
by solving (3.14) with time step 0 _._ 03 and diffusion constant _η_ = 10 _[−]_ [4] ; with centered
second order finite differences, this incurs 34 time steps and hence a cost _O_ (34 _K_ ). We
fix the hyperparameters _α_ _[′]_ = 2, _τ_ _[′]_ = 7 _._ 5, _s_ [+] = 1 _/_ 12, _s_ _[−]_ = _−_ 1 _/_ 3, and _δ_ = 0 _._ 15 for
the map _ϕ_ . Unlike in Subsection 4.1, we find via grid search on _λ_ that regularization
during training does improve the reconstruction of the Darcy flow solution operator
and hence we train with _λ_ := 10 _[−]_ [8] fixed. We remark that, for simplicity, the above
hyperparameters were not systematically and jointly optimized; as a consequence the
RFM performance has the capacity to improve beyond the results in this section.
Darcy flow is characterized by the geometry of the high contrast coefficients _a ∼_ _ν_ .
As seen in Figure 7, the solution inherits the steep interfaces of the input. However, we
see that a trained RFM with predictor-corrector random features (3.12) captures these
interfaces well, albeit with slight smoothing; the error concentrates on the location of
the interface. The effect of increasing _m_ and _n_ on the test error is shown in Figure 8b.
Here, the error appears to saturate more than was observed for the Burgers’ equation
problem (Figure 5b). However, the smallest test error achieved for the best performing
RFM configuration is 0 _._ 0381, which is on the same scale as the error reported in
competing neural network-based methods [13, 56] for the same Darcy flow setup.
The RFM is able to be successfully trained and tested on different resolutions for
Darcy flow. Figure 8a shows that, again, for low resolutions, the smallest relative test
error is achieved when the train and test resolutions are identical (here, for _r_ = 17).
However, when the resolution is increased away from this low resolution regime, the
relative test error slightly increases then approaches a constant value, reflecting the
function space design of the method. Training the RFM on a high resolution mesh
poses no issues when transferring to lower or higher resolutions for model evaluation,
and it achieves consistent error for test resolutions sufficiently large (i.e., _r ≥_ 33, the
regime where discretization error starts to become negligible). Additionally, the RFM
basis functions _{ϕ_ ( _·_ ; _θ_ _j_ ) _}_ _[m]_ _j_ =1 [are defined without any dependence on the training data]
unlike in other competing approaches based on similar shallow linear approximations,
such as the reduced basis method or the PCA-NN method in [13]. Consequently, our
random feature model may be directly evaluated on any desired mesh resolution once
trained (“super-resolution”), whereas those aforementioned approaches require some


24 N. H. NELSEN AND A. M. STUART


(a) Truth (b) Approximation


(c) Input (d) Pointwise Error


Fig. 7: Representative input-output test sample for the Darcy flow solution map:
Here, _n_ = 256, _m_ = 350, and _K_ = 257 [2] . Figure 7c shows a sample input, Figure 7a
the resulting output (truth), Figure 7b a trained RFM prediction, and Figure 7d the
pointwise error. The relative _L_ [2] error for this single prediction is 0 _._ 0122.


form of interpolation to transfer between different mesh sizes (see [13], Sec. 4.3).
In Figure 9, we again confirm that our method is invariant to the refinement of the
mesh and improves with more random features. While the difference at low resolutions
is more pronounced than that observed for Burgers’ equation, our results for Darcy
flow still suggest that the expected relative test error converges to a constant value
as resolution increases; an estimate of this rate of convergence is seen in Figure 9b,
where we plot the relative error of the learned parameter _α_ [(] _[r]_ [)] at resolution _r_ w.r.t. the
parameter learned at the highest resolution trained, which was _r_ = 129. Although we
do not observe the limiting error following the Monte Carlo rate in _m_, which suggests
that the RKHS _H_ _k_ _µ_ induced by the choice of _ϕ_ may not be expressive enough (e.g.,
not universal [82]), the numerical results make clear that our method nonetheless
performs well as an operator approximator.


THE RANDOM FEATURE MODEL ON BANACH SPACE 25







0 _._ 12


0 _._ 10


0 _._ 08


0 _._ 06


0 _._ 04



10 _[−]_ [1]



0 50 100 150 200 250

Resolution


(a)



0 100 200 300 400 500

_m_


(b)



Fig. 8: Expected relative test error of a trained RFM for Darcy flow with _n_ _[′]_ = 1000
test pairs: Figure 8a displays the invariance of test error w.r.t. training and testing
on different resolutions for _m_ = 512 and _n_ = 256 fixed; the RFM can train and test
on different mesh sizes without significant loss of accuracy. Figure 8b shows the decay
of the test error for resolution _r_ = 33 fixed as a function of _m_ and _n_ ; the smallest
error achieved is 0 _._ 0381 for _n_ = 500 and _m_ = 512.


10 [0]





0 _._ 070


0 _._ 065


0 _._ 060


0 _._ 055


0 _._ 050


0 _._ 045


0 _._ 040





0 25 50 75 100 125

Resolution


(a)



10 _[−]_ [1]


10 _[−]_ [2]



0 20 40 60

Resolution


(b)



Fig. 9: Results of a trained RFM for Darcy flow: Here, _n_ = 128 training and _n_ _[′]_ = 1000
testing pairs were used. Figure 9a demonstrates resolution-invariant test error for
various _m_, while Figure 9b displays the relative error of the learned coefficient _α_ [(] _[r]_ [)]

at resolution _r_ w.r.t. the coefficient learned on the highest resolution ( _r_ = 129).


**5. Conclusions.** In this article, we introduced a random feature methodology
for the data-driven approximation of maps between infinite-dimensional Banach spaces. The random feature model, as an emulator of such maps, performs dimension


26 N. H. NELSEN AND A. M. STUART


reduction in the sense that the original infinite-dimensional learning problem reduces
to an approximate problem of finding _m_ real numbers (Section 2). Our conceptually
infinite-dimensional algorithm is non-intrusive and results in a scalable method that
is consistent with the continuum limit, robust to discretization, and highly flexible
in practical use. These benefits were verified in numerical experiments for two nonlinear forward operators based on PDEs, one involving a semigroup and another a
coefficient-to-solution operator (Section 4). While the random feature-based operator
emulator learned from data is not guaranteed to be cheaper to evaluate than a full order solver in general, our design of problem-specific random feature maps in Section 3
leads to efficient _O_ ( _mK_ log _K_ ) evaluation of an _m_ -term RFM for simple physical domain geometries and hence competitive computational cost in many-query settings.
A straightforward GPU implementation would provide further acceleration.
There are various directions for future work. We are interested in application
of random feature methods to more challenging problems in the sciences, such as
climate modeling and material modeling, and to the solution of design and inverse
problems arising in those settings with the RFM serving as a cheap emulator. Of
great importance in furthering the methodology is the question of how to adapt the
random features to data instead of manually constructing them. Some possibilities
along this line of work include the Bayesian optimization of RFM hyperparameters, as
frequently used in Gaussian process regression, or more general hierarchical learning of
the pair ( _ϕ, µ_ ) itself, both of which would lead to data-adapted induced kernels. Such
developments would make the RFM more streamlined, competitive with deep learning
alternatives, and serve to further clarify the effectiveness of function space learning
algorithms. Finally, the development of a theory which underpins our method, allows
for proof of convergence, and characterizes the quality of the RKHS spaces induced by
random feature maps, would be both mathematically interesting and highly desirable
as it would help guide methodological development.


**Appendix A. Proofs of Results.**


_Proof of Result_ 2.5 _._ Fix _a ∈X_ and _y ∈Y_ . Then, we note that


(A.1) _k_ _µ_ ( _·, a_ ) _y_ = _⟨ϕ_ ( _a_ ; _θ_ ) _, y⟩_ _Y_ _ϕ_ ( _·_ ; _θ_ ) _µ_ ( _dθ_ ) = _A⟨ϕ_ ( _a_ ; _·_ ) _, y⟩_ _Y_ _∈_ im( _A_ ) _,_
�


since _⟨ϕ_ ( _a_ ; _·_ ) _, y⟩_ _Y_ _∈_ _L_ [2] _µ_ [(Θ;][ R][) by the Cauchy-Schwarz inequality.]
Now we show that im( _A_ ) admits a reproducing property of the form (2.10). First,
note that _A_ can be viewed as a bijection between its coimage and image spaces, and
we denote this bijection by


˜
(A.2) _A_ : ker( _A_ ) _[⊥]_ _→_ im( _A_ ) _._


For any _F, G ∈_ im( _A_ ), define the candidate RKHS inner product _⟨·, ·⟩_ by



˜
(A.3) _⟨F, G⟩_ := � _A_ _[−]_ [1] _F,_ _A_ [˜] _[−]_ [1] _G_ �



_L_ [2] _µ_ (Θ;R) _[.]_



This is indeed a valid inner product since _A_ [˜] is invertible. Note that for any _q ∈_ ker( _A_ ),



� _q, ⟨ϕ_ ( _a_ ; _·_ ) _, y⟩_ _Y_ �



_L_ [2] _µ_ (Θ;R) [=] � _q_ ( _θ_ ) _⟨ϕ_ ( _a_ ; _θ_ ) _, y⟩_ _Y_ _µ_ ( _dθ_ )


= _q_ ( _θ_ ) _ϕ_ ( _a_ ; _θ_ ) _µ_ ( _dθ_ ) _, y_
� [�] � _Y_


= 0


THE RANDOM FEATURE MODEL ON BANACH SPACE 27


so that _⟨ϕ_ ( _a_ ; _·_ ) _, y⟩_ _Y_ _∈_ ker( _A_ ) _[⊥]_ . Then for any _F ∈_ im( _A_ ), we compute



_⟨k_ _µ_ ( _·, a_ ) _y, F_ _⟩_ = � _⟨ϕ_ ( _a_ ; _·_ ) _, y⟩_ _Y_ _,_ _A_ [˜] _[−]_ [1] _F_ �



_L_ [2] _µ_ (Θ;R)



= _⟨ϕ_ ( _a_ ; _θ_ ) _, y⟩_ _Y_ ( _A_ [˜] _[−]_ [1] _F_ )( _θ_ ) _µ_ ( _dθ_ )
�


= ( _A_ [˜] _[−]_ [1] _F_ )( _θ_ ) _ϕ_ ( _a_ ; _θ_ ) _µ_ ( _dθ_ ) _, y_
� [�] � _Y_

= � _y,_ ( _AA_ [˜] _[−]_ [1] _F_ )( _a_ )� _Y_

= _⟨y, F_ ( _a_ ) _⟩_ _Y_ _,_



which gives exactly (2.10) if our candidate inner product is defined to be the RKHS
inner product. Since _F ∈_ im( _A_ ) is arbitrary, this and (A.1) together imply that
im( _A_ ) = _H_ _k_ _µ_ is the RKHS induced by _k_ _µ_ as shown in [26, 46].

_Proof of Result_ 2.6 _._ Since _L_ [2]
_µ_ [(] _[m]_ [)] [(Θ;][ R][) is isomorphic to][ R] _[m]_ [, we can consider the]
map _A_ : R _[m]_ _→_ _L_ [2] _ν_ [(] _[X]_ [;] _[ Y]_ [) defined in][ (2.14)][ and use][ Result 2.5][ to conclude that]



1
(A.4) _H_ _k_ ( _m_ ) = im( _A_ ) =
� _m_



_m_
� _c_ _j_ _ϕ_ ( _·_ ; _θ_ _j_ ) : _c ∈_ R _[m]_ = span _{ϕ_ _j_ _}_ _[m]_ _j_ =1 _[,]_

_j_ =1 �



since the _{ϕ_ _j_ _}_ _[m]_ _j_ =1 [are assumed linearly independent.]


_Proof of Result_ 2.8 _._ Recall from Result 2.6 that the RKHS _H_ _k_ ( _m_ ) comprises the
linear span of the _{ϕ_ _j_ := _ϕ_ ( _·_ ; _θ_ _j_ ) _}_ _[m]_ _j_ =1 [.] Hence _ϕ_ _j_ _∈H_ _k_ ( _m_ ), and note that by the
reproducing kernel property (2.10), for any _F ∈H_ _k_ ( _m_ ), _a ∈X_ and _y ∈Y_,



_⟨y, F_ ( _a_ ) _⟩_ _Y_ = � _k_ [(] _[m]_ [)] ( _·, a_ ) _y, F_ � _H_ _k_ ( _m_ )



= [1]


_m_



_m_
� _⟨ϕ_ _j_ ( _a_ ) _, y⟩_ _Y_ _⟨ϕ_ _j_ _, F_ _⟩_ _H_ _k_ ( _m_ )

_j_ =1



= _y,_ [1]
� _m_



_m_
� _⟨ϕ_ _j_ _, F_ _⟩_ _H_ _k_ ( _m_ ) _ϕ_ _j_ ( _a_ ) _._

_j_ =1 � _Y_



Since this is true for all _y ∈Y_, we deduce that



(A.5) _F_ = [1]

_m_



_m_
� _α_ _j_ _ϕ_ _j_ _,_ _α_ _j_ = _⟨ϕ_ _j_ _, F_ _⟩_ _H_ _k_ ( _m_ ) _._

_j_ =1



As the _{ϕ_ _j_ _}_ _[m]_ _j_ =1 [are assumed linearly independent, we deduce that the representa-]
tion (A.5) is unique.
Finally, we calculate the RKHS norm of any such _F_ in terms of _α_ :



1
_∥F_ _∥_ [2] _H_ _k_ ( _m_ ) [=] _[ ⟨][F, F]_ _[⟩]_ _H_ _k_ ( _m_ ) [=] � _m_



_m_
� _α_ _j_ _ϕ_ _j_ _, F_

_j_ =1 � _H_ _k_ ( _m_ )



= [1]


_m_


= [1]


_m_



_m_
� _α_ _j_ _⟨ϕ_ _j_ _, F_ _⟩_ _H_ _k_ ( _m_ )

_j_ =1



_m_
� _α_ _j_ [2] _[.]_

_j_ =1



Substituting this into (2.24), we obtain the desired equivalence with (2.23).


28 N. H. NELSEN AND A. M. STUART


**Appendix B. Further Remarks on Integral Representation of RKHS.**
We recall the linear operator _A_ (2.14) from Subsection 2.3. In this appendix, we
clarify the meaning of Equation (2.12) and show that _A_ is a square root of _T_ _k_ _µ_ .
Similar discussion is provided by Bach in [3], Sec. 2, for the special case _Y_ = R.
By the assumption _ϕ ∈_ _L_ [2] _ν×µ_ [(] _[X ×]_ [ Θ;] _[ Y]_ [) and Cauchy-Schwarz inequality, we find]


(B.1) _A ∈L_ � _L_ [2] _µ_ [(Θ;][ R][)] _[, L]_ _ν_ [2] [(] _[X]_ [;] _[ Y]_ [)] � _._


Now let _F ∈_ im( _A_ ) = _H_ _k_ _µ_ . We have _F_ = _Ac_ for some _c ∈_ _L_ [2] _µ_ [. But since ker(] _[A]_ [)]
is closed, _L_ [2] _µ_ [= ker(] _[A]_ [)] _[ ⊕]_ [ker(] _[A]_ [)] _[⊥]_ [and hence there exist unique] _[ q]_ _[F]_ _[∈]_ [ker(] _[A]_ [) and]
_c_ _F_ _∈_ ker( _A_ ) _[⊥]_ such that _c_ = _q_ _F_ + _c_ _F_ . Using the notation in (A.2), we have _c_ _F_ = _A_ [˜] _[−]_ [1] _F_
by definition of _A_ [˜] . The reproducing property in Proof 1 produced the representation
_F_ = _Ac_ _F_ ; in fact, the similar calculation leading to (2.12) in Subsection 2.3 also
identified the unique _c_ _F_, there defined formally by _c_ _F_ ( _θ_ ) = _⟨ϕ_ ( _·_ ; _θ_ ) _, F_ _⟩_ _H_ _kµ_ . Indeed,


_⟨c_ _F_ _, q⟩_ _L_ 2 _µ_ (Θ;R) = _⟨ϕ_ ( _·_ ; _θ_ ) _, F_ _⟩_ _H_ _kµ_ _q_ ( _θ_ ) _µ_ ( _dθ_ )
�


= _q_ ( _θ_ ) _ϕ_ ( _·_ ; _θ_ ) _µ_ ( _dθ_ ) _, F_
� [�] � _H_ _kµ_


= 0


for any _q ∈_ ker( _A_ ). Hence _c_ _F_ _∈_ ker( _A_ ) _[⊥]_, and we interpret Equation (2.12) as
formal notation for the unique element _A_ [˜] _[−]_ [1] _F ∈_ ker( _A_ ) _[⊥]_ . Using formula (A.3) and
orthogonality, we also obtain the following useful characterization of the RKHS norm:


˜ 2
(B.2) _∥F_ _∥_ [2] _H_ _kµ_ [=] �� _A_ _[−]_ [1] _F_ �� _L_ [2] _µ_ [=] _[ ∥][c]_ _[F]_ _[ ∥]_ _L_ [2] [2] _µ_ [= min] _c∈C_ _F_ _[∥][c][∥]_ _L_ [2] [2] _µ_ _[,]_


where _C_ _F_ := _{c ∈_ _L_ [2] _µ_ [(Θ;][ R][) :] _[ A][c]_ [ =] _[ F]_ _[}]_ [.]
Finally, we show that _AA_ _[∗]_ = _T_ _k_ _µ_ . This means that the RKHS is equal to the
image of two different square roots of integral operator _T_ _k_ _µ_ : _H_ _k_ _µ_ = im( _T_ _k_ [1] _µ_ _[/]_ [2] [) = im(] _[A]_ [).]
First, for any _F ∈_ _L_ [2] _ν_ [(] _[X]_ [;] _[ Y]_ [) and] _[ c][ ∈]_ _[L]_ [2] _µ_ [(Θ;][ R][),]



_⟨F, Ac⟩_ _L_ 2 _ν_ [=] � _F,_ � _c_ ( _θ_ ) _ϕ_ ( _·_ ; _θ_ ) _µ_ ( _dθ_ )�



_L_ [2] _ν_



= _c_ ( _θ_ ) _⟨F, ϕ_ ( _·_ ; _θ_ ) _⟩_ _L_ 2 _ν_ _µ_ ( _dθ_ )
�


= _⟨F_ ( _a_ _[′]_ ) _, ϕ_ ( _a_ _[′]_ ; _·_ ) _⟩_ _Y_ _ν_ ( _da_ _[′]_ ) _, c_
� [�] �



_L_ [2]
_µ_



by the Fubini-Tonelli theorem. So, we deduce that the adjoint of _A_ is


_A_ _[∗]_ : _L_ [2] _ν_ [(] _[X]_ [;] _[ Y]_ [)] _[ →]_ _[L]_ [2] _µ_ [(Θ;][ R][)]

(B.3)
_F �→A_ _[∗]_ _F_ := _⟨F_ ( _a_ _[′]_ ) _, ϕ_ ( _a_ _[′]_ ; _·_ ) _⟩_ _Y_ _ν_ ( _da_ _[′]_ ) _,_
�


THE RANDOM FEATURE MODEL ON BANACH SPACE 29


which is bounded since _A_ is bounded. For any _F ∈_ _L_ [2] _ν_ [(] _[X]_ [;] _[ Y]_ [), we compute]


_AA_ _[∗]_ _F_ = ( _A_ _[∗]_ _F_ )( _θ_ ) _ϕ_ ( _·_ ; _θ_ ) _µ_ ( _dθ_ )
� Θ



=
� Θ


=
� _X_



_⟨F_ ( _a_ _[′]_ ) _, ϕ_ ( _a_ _[′]_ ; _θ_ ) _⟩_ _Y_ _ϕ_ ( _·_ ; _θ_ ) _ν_ ( _da_ _[′]_ ) _µ_ ( _dθ_ )

� _X_



_ϕ_ ( _·_ ; _θ_ ) _⊗_ _ϕ_ ( _a_ _[′]_ ; _θ_ ) _µ_ ( _dθ_ ) _F_ ( _a_ _[′]_ ) _ν_ ( _da_ _[′]_ )

�� Θ �



= _T_ _k_ _µ_ _F,_


again by Fubini-Tonelli, as desired.


**Acknowledgments.** The authors thank Bamdad Hosseini and Nikola B. Kovachki for helpful discussions and are grateful to the two anonymous referees for their
careful reading and insightful comments.


REFERENCES


[1] B. Adcock, S. Brugiapaglia, N. Dexter, and S. Moraga, _Deep neural networks are effective_
_at learning high-dimensional hilbert-valued functions from limited data_, arXiv preprint
arXiv:2012.06081, (2020).

[2] N. Aronszajn, _Theory of reproducing kernels_, Transactions of the American Mathematical
Society, 68 (1950), pp. 337–404.

[3] F. Bach, _On the equivalence between kernel quadrature rules and random feature expansions_,
The Journal of Machine Learning Research, 18 (2017), pp. 714–751.

[4] Y. Bar-Sinai, S. Hoyer, J. Hickey, and M. P. Brenner, _Learning data-driven discretizations_
_for partial differential equations_, Proceedings of the National Academy of Sciences, 116
(2019), pp. 15344–15349.

[5] M. Barrault, Y. Maday, N. C. Nguyen, and A. T. Patera, _An ‘empirical interpola-_
_tion’method: application to efficient reduced-basis discretization of partial differential equa-_
_tions_, Comptes Rendus Mathematique, 339 (2004), pp. 667–672.

[6] A. R. Barron, _Universal approximation bounds for superpositions of a sigmoidal function_,
IEEE Transactions on Information theory, 39 (1993), pp. 930–945.

[7] J. Bear and M. Y. Corapcioglu, _Fundamentals of transport phenomena in porous media_,
vol. 82, Springer Science & Business Media, 2012.

[8] M. Belkin, D. Hsu, S. Ma, and S. Mandal, _Reconciling modern machine-learning practice_
_and the classical bias–variance trade-off_, Proceedings of the National Academy of Sciences,
116 (2019), pp. 15849–15854.

[9] P. Benner, A. Cohen, M. Ohlberger, and K. Willcox, _Model reduction and approximation:_
_theory and algorithms_, vol. 15, SIAM, 2017.

[10] A. Berlinet and C. Thomas-Agnan, _Reproducing kernel Hilbert spaces in probability and_
_statistics_, Springer Science & Business Media, 2011.

[11] C. Bernardi and R. Verf¨urth, _Adaptive finite element methods for elliptic equations with_
_non-smooth coefficients_, Numerische Mathematik, 85 (2000), pp. 579–608.

[12] G. Beylkin and M. J. Mohlenkamp, _Algorithms for numerical analysis in high dimensions_,
SIAM Journal on Scientific Computing, 26 (2005), pp. 2133–2159.

[13] K. Bhattacharya, B. Hosseini, N. B. Kovachki, and A. M. Stuart, _Model reduction and_
_neural networks for parametric pdes_, arXiv preprint arXiv:2005.03180, (2020).

[14] D. Bigoni, Y. Chen, N. G. Trillos, Y. Marzouk, and D. Sanz-Alonso, _Data-driven forward_
_discretizations for Bayesian inversion_, arXiv preprint arXiv:2003.07991, (2020).

[15] R. Brault, M. Heinonen, and F. Buc, _Random fourier features for operator-valued kernels_,
in Asian Conference on Machine Learning, 2016, pp. 110–125.

[16] Y. Cao and Q. Gu, _Generalization bounds of stochastic gradient descent for wide and deep_
_neural networks_, in Advances in Neural Information Processing Systems, 2019, pp. 10835–
10845.

[17] A. Caponnetto and E. De Vito, _Optimal rates for the regularized least-squares algorithm_,
Foundations of Computational Mathematics, 7 (2007), pp. 331–368.

[18] C. Carmeli, E. De Vito, and A. Toigo, _Vector valued reproducing kernel Hilbert spaces of_
_integrable functions and Mercer theorem_, Analysis and Applications, 4 (2006), pp. 377–408.


30 N. H. NELSEN AND A. M. STUART


[19] G. Chen and K. Fidkowski, _Output-based error estimation and mesh adaptation using con-_
_volutional neural networks: Application to a scalar advection-diffusion problem_, in AIAA
Scitech 2020 Forum, 2020, p. 1143.

[20] T. Chen and H. Chen, _Universal approximation to nonlinear operators by neural networks with_
_arbitrary activation functions and its application to dynamical systems_, IEEE Transactions
on Neural Networks, 6 (1995), pp. 911–917.

[21] M. Cheng, T. Y. Hou, M. Yan, and Z. Zhang, _A data-driven stochastic method for ellip-_
_tic PDEs with random coefficients_, SIAM/ASA Journal on Uncertainty Quantification, 1
(2013), pp. 452–493.

[22] A. Chkifa, A. Cohen, R. DeVore, and C. Schwab, _Sparse adaptive taylor approximation_
_algorithms for parametric and stochastic elliptic pdes_, ESAIM: Mathematical Modelling
and Numerical Analysis, 47 (2013), pp. 253–280.

[23] A. Cohen and R. DeVore, _Approximation of high-dimensional parametric PDEs_, Acta Numerica, 24 (2015), pp. 1–159.

[24] A. Cohen and G. Migliorati, _Optimal weighted least-squares methods_, arXiv preprint
arXiv:1608.00512, (2016).

[25] S. L. Cotter, G. O. Roberts, A. M. Stuart, and D. White, _Mcmc methods for functions:_
_modifying old algorithms to make them faster_, Statistical Science, (2013), pp. 424–446.

[26] F. Cucker and S. Smale, _On the mathematical foundations of learning_, Bulletin of the American mathematical society, 39 (2002), pp. 1–49.

[27] M. Dashti and A. M. Stuart, _The Bayesian Approach to Inverse Problems_, Springer Interna[tional Publishing, Cham, 2017, pp. 311–428, https://doi.org/10.1007/978-3-319-12385-1](https://doi.org/10.1007/978-3-319-12385-1_7) ~~7~~ .

[28] L. Demanet, _Curvelets, wave atoms, and wave equations_, PhD thesis, California Institute of
Technology, 2006.

[29] R. A. DeVore, _The theoretical foundation of reduced basis methods_, Model Reduction and
approximation: Theory and Algorithms, (2014), pp. 137–168.

[30] A. Doostan and G. Iaccarino, _A least-squares approximation of partial differential equa-_
_tions with high-dimensional random inputs_, Journal of Computational Physics, 228 (2009),
pp. 4332–4345.

[31] M. M. Dunlop, M. A. Iglesias, and A. M. Stuart, _Hierarchical bayesian level set inversion_,
Statistics and Computing, 27 (2017), pp. 1555–1584.

[32] L. C. Evans, _Partial differential equations_, vol. 19, American Mathematical Soc., 2010.

[33] Y. Fan and L. Ying, _Solving electrical impedance tomography with deep learning_, Journal of
Computational Physics, 404 (2020), pp. 109–119.

[34] J. Feliu-Faba, Y. Fan, and L. Ying, _Meta-learning pseudo-differential operators with deep_
_neural networks_, Journal of Computational Physics, 408 (2020), p. 109309.

[35] B. Fornberg, _A practical guide to pseudospectral methods_, vol. 1, Cambridge university press,
1998.

[36] H. Gao, J.-X. Wang, and M. J. Zahr, _Non-intrusive model reduction of large-scale, nonlinear_
_dynamical systems using deep learning_, arXiv preprint arXiv:1911.03808, (2019).

[37] M. Geist, P. Petersen, M. Raslan, R. Schneider, and G. Kutyniok, _Numerical so-_
_lution of the parametric diffusion equation by deep neural networks_, arXiv preprint
arXiv:2004.12131, (2020).

[38] D. Gilbarg and N. S. Trudinger, _Elliptic partial differential equations of second order_,
springer, 2015.

[39] R. Gonzalez-Garcia, R. Rico-Martinez, and I. Kevrekidis, _Identification of distributed_
_parameter systems: A neural net based approach_, Computers & chemical engineering, 22
(1998), pp. S965–S968.

[40] M. Griebel and C. Rieger, _Reproducing kernel Hilbert spaces for parametric partial differen-_
_tial equations_, SIAM/ASA Journal on Uncertainty Quantification, 5 (2017), pp. 111–137.

[41] E. Haber and L. Ruthotto, _Stable architectures for deep neural networks_, Inverse Problems,
34 (2017), p. 014004.

[42] T. Hastie, R. Tibshirani, and J. Friedman, _The elements of statistical learning: data mining,_
_inference, and prediction_, Springer Science & Business Media, 2009.

[43] J. S. Hesthaven and S. Ubbiali, _Non-intrusive reduced order modeling of nonlinear problems_
_using neural networks_, Journal of Computational Physics, 363 (2018), pp. 55–78.

[44] M. Hinze, R. Pinnau, M. Ulbrich, and S. Ulbrich, _Optimization with PDE constraints_,
vol. 23, Springer Science & Business Media, 2008.

[45] A. Jacot, F. Gabriel, and C. Hongler, _Neural tangent kernel: Convergence and general-_
_ization in neural networks_, in Advances in neural information processing systems, 2018,
pp. 8571–8580.

[46] H. Kadri, E. Duflos, P. Preux, S. Canu, A. Rakotomamonjy, and J. Audiffren,


THE RANDOM FEATURE MODEL ON BANACH SPACE 31


_Operator-valued kernels for learning from functional response data_, The Journal of Machine Learning Research, 17 (2016), pp. 613–666.

[47] A.-K. Kassam and L. N. Trefethen, _Fourth-order time-stepping for stiff PDEs_, SIAM Journal on Scientific Computing, 26 (2005), pp. 1214–1233.

[48] R. Kempf, H. Wendland, and C. Rieger, _Kernel-based reconstructions for parametric_
_PDEs_, in International Workshop on Meshfree Methods for Partial Differential Equations,
Springer, 2017, pp. 53–71.

[49] Y. Khoo, J. Lu, and L. Ying, _Solving parametric pde problems with artificial neural networks_,
arXiv preprint arXiv:1707.03351, (2017).

[50] A. Kiselev, F. Nazarov, and R. Shterenberg, _Blow up and regularity for fractal burgers_
_equation_, arXiv preprint arXiv:0804.3549, (2008).

[51] Y. Korolev, _Two-layer neural networks with values in a banach space_, arXiv preprint
arXiv:2105.02095, (2021).

[52] G. Kutyniok, P. Petersen, M. Raslan, and R. Schneider, _A theoretical analysis of deep_
_neural networks and parametric PDEs_, arXiv preprint arXiv:1904.00377, (2019).

[53] K. Lee and K. T. Carlberg, _Model reduction of dynamical systems on nonlinear mani-_
_folds using deep convolutional autoencoders_, Journal of Computational Physics, 404 (2020),
p. 108973.

[54] Y. Li, J. Lu, and A. Mao, _Variational training of neural network approximations of solution_
_maps for physical models_, Journal of Computational Physics, 409 (2020), p. 109338.

[55] Z. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, and
A. Anandkumar, _Fourier neural operator for parametric partial differential equations_,
arXiv preprint arXiv:2010.08895, (2020).

[56] Z. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, and
A. Anandkumar, _Neural operator: Graph kernel network for partial differential equa-_
_tions_, arXiv preprint arXiv:2003.03485, (2020).

[57] Z. Long, Y. Lu, X. Ma, and B. Dong, _Pde-net: Learning PDEs from data_, arXiv preprint
arXiv:1710.09668, (2017).

[58] L. Lu, P. Jin, and G. E. Karniadakis, _Deeponet: Learning nonlinear operators for identifying_
_differential equations based on the universal approximation theorem of operators_, arXiv
preprint arXiv:1910.03193, (2019).

[59] D. G. Luenberger, _Optimization by vector space methods_, John Wiley & Sons, 1997.

[60] C. Ma, L. Wu, and E. Weinan, _Machine learning from a continuous viewpoint_, arXiv preprint
arXiv:1912.12777, (2019).

[61] C. Ma, L. Wu, and E. Weinan, _On the generalization properties of minimum-norm solutions_
_for over-parameterized neural network models_, arXiv preprint arXiv:1912.06987, (2019).

[62] B. Mat´ern, _Spatial variation_, vol. 36, Springer Science & Business Media, 2013.

[63] C. A. Micchelli and M. Pontil, _On learning vector-valued functions_, Neural computation,
17 (2005), pp. 177–204.

[64] R. M. Neal, _Priors for infinite networks_, in Bayesian Learning for Neural Networks, Springer,
1996, pp. 29–53.

[65] T. O’Leary-Roseberry, U. Villa, P. Chen, and O. Ghattas, _Derivative-informed projected_
_neural networks for high-dimensional parametric maps governed by pdes_, arXiv preprint
arXiv:2011.15110, (2020).

[66] J. A. Opschoor, C. Schwab, and J. Zech, _Deep learning in high dimension: Relu net-_
_work expression rates for bayesian pde inversion_, SAM Research Report, 2020 (2020),
p. OSZ20 ~~9~~ 20.

[67] R. G. Patel and O. Desjardins, _Nonlinear integro-differential operator regression with neural_
_networks_, arXiv preprint arXiv:1810.08552, (2018).

[68] R. G. Patel, N. A. Trask, M. A. Wood, and E. C. Cyr, _A physics-informed opera-_
_tor regression framework for extracting data-driven continuum models_, arXiv preprint
arXiv:2009.11992, (2020).

[69] B. Peherstorfer, K. Willcox, and M. Gunzburger, _Survey of multifidelity methods in_
_uncertainty propagation, inference, and optimization_, Siam Review, 60 (2018), pp. 550–
591.

[70] A. Rahimi and B. Recht, _Random features for large-scale kernel machines_, in Advances in
neural information processing systems, 2008, pp. 1177–1184.

[71] A. Rahimi and B. Recht, _Uniform approximation of functions with random bases_, in 2008
46th Annual Allerton Conference on Communication, Control, and Computing, IEEE,
2008, pp. 555–561.

[72] A. Rahimi and B. Recht, _Weighted sums of random kitchen sinks: Replacing minimization_
_with randomization in learning_, Advances in neural information processing systems, 21


32 N. H. NELSEN AND A. M. STUART


(2008), pp. 1313–1320.

[73] M. Raissi, P. Perdikaris, and G. E. Karniadakis, _Physics-informed neural networks: A deep_
_learning framework for solving forward and inverse problems involving nonlinear partial_
_differential equations_, Journal of Computational Physics, 378 (2019), pp. 686–707.

[74] R. Rico-Martinez, K. Krischer, I. Kevrekidis, M. Kube, and J. Hudson, _Discrete-vs._
_continuous-time nonlinear signal processing of cu electrodissolution data_, Chemical Engineering Communications, 118 (1992), pp. 25–48.

[75] F. Rossi and B. Conan-Guez, _Functional multi-layer perceptron: a non-linear tool for func-_
_tional data analysis_, Neural networks, 18 (2005), pp. 45–60.

[76] L. Ruthotto and E. Haber, _Deep neural networks motivated by partial differential equations_,
Journal of Mathematical Imaging and Vision, (2019), pp. 1–13.

[77] N. D. Santo, S. Deparis, and L. Pegolotti, _Data driven approximation of parametrized_
_PDEs by reduced basis and neural networks_, arXiv preprint arXiv:1904.01514, (2019).

[78] C. Schwab and J. Zech, _Deep learning in high dimension: Neural network expression rates_
_for generalized polynomial chaos expansions in uq_, Analysis and Applications, 17 (2019),
pp. 19–55.

[79] J. Sirignano and K. Spiliopoulos, _Dgm: A deep learning algorithm for solving partial dif-_
_ferential equations_, Journal of Computational Physics, 375 (2018), pp. 1339–1364.

[80] P. D. Spanos and R. Ghanem, _Stochastic finite element expansion for random media_, Journal
of engineering mechanics, 115 (1989), pp. 1035–1053.

[81] B. Stevens and T. Colonius, _Finitenet: A fully convolutional lstm network architecture for_
_time-dependent partial differential equations_, arXiv preprint arXiv:2002.03014, (2020).

[82] Y. Sun, A. Gilbert, and A. Tewari, _On the approximation properties of random relu features_,
arXiv preprint arXiv:1810.04374, (2019).

[83] N. Trask, R. G. Patel, B. J. Gross, and P. J. Atzberger, _Gmls-nets: A framework for_
_learning from unstructured data_, arXiv preprint arXiv:1909.05371, (2019).

[84] R. K. Tripathy and I. Bilionis, _Deep uq: Learning deep neural network surrogate models for_
_high dimensional uncertainty quantification_, Journal of computational physics, 375 (2018),
pp. 565–588.

[85] E. Weinan, _A proposal on machine learning via dynamical systems_, Communications in Mathematics and Statistics, 5 (2017), pp. 1–11.

[86] E. Weinan, J. Han, and Q. Li, _A mean-field optimal control formulation of deep learning_,
Research in the Mathematical Sciences, 6 (2019), p. 10.

[87] E. Weinan and B. Yu, _The deep ritz method: a deep learning-based numerical algorithm for_
_solving variational problems_, Communications in Mathematics and Statistics, 6 (2018),
pp. 1–12.

[88] H. Wendland, _Scattered data approximation_, vol. 17, Cambridge university press, 2004.

[89] C. K. Williams, _Computing with infinite networks_, in Advances in neural information processing systems, 1997, pp. 295–301.

[90] C. K. Williams and C. E. Rasmussen, _Gaussian processes for machine learning_, vol. 2, MIT
press Cambridge, MA, 2006.

[91] N. Winovich, K. Ramani, and G. Lin, _Convpde-uq: Convolutional neural networks with_
_quantified uncertainty for heterogeneous elliptic partial differential equations on varied_
_domains_, Journal of Computational Physics, 394 (2019), pp. 263–279.

[92] K. Wu and D. Xiu, _Data-driven deep learning of partial differential equations in modal space_,
Journal of Computational Physics, 408 (2020), p. 109307.

[93] Y. Zhu and N. Zabaras, _Bayesian deep convolutional encoder–decoder networks for surrogate_
_modeling and uncertainty quantification_, Journal of Computational Physics, 366 (2018),
pp. 415–447.


